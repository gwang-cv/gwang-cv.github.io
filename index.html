<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Hello World</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hello World">
<meta property="og:url" content="http://gwang-cv.github.io/index.html">
<meta property="og:site_name" content="Hello World">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hello World">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Gang Wang</a></h1>
		</hgroup>

		
		<p class="header-subtitle">a computer vision researchGO</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>Über</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Home</a></li>
				        
							<li><a href="/archives">Archives</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
					        
								<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/DeepLearning/" style="font-size: 16.67px;">DeepLearning</a> <a href="/tags/ML/" style="font-size: 10px;">ML</a> <a href="/tags/Mac/" style="font-size: 20px;">Mac</a> <a href="/tags/Math/" style="font-size: 10px;">Math</a> <a href="/tags/Matlab/" style="font-size: 10px;">Matlab</a> <a href="/tags/Python/" style="font-size: 13.33px;">Python</a> <a href="/tags/Researcher/" style="font-size: 10px;">Researcher</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">Hello, I&#39;m Gang Wang. This is my blog, enjoy it.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Gang Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Gang Wang</h1>
			</hgroup>
			
			<p class="header-subtitle">a computer vision researchGO</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Home</a></li>
		        
					<li><a href="/archives">Archives</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
			        
						<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Git配置出错Permission Denied" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/07/06/Git配置出错Permission Denied/" class="article-date">
  	<time datetime="2017-07-06T03:20:16.000Z" itemprop="datePublished">2017-07-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/06/Git配置出错Permission Denied/">Git配置出现Permission denied问题</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>MacOS github后台配置ssh key之后本地无法git clone的问题 </p>
<pre><code><span class="variable">Permission</span> denied (publickey).
</code></pre><p>当你在github后台添加了ssh keys之后，在本地这么测试一下：</p>
<pre><code><span class="title">ssh</span> -T git<span class="variable">@github</span>.com
</code></pre><p>如果返回是：</p>
<pre><code><span class="variable">Permission</span> denied (publickey).
</code></pre><p>那么你可能要在本地ssh-add一下，当然在这之前你可以使用 ssh -vT git@github.com 查看一下到底是因为什么原因导致的失败。</p>
<pre><code><span class="label">ssh</span>-<span class="keyword">add </span>~/.ssh/id_rsa (maybe: ssh-<span class="keyword">add </span>~/id_rsa)
</code></pre><p>然后会返回如下：</p>
<pre><code>Enter passphrase <span class="keyword">for</span> <span class="regexp">/Users/</span>xxx<span class="regexp">/.ssh/</span><span class="string">id_rsa:</span>
Identity <span class="string">added:</span> <span class="regexp">/Users/</span>xxx<span class="regexp">/.ssh/</span>id_rsa (<span class="regexp">/Users/</span>xxx<span class="regexp">/.ssh/</span>youraccount_rsa)
</code></pre><p>之后再使用 </p>
<pre><code><span class="title">ssh</span> -T git<span class="variable">@github</span>.com
</code></pre><p>会返回成功：</p>
<pre><code>Hi youraccount! You've successfully authenticated, <span class="keyword">but</span> GitHub <span class="keyword">does</span> <span class="keyword">not</span> provide shell access.
</code></pre><p>说明你目前本地的ssh已经切换到了id_rsa这个账号，</p>
<p>之后便可以进行git clone到本地的操作了！</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Mac/">Mac</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Debug/">Debug</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Ubuntu16.04+Titan X+CUDA8.0+cudnn5" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/30/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/" class="article-date">
  	<time datetime="2016-12-30T03:15:04.366Z" itemprop="datePublished">2016-12-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/30/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/">Ubuntu16.04+Titan X+CUDA8.0+cudnn5.1+Caffe</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>1.安装Ubuntu16.04 LTS x64</strong></p>
<p>利用工具rufus制作USB系统盘(官方下载64位版本: <a href="http://www.ubuntu.com/download/alternative-downloads" target="_blank" rel="external">ubuntu-16.04-desktop-amd64.iso</a>)，因为已有Win7系统，此处选择“Install Ubuntu alongside Windows Boot Manager”，分区采用默认选择，语言选择English，安装完毕。</p>
<p><em>注：此时显示器VGA接口接到主板集成显卡接口上。</em><br><em>PS: or always plug VGA to Nvidia Titan X, and then set “nomodeset” in /etc/default/grub, then install nvidia drivers in tty…</em></p>
<p><strong>2.更新源</strong></p>
<pre><code>cd /etc/apt/
sudo cp sources<span class="class">.list</span> sources<span class="class">.list</span><span class="class">.bak</span>
sudo gedit sources.list
</code></pre><p>在sources.list文件头部添加如下源：</p>
<pre><code>deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
</code></pre><p>然后更新源和安装的包:</p>
<pre><code>sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>
sudo apt-<span class="built_in">get</span> upgrade
</code></pre><p><strong>3.安装NVIDIA显卡驱动</strong></p>
<p>采用ppa安装方式，没选择最新的nvidia-370，我选择了nvidia-367。</p>
<p>Ctrl+Alt+F1进入tty命令控制台，停止lightdm，然后开始安装驱动。</p>
<pre><code>sudo services lightdm stop

sudo<span class="instruction"> add-apt-repository </span>ppa:graphics-drivers/ppa
sudo apt-get update
sudo apt-get install nvidia-367
sudo apt-get install mesa-common-dev
sudo apt-get install freeglut3-dev
sudo reboot
</code></pre><p><em>将显示器VGA接口换到NVIDIA显卡上。</em></p>
<p>PS: If login loop, then Ctrl+Alt+F1, and then uninstall nvidia driver and reinstall again..</p>
<pre><code>sudo apt-get purge nvidia-*
sudo<span class="instruction"> add-apt-repository </span>ppa:graphics-drivers/ppa
sudo apt-get update 
sudo apt-get install nvidia-367
sudo apt-get install mesa-common-dev
sudo apt-get install freeglut3-dev
sudo reboot
</code></pre><p><strong>4.修改分辨率</strong></p>
<p>启动到界面之后发现分辨率只有1366x768，显示器适合1920x1080，采用xrandr并修改xorg.conf来解决。[或者，更容易的是采用一个HDMI的转接头来解决！]</p>
<pre><code>sudo gedit /etc/X11/xorg<span class="class">.conf</span>
修改如下：
HorizSync <span class="number">31.0</span> - <span class="number">84.0</span>
VertRefresh <span class="number">56.0</span>-<span class="number">77.0</span>
</code></pre><p>即最终的xorg.conf文件为：</p>
<pre><code><span class="title">Section "Device"    </span>
    Identifier <span class="string">"Configured Video Device"</span>
EndSection

<span class="title">Section "Monitor"</span>
    Identifier <span class="string">"Configured Monitor"</span>
    Horizsync 30-84
    Vertrefresh 56-77
EndSection

<span class="title">Section "Screen"</span>
Identifier <span class="string">"Default Screen"</span>
Monitor <span class="string">"Configured Monitor"</span>
Device <span class="string">"Configured Video Device"</span>
    SubSection <span class="string">"Display"</span>
        Modes <span class="string">"1920x1080"</span> <span class="string">"1360x768"</span> <span class="string">"1024x768"</span> <span class="string">"1152x864"</span>
    EndSubSection
EndSection        
</code></pre><p>注销系统再次登录后，选择适合的桌面分辨率即可。</p>
<p><strong>5.安装CUDA8.0</strong></p>
<p>到官网下载<a href="https://developer.nvidia.com/cuda-release-candidate-download" target="_blank" rel="external">cuda_8.0.44_linux.run</a>，复制到根目录下。</p>
<pre><code>sudo sh cuda_8.0.44_linux.<span class="command">run</span> <span class="comment">--tmpdir=/tmp/</span>
</code></pre><p>遇到问题：incomplete installation，然后执行</p>
<pre><code>sudo apt-<span class="built_in">get</span> install freeglut3-<span class="built_in">dev</span> build-essential libx11-<span class="built_in">dev</span> libxmu-<span class="built_in">dev</span> libxi-<span class="built_in">dev</span> libgl1-mesa-glx libglu1-mesa libglu1-mesa-<span class="built_in">dev</span>
sudo sh cuda_8.0.44_linux.run -silent -driver
</code></pre><p>注：此时安装过程中提示是否要安装NVIDIA驱动时选择no。其他选择yes或默认即可。</p>
<pre><code>Install NVIDIA Accelerated Graphics Driver <span class="keyword">for</span> Linux-x86_64 <span class="number">361.62</span>? <span class="params">(y)</span>es/<span class="params">(n)</span>o/<span class="params">(q)</span>uit: n
</code></pre><p>安装完毕后声明环境变量：</p>
<pre><code><span class="title">sudo</span> gedit ~/.bashrc
</code></pre><p>在.bashrc尾部添加如下内容：</p>
<pre><code>export <span class="constant">PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/bin</span><span class="variable">${</span><span class="constant">PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">PATH</span>}}
export <span class="constant">LD_LIBRARY_PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/lib</span>64<span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span>}}
</code></pre><p>测试下安装是否成功：</p>
<p>测试1：</p>
<pre><code>cd NVIDI<span class="built_in">A_CUDA</span>-<span class="number">8.0</span>_Samples/
nvidia-smi
</code></pre><p>输出：</p>
<pre><code>Tue Oct 18 15:20:34 2016       
+-----------------------------------------------------------------------------+
|<span class="string"> NVIDIA-SMI 367.44                 Driver Version: 367.44                    </span>|
|<span class="string">-------------------------------+----------------------+----------------------+
</span>|<span class="string"> GPU  Name        Persistence-M</span>|<span class="string"> Bus-Id        Disp.A </span>|<span class="string"> Volatile Uncorr. ECC </span>|
|<span class="string"> Fan  Temp  Perf  Pwr:Usage/Cap</span>|<span class="string">         Memory-Usage </span>|<span class="string"> GPU-Util  Compute M. </span>|
|<span class="string">===============================+======================+======================</span>|
|<span class="string">   0  GeForce GTX TIT...  Off  </span>|<span class="string"> 0000:01:00.0      On </span>|<span class="string">                  N/A </span>|
|<span class="string"> 22%   48C    P5    27W / 250W </span>|<span class="string">    169MiB / 12205MiB </span>|<span class="string">      1%      Default </span>|
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
|<span class="string"> Processes:                                                       GPU Memory </span>|
|<span class="string">  GPU       PID  Type  Process name                               Usage      </span>|
|<span class="string">=============================================================================</span>|
|<span class="string">    0      2421    G   /usr/lib/xorg/Xorg                             105MiB </span>|
|<span class="string">    0     10062    G   compiz                                          63MiB </span>|
+-----------------------------------------------------------------------------+
</code></pre><p>测试2：</p>
<pre><code>cd <span class="number">1</span>_Utilities/deviceQuery
make
<span class="attribute">...</span><span class="attribute">...</span><span class="built_in">..
</span><span class="built_in">.</span>/deviceQuery 
</code></pre><p>输出：</p>
<pre><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "GeForce GTX TITAN X"
  CUDA Driver Version / Runtime Version          8.0 / 8.0
  CUDA Capability Major/Minor version number:    5.2
  Total amount of global memory:                 12205 MBytes (12798197760 bytes)
  (24) Multiprocessors, (128) CUDA Cores/MP:     3072 CUDA Cores
  GPU Max Clock rate:                            1076 MHz (1.08 GHz)
  Memory Clock rate:                             3505 Mhz
  Memory Bus Width:                              384-bit
  L2 <span class="operator"><span class="keyword">Cache</span> <span class="keyword">Size</span>:                                 <span class="number">3145728</span> bytes
  Maximum Texture Dimension <span class="keyword">Size</span> (x,y,z)         <span class="number">1</span>D=(<span class="number">65536</span>), <span class="number">2</span>D=(<span class="number">65536</span>, <span class="number">65536</span>), <span class="number">3</span>D=(<span class="number">4096</span>, <span class="number">4096</span>, <span class="number">4096</span>)
  Maximum Layered <span class="number">1</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">1</span>D=(<span class="number">16384</span>), <span class="number">2048</span> layers
  Maximum Layered <span class="number">2</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">2</span>D=(<span class="number">16384</span>, <span class="number">16384</span>), <span class="number">2048</span> layers
  Total amount <span class="keyword">of</span> constant memory:               <span class="number">65536</span> bytes
  Total amount <span class="keyword">of</span> shared memory per block:       <span class="number">49152</span> bytes
  Total <span class="built_in">number</span> <span class="keyword">of</span> registers available per block: <span class="number">65536</span>
  Warp <span class="keyword">size</span>:                                     <span class="number">32</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per multiprocessor:  <span class="number">2048</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per block:           <span class="number">1024</span>
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a thread block (x,y,z): (<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">64</span>)
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a grid <span class="keyword">size</span>    (x,y,z): (<span class="number">2147483647</span>, <span class="number">65535</span>, <span class="number">65535</span>)
  Maximum memory pitch:                          <span class="number">2147483647</span> bytes
  Texture alignment:                             <span class="number">512</span> bytes
  <span class="keyword">Concurrent</span> copy <span class="keyword">and</span> kernel execution:          Yes <span class="keyword">with</span> <span class="number">2</span> copy <span class="keyword">engine</span>(s)
  Run <span class="keyword">time</span> <span class="keyword">limit</span> <span class="keyword">on</span> kernels:                     Yes
  Integrated GPU sharing Host Memory:            <span class="keyword">No</span>
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class="keyword">for</span> Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI <span class="keyword">Domain</span> ID / Bus ID / location ID:   <span class="number">0</span> / <span class="number">1</span> / <span class="number">0</span>
  Compute <span class="keyword">Mode</span>:
     &lt; <span class="keyword">Default</span> (multiple host threads can <span class="keyword">use</span> ::cudaSetDevice() <span class="keyword">with</span> device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver <span class="keyword">Version</span> = <span class="number">8.0</span>, CUDA Runtime <span class="keyword">Version</span> = <span class="number">8.0</span>, NumDevs = <span class="number">1</span>, Device0 = GeForce GTX TITAN X
Result = PASS</span>
</code></pre><p>测试3：</p>
<pre><code>cd <span class="built_in">..</span><span class="subst">/</span><span class="built_in">..</span>/<span class="number">5</span>_Simulations/nbody<span class="subst">/</span>
make
<span class="attribute">...</span><span class="attribute">...</span><span class="attribute">...</span>
<span class="built_in">.</span>/nbody <span class="attribute">-benchmark</span> <span class="attribute">-numbodies</span><span class="subst">=</span><span class="number">256000</span> <span class="attribute">-device</span><span class="subst">=</span><span class="number">0</span>
</code></pre><p>输出：</p>
<pre><code>mark -numbodies=<span class="number">256000</span> -device=<span class="number">0</span>
Run <span class="string">"nbody -benchmark [-numbodies=&lt;numBodies&gt;]"</span> <span class="keyword">to</span> measure performance.
-fullscreen       (<span class="command">run</span> n-body simulation <span class="keyword">in</span> fullscreen mode)
-fp64             (use double precision floating point values <span class="keyword">for</span> simulation)
-hostmem          (stores simulation data <span class="keyword">in</span> host memory)
-benchmark        (<span class="command">run</span> benchmark <span class="keyword">to</span> measure performance) 
-numbodies=&lt;N&gt;    (<span class="type">number</span> <span class="keyword">of</span> bodies (&gt;= <span class="number">1</span>) <span class="keyword">to</span> <span class="command">run</span> <span class="keyword">in</span> simulation) 
-device=&lt;d&gt;       (<span class="keyword">where</span> d=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2.</span>... <span class="keyword">for</span> <span class="keyword">the</span> CUDA device <span class="keyword">to</span> use)
-numdevices=&lt;i&gt;   (<span class="keyword">where</span> i=(<span class="type">number</span> <span class="keyword">of</span> CUDA devices &gt; <span class="number">0</span>) <span class="keyword">to</span> use <span class="keyword">for</span> simulation)
-compare          (compares simulation results <span class="property">running</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> default GPU <span class="keyword">and</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-cpu              (<span class="command">run</span> n-body simulation <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-tipsy=&lt;<span class="type">file</span>.bin&gt; (load a tipsy model <span class="type">file</span> <span class="keyword">for</span> simulation)

NOTE: The CUDA Samples are <span class="keyword">not</span> meant <span class="keyword">for</span> performance measurements. Results may vary when GPU Boost <span class="keyword">is</span> enabled.

&gt; Windowed mode
&gt; Simulation data stored <span class="keyword">in</span> video memory
&gt; Single precision floating point simulation
&gt; <span class="number">1</span> Devices used <span class="keyword">for</span> simulation
gpuDeviceInit() CUDA Device [<span class="number">0</span>]: <span class="string">"GeForce GTX TITAN X
&gt; Compute 5.2 CUDA device: [GeForce GTX TITAN X]
number of bodies = 256000
256000 bodies, total time for 10 iterations: 3104.433 ms
= 211.105 billion interactions per second
= 4222.091 single-precision GFLOP/s at 20 flops per interaction</span>
</code></pre><p><strong>6.安装OpenCV 3.1.0</strong></p>
<p>从官网下载zip源代码，解压到根目录下。<br>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> -y remove ffmpeg x264 libx264-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libopencv-<span class="built_in">dev</span> build-essential checkinstall cmake pkg-config yasm  libjpeg-<span class="built_in">dev</span> libjasper-<span class="built_in">dev</span> libavcodec-<span class="built_in">dev</span> libavformat-<span class="built_in">dev</span> libswscale-<span class="built_in">dev</span> libdc1394-<span class="number">22</span>-<span class="built_in">dev</span>  libgstreamer0.10-<span class="built_in">dev</span> libgstreamer-plugins-base0.10-<span class="built_in">dev</span> libv4l-<span class="built_in">dev</span> python-<span class="built_in">dev</span> python-numpy libtbb-<span class="built_in">dev</span> libqt4-<span class="built_in">dev</span> libgtk2.0-<span class="built_in">dev</span> libfaac-<span class="built_in">dev</span> libmp3lame-<span class="built_in">dev</span> libopencore-amrnb-<span class="built_in">dev</span> libopencore-amrwb-<span class="built_in">dev</span> libtheora-<span class="built_in">dev</span> libvorbis-<span class="built_in">dev</span> libxvidcore-<span class="built_in">dev</span> x264 v4l-utils ffmpeg libgtk2.0-<span class="built_in">dev</span>

cd opencv-<span class="number">3.1</span>.0
mkdir build   
cd build/
cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON ..
make -j8
sudo make install
</code></pre><p>遇到的错误：Errors</p>
<pre><code><span class="keyword">error</span>: ‘NppiGraphcutState’ has <span class="keyword">not</span> been declared
<span class="keyword">error</span>: ‘NppiGraphcutState’ <span class="keyword">does</span> <span class="keyword">not</span> <span class="property">name</span> a type
...
</code></pre><p>解决方法：(由于CUDA版本高于8.0，所以需要做如下修改。在源文件中找到“graphcuts.cpp”)</p>
<p>将：</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)</span>
</code></pre><p>改为:</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)  || (CUDART_VERSION &gt;= 8000)</span>
</code></pre><p>because graphcuts is not supported directly with CUDA8 anymore.</p>
<p>安装成功后配置环境：</p>
<pre><code>sudo <span class="keyword">sh</span> -c 'echo <span class="string">"/usr/local/lib"</span> &gt; /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/opencv.<span class="keyword">conf</span>'
sudo ldconfig
</code></pre><p>测试OpenCV安装是否成功：</p>
<pre><code><span class="built_in">mkdir</span> DisplayImage  
<span class="built_in">cd</span> DisplayImage 
gedit DisplayImage.cpp 
</code></pre><p>添加代码：</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> &lt;stdio.h&gt;  </span>
<span class="preprocessor">#<span class="keyword">include</span> &lt;opencv2/opencv.hpp&gt;  </span>
<span class="keyword">using</span> <span class="keyword">namespace</span> cv;  

<span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span>  
</span>{  
     <span class="keyword">if</span>(argc!= <span class="number">2</span>)  
     {  
               <span class="built_in">printf</span>(<span class="string">"usage:DisplayImage.out &lt;Image_Path&gt;\n"</span>);  
               <span class="keyword">return</span> -<span class="number">1</span>;  
     }  

     Mat image;  
     image= imread(argv[<span class="number">1</span>], <span class="number">1</span>);  

    <span class="keyword">if</span>(!image.data)  
    {  
               <span class="built_in">printf</span>(<span class="string">"Noimage data\n"</span>);  
               <span class="keyword">return</span> -<span class="number">1</span>;  
     }  

     namedWindow(<span class="string">"DisplayImage"</span>,CV_WINDOW_AUTOSIZE);  
     imshow(<span class="string">"DisplayImage"</span>,image);  

     waitKey(<span class="number">0</span>);  
     <span class="keyword">return</span> <span class="number">0</span>;  
}  
</code></pre><p>创建CMake文件：</p>
<pre><code>gedit CMakeLists<span class="class">.txt</span>  
</code></pre><p>添加内容：</p>
<pre><code><span class="function"><span class="title">cmake_minimum_required</span><span class="params">(VERSION <span class="number">2.8</span>)</span></span>  
<span class="function"><span class="title">project</span><span class="params">(DisplayImage)</span></span>  
<span class="function"><span class="title">find_package</span><span class="params">(OpenCV REQUIRED)</span></span>  
<span class="function"><span class="title">add_executable</span><span class="params">(DisplayImage DisplayImage.cpp)</span></span>  
<span class="function"><span class="title">target_link_libraries</span><span class="params">(DisplayImage ${OpenCV_LIBS})</span></span> 
</code></pre><p>编译：</p>
<pre><code>cmake .  
<span class="built_in">make</span> 
</code></pre><p>执行：</p>
<pre><code>./DisplayImage lena<span class="class">.jpg</span>  
</code></pre><p><strong>7.安装cudnn 5.1</strong></p>
<p>从官网下载<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cudnn-8.0-linux-x64-v5.1.tgz</a> for CUDA 8.0. 解压到当前目录：</p>
<pre><code><span class="tag">tar</span> <span class="tag">-zxvf</span> <span class="tag">cudnn-8</span><span class="class">.0-linux-x64-v5</span><span class="class">.1</span><span class="class">.tgz</span>
</code></pre><p>解压后的文件如下：</p>
<pre><code>cuda/include/cudnn<span class="class">.h</span>
cuda/lib64/libcudnn<span class="class">.so</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5.1</span>.<span class="number">5</span>
cuda/lib64/libcudnn_static.a
</code></pre><p>然后执行：</p>
<pre><code>sudo cp cuda<span class="regexp">/include/</span>cudnn.h <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/
sudo cp cuda<span class="regexp">/lib64/</span>libcudnn* <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/cudnn.h
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/libcudnn*
</code></pre><p><strong>8.安装MATLAB 2014a</strong></p>
<p>需要注意的是Ubuntu16.04 LTS的gcc版本为5.4，而Matlab2014a支持的是gcc4.7。</p>
<p>降级安装gcc/g++版本为4.7.x</p>
<p>下载gcc/g++ 4.7.x</p>
<pre><code>sudo apt-get <span class="operator"><span class="keyword">install</span> -y gcc-<span class="number">4.7</span>

sudo apt-<span class="keyword">get</span> <span class="keyword">install</span> -y g++-<span class="number">4.7</span></span>
</code></pre><p>链接gcc/g++实现降级</p>
<pre><code><span class="keyword">cd</span> /usr/bin

sudo <span class="keyword">rm</span> gcc

sudo ln -s gcc-4.7 gcc

sudo <span class="keyword">rm</span> <span class="keyword">g</span>++

sudo ln -s <span class="keyword">g</span>++-4.7 <span class="keyword">g</span>++
</code></pre><hr>
<p>升级 gcc 到 gcc-5版本</p>
<p>首先添加ppa到库：</p>
<pre><code>sudo<span class="instruction"> add-apt-repository </span>ppa:ubuntu-toolchain-r/test
sudo apt-get update
</code></pre><p>如果提示未安装，还需要先安装它的包：</p>
<pre><code>sudo apt-get <span class="operator"><span class="keyword">install</span> software-properties-common

sudo apt-<span class="keyword">get</span> <span class="keyword">upgrade</span>
sudo apt-<span class="keyword">get</span> <span class="keyword">install</span> gcc-<span class="number">5</span> g++-<span class="number">5</span></span>
</code></pre><p>（非必须）现在可以考虑刷新一下，否则locate等命令是找不到的：</p>
<pre><code><span class="title">sudo</span> updatedb &amp;&amp; sudo ldconfig
locate gcc
</code></pre><p>你会发现  gcc -v 显示出来的版本还是gcc-4.7的，因此需要更新一下链接：</p>
<pre><code>update-alternatives --install <span class="regexp">/usr/</span>bin<span class="regexp">/gcc gcc /u</span>sr<span class="regexp">/bin/g</span>cc-<span class="number">5</span> <span class="number">53</span> \
--slave <span class="regexp">/usr/</span>bin<span class="regexp">/g++ g++ /u</span>sr<span class="regexp">/bin/g</span>++-<span class="number">5</span> \
--slave <span class="regexp">/usr/</span>bin<span class="regexp">/gcc-ar gcc-ar /u</span>sr<span class="regexp">/bin/g</span>cc-ar-<span class="number">5</span> \
--slave <span class="regexp">/usr/</span>bin<span class="regexp">/gcc-nm gcc-nm /u</span>sr<span class="regexp">/bin/g</span>cc-nm-<span class="number">5</span> \
--slave <span class="regexp">/usr/</span>bin<span class="regexp">/gcc-ranlib gcc-ranlib /u</span>sr<span class="regexp">/bin/g</span>cc-ranlib-<span class="number">5</span>
</code></pre><p>=======================================================</p>
<p>用Crack文件中的install替换matlab2014安装目录下/java/jar/下的install文件，然后执行install程序</p>
<pre><code><span class="built_in">cd</span> <span class="string">"MatlabFolder"</span>
sudo ./install
</code></pre><p>注意：选择“不联网安装”；当出现密钥时，随意输入20个数字12345-67890-12345-67890即可；需要激活时选择不要联网激活，用Crack目录下的“license_405329_R2014a.lic”文件激活。</p>
<p>安装完成之后，将Crack/Linux目录下的libmwservices.so文件拷贝到/usr/local/MATLAB/R2014a/bin/glnxa64。</p>
<pre><code>cd ..
cd Crack<span class="regexp">/Linux/</span>
sudo cp libmwservices.so <span class="regexp">/usr/</span>local<span class="regexp">/MATLAB/</span>R2014a<span class="regexp">/bin/g</span>lnxa64
</code></pre><p>打开Matlab并激活：</p>
<pre><code><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/MATLAB/R2014a/bin
sudo ./matlab <span class="comment"># sudo不可缺少，否则选择激活文件后报错</span>
</code></pre><p>Test GPUdevice in Matlab:</p>
<pre><code>&gt;&gt; gpuDevice

ans = 

  CUDADevice <span class="keyword">with</span> properties:

                  Name: <span class="string">'GeForce GTX TITAN X'</span>
                 <span class="keyword">Index</span>: <span class="number">1</span>
     ComputeCapability: <span class="string">'5.2'</span>
        SupportsDouble: <span class="number">1</span>
         DriverVersion: <span class="number">8</span>
        ToolkitVersion: <span class="number">5.5000</span>
    MaxThreadsPerBlock: <span class="number">1024</span>
      MaxShmemPerBlock: <span class="number">49152</span>
    MaxThreadBlockSize: [<span class="number">1024</span> <span class="number">1024</span> <span class="number">64</span>]
           MaxGridSize: [<span class="number">2.1475</span>e+<span class="number">09</span> <span class="number">65535</span> <span class="number">65535</span>]
             SIMDWidth: <span class="number">32</span>
           TotalMemory: <span class="number">1.2796</span>e+<span class="number">10</span>
            FreeMemory: <span class="number">1.2475</span>e+<span class="number">10</span>
       MultiprocessorCount: <span class="number">24</span>
          ClockRateKHz: <span class="number">1076000</span>
           ComputeMode: <span class="string">'Default'</span>
      GPUOverlapsTransfers: <span class="number">1</span>
    KernelExecutionTimeout: <span class="number">1</span>
      CanMapHostMemory: <span class="number">1</span>
       DeviceSupported: <span class="number">1</span>
        DeviceSelected: <span class="number">1</span>
</code></pre><p><strong>9.Python</strong></p>
<p>选用Ubuntu16.04默认的安装和配置，python版本2.7.12.</p>
<p><strong>10.BLAS安装与配置</strong></p>
<p>BLAS（基础线性代数集合）是一个应用程序接口的标准。caffe官网上推荐了三种实现：ATLAS, MKL, OpenBLAS。其中ATLAS可以直接通过命令行安装。MKL是微软开发的商业工具包，面向科研和学生免费开放。申请学生版的<a href="https://software.intel.com/en-us/qualify-for-free-software/student" target="_blank" rel="external">Parallel Studio XE Cluster Edition</a>，下载parallel_studio_xe_2017.tgz。注意接收邮件中的序列号(<em>2HWS-34Z7S69B</em>)。</p>
<pre><code>tar zxvf parallel_studio_xe_2017.tgz   <span class="comment">#解压下载文件</span>
 chmod <span class="number">777</span> parallel_studio_xe_2017 -R   <span class="comment">#获取文件权限</span>
<span class="built_in">cd</span> parallel_studio_xe_2017/
 sudo ./install_GUI.sh
</code></pre><p>安装完成之后，进行相关文件的链接：</p>
<pre><code>sudo gedit /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/intel_mkl.<span class="keyword">conf</span>
</code></pre><p>添加库文件:</p>
<pre><code><span class="regexp">/opt/i</span>ntel<span class="regexp">/lib/i</span>ntel64
<span class="regexp">/opt/i</span>ntel<span class="regexp">/mkl/</span>lib<span class="regexp">/intel64</span>
</code></pre><p>编译链接使lib文件生效：</p>
<pre><code><span class="title">sudo</span> ldconfig    
</code></pre><p>如果选择安装ATLAS，在终端输入<code>sudo apt-get install libatlas-base-dev</code>即可。</p>
<p><strong>11.Caffe的安装与配置</strong></p>
<p>Caffe是由BVLC开发的一个深度学习框架，主要由贾扬清在UC Berkeley攻读PhD期间完成。参考官网上的<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">教程</a>以及Github上针对Ubuntu15.04和16.04的<a href="https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide" target="_blank" rel="external">教程</a>。从官方下载caffe源包<a href="https://github.com/BVLC/caffe" target="_blank" rel="external">caffe-master</a>。</p>
<p>安装库文件：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libboost-<span class="built_in">all</span>-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span> protobuf-compiler
</code></pre><p>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install -y build-essential cmake git pkg-config libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libopencv-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> protobuf-compiler libatlas-base-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> install --no-install-recommends libboost-<span class="built_in">all</span>-<span class="built_in">dev</span>
</code></pre><p>Python接口依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install <span class="keyword">python</span>-dev
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-pip
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-dev
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-numpy <span class="keyword">python</span>-scipy # (Python <span class="number">2.7</span> development <span class="keyword">files</span>)
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-dev
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-numpy <span class="keyword">python3</span>-scipy # (Python <span class="number">3.5</span> development <span class="keyword">files</span>)
</code></pre><p>在python文件夹内，使用root执行依赖项的检查与安装：</p>
<pre><code>sudo <span class="keyword">su</span>
<span class="keyword">cd</span> caffe-master/python
<span class="keyword">for</span> req <span class="keyword">in</span> $(<span class="keyword">cat</span> requirements.txt); <span class="keyword">do</span> pip install <span class="label">$req</span>; done
</code></pre><p>Makefile.config：</p>
<pre><code>cd ~/caffe-master
cp Makefile<span class="class">.config</span><span class="class">.example</span> Makefile.config
</code></pre><p>配置如下：</p>
<pre><code><span class="preprocessor">## Refer to http://caffe.berkeleyvision.org/installation.html</span>
<span class="preprocessor"># Contributions simplifying and improving our build system are welcome!</span>

<span class="preprocessor"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>
<span class="constant"> USE_CUDNN </span>:= <span class="number">1</span>

<span class="preprocessor"># CPU-only switch (uncomment to build without GPU support).</span>
<span class="preprocessor"># CPU_ONLY := 1</span>

<span class="preprocessor"># uncomment to disable IO dependencies and corresponding data layers</span>
 <span class="constant"> USE_OPENCV </span>:= <span class="number">1</span>
<span class="preprocessor"># USE_LEVELDB := 0</span>
<span class="preprocessor"># USE_LMDB := 0</span>

<span class="preprocessor"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span>
<span class="preprocessor">#    You should not set this flag if you will be reading LMDBs with any</span>
<span class="preprocessor">#    possibility of simultaneous read and write</span>
<span class="preprocessor"># ALLOW_LMDB_NOLOCK := 1</span>

<span class="preprocessor"># Uncomment if you're using OpenCV 3</span>
<span class="constant"> OPENCV_VERSION </span>:= <span class="number">3</span>

<span class="preprocessor"># To customize your choice of compiler, uncomment and set the following.</span>
<span class="preprocessor"># N.B. the default for Linux is g++ and the default for OSX is clang++</span>
<span class="preprocessor"># CUSTOM_CXX := g++</span>

<span class="preprocessor"># CUDA directory contains bin/ and lib/ directories that we need.</span>
CUDA_DIR := /usr/local/cuda
<span class="preprocessor"># On Ubuntu 14.04, if cuda tools are installed via</span>
<span class="preprocessor"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>
<span class="preprocessor"># CUDA_DIR := /usr</span>

<span class="preprocessor"># CUDA architecture setting: going with all of them.</span>
<span class="preprocessor"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span>
CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \
        -gencode arch=compute_20,code=sm_21 \
        -gencode arch=compute_30,code=sm_30 \
        -gencode arch=compute_35,code=sm_35 \
        -gencode arch=compute_50,code=sm_50 \
        -gencode arch=compute_50,code=compute_50

<span class="preprocessor"># BLAS choice:</span>
<span class="preprocessor"># atlas for ATLAS (default)</span>
<span class="preprocessor"># mkl for MKL</span>
<span class="preprocessor"># open for OpenBlas</span>
BLAS := mkl
<span class="preprocessor"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span>
<span class="preprocessor"># Leave commented to accept the defaults for your choice of BLAS</span>
<span class="preprocessor"># (which should work)!</span>
<span class="preprocessor"># BLAS_INCLUDE := /path/to/your/blas</span>
<span class="preprocessor"># BLAS_LIB := /path/to/your/blas</span>

<span class="preprocessor"># Homebrew puts openblas in a directory that is not on the standard search path</span>
<span class="preprocessor"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span>
<span class="preprocessor"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span>

<span class="preprocessor"># This is required only if you will compile the matlab interface.</span>
<span class="preprocessor"># MATLAB directory should contain the mex binary in /bin.</span>
 <span class="constant"> MATLAB_DIR </span>:= /usr/local/MATLAB/R2014a
<span class="preprocessor"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span>

<span class="preprocessor"># NOTE: this is required only if you will compile the python interface.</span>
<span class="preprocessor"># We need to be able to find Python.h and numpy/arrayobject.h.</span>
PYTHON_INCLUDE := /usr/include/python2.7 \
        /usr/local/lib/python2.7/dist-packages/numpy/core/include
<span class="preprocessor"># Anaconda Python distribution is quite popular. Include path:</span>
<span class="preprocessor"># Verify anaconda location, sometimes it's in root.</span>
<span class="preprocessor"># ANACONDA_HOME := $(HOME)/anaconda</span>
<span class="preprocessor"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>
        # $(ANACONDA_HOME)/include/python2.7 \
        # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

<span class="preprocessor"># Uncomment to use Python 3 (default is Python 2)</span>
<span class="preprocessor"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>
<span class="preprocessor"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>
<span class="preprocessor">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span>

<span class="preprocessor"># We need to be able to find libpythonX.X.so or .dylib.</span>
PYTHON_LIB := /usr/lib
<span class="preprocessor"># PYTHON_LIB := $(ANACONDA_HOME)/lib</span>

<span class="preprocessor"># Homebrew installs numpy in a non standard path (keg only)</span>
<span class="preprocessor"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span>
<span class="preprocessor"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span>

<span class="preprocessor"># Uncomment to support layers written in Python (will link against Python libs)</span>
 <span class="constant"> WITH_PYTHON_LAYER </span>:= <span class="number">1</span>

<span class="preprocessor"># Whatever else you find you need goes here.</span>
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include  /usr/include/hdf5/serial
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/share/OpenCV/<span class="number">3</span>rdparty/lib/

<span class="preprocessor"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span>
<span class="preprocessor"># INCLUDE_DIRS += $(shell brew --prefix)/include</span>
<span class="preprocessor"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span>

<span class="preprocessor"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span>
<span class="preprocessor"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span>
<span class="preprocessor"># USE_PKG_CONFIG := 1</span>

<span class="preprocessor"># N.B. both build and distribute dirs are cleared on `make clean`</span>
BUILD_DIR := build
DISTRIBUTE_DIR := distribute

<span class="preprocessor"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span>
<span class="preprocessor"># DEBUG := 1</span>

<span class="preprocessor"># The ID of the GPU that 'make runtest' will use to run unit tests.</span>
TEST_GPUID := <span class="number">0</span>

<span class="preprocessor"># enable pretty build (comment to see full commands)</span>
Q ?= @
</code></pre><p>在Makefile中配置：</p>
<pre><code><span class="label">LIBRARIES</span> += glog gflags protobuf <span class="keyword">boost_system </span><span class="keyword">boost_filesystem </span>m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs
</code></pre><p>hdf5的配置：官方说这对于Ubuntu 16.04是必须的。libhdf5的版本号需要根据实际来修改下。</p>
<pre><code>sudo find . -<span class="keyword">type</span> f -exec sed -i -<span class="keyword">e</span> 's^<span class="string">"hdf5.h"</span>^<span class="string">"hdf5/serial/hdf5.h"</span>^<span class="keyword">g</span>' -<span class="keyword">e</span> 's^<span class="string">"hdf5_hl.h"</span>^<span class="string">"hdf5/serial/hdf5_hl.h"</span>^<span class="keyword">g</span>' '{}' \;
<span class="keyword">cd</span> /usr/lib/x86_64-linux-gnu
sudo ln -s libhdf5_serial.<span class="keyword">so</span>.10.1.0 libhdf5.<span class="keyword">so</span>
sudo ln -s libhdf5_serial_hl.<span class="keyword">so</span>.10.0.2 libhdf5_hl.<span class="keyword">so</span>
</code></pre><p>编译：</p>
<pre><code><span class="keyword">cd</span> ~/caffe-master
<span class="keyword">make</span> clean
<span class="keyword">make</span> <span class="keyword">all</span> -j8
<span class="keyword">make</span> test -j8
<span class="keyword">make</span> runtest -j8
<span class="keyword">make</span> pycaffe -j8
<span class="keyword">make</span> matcaffe -j8
</code></pre><p>编译接口matcaffe时，有如下警告：</p>
<pre><code>Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0-6ubuntu1~16.04.2)'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
MEX completed successfully.
</code></pre><p>若OpenCV安装不正确则会在caffe编译过程中遇到如下错误：</p>
<pre><code><span class="regexp">/usr/</span>bin/<span class="string">ld:</span> cannot find -lopencv_imgcodecs
<span class="string">collect2:</span> <span class="string">error:</span> ld returned <span class="number">1</span> exit status
<span class="string">Makefile:</span><span class="number">566</span>: recipe <span class="keyword">for</span> target <span class="string">'.build_release/lib/libcaffe.so.1.0.0-rc3'</span> failed
<span class="string">make:</span> *** [.build_release<span class="regexp">/lib/</span>libcaffe.so.1.0.0-rc3] Error <span class="number">1</span>
</code></pre><p>MNIST测试：</p>
<pre><code>sh data/mnist/get_mnist<span class="class">.sh</span>  #数据预处理
sh examples/mnist/create_mnist<span class="class">.sh</span> #重建lmdb文件。Caffe支持多种数据格式: <span class="function"><span class="title">Image</span><span class="params">(.jpg, .png等)</span></span>,leveldb,lmdb,HDF5. 生成mnist-train-lmdb 和 mnist-train-lmdb文件夹，这里包含了lmdb格式的数据集
sh examples/mnist/train_lenet<span class="class">.sh</span> #训练mnist
</code></pre><p>输出：</p>
<pre><code>I<span class="number">1019 21:48</span>:<span class="number">30.078994</span> 20063 caffe.cpp:217] Using GPUs 0
I<span class="number">1019 21:48</span>:<span class="number">30.092034</span> 20063 caffe.cpp:222] GPU 0: GeForce GTX TITAN X
...
....
.....
I<span class="number">1019 21:48</span>:<span class="number">49.415398</span> 20063 solver.cpp:317] Iteration 10000, loss = <span class="number">0.00242468</span>
I<span class="number">1019 21:48</span>:<span class="number">49.415410</span> 20063 solver.cpp:337] Iteration 10000, Testing net (#0)
I<span class="number">1019 21:48</span>:<span class="number">49.479605</span> 20063 solver.cpp:404] Test net output #0: accuracy = 0.9914
I<span class="number">1019 21:48</span>:<span class="number">49.479625</span> 20063 solver.cpp:404] Test net output #1: loss = <span class="number">0.0284448</span> (* 1 = <span class="number">0.0284448</span> loss)
I<span class="number">1019 21:48</span>:<span class="number">49.479629</span> 20063 solver.cpp:322] Optimization Done.
I<span class="number">1019 21:48</span>:<span class="number">49.479632</span> 20063 caffe.cpp:254] Optimization Done.
</code></pre><p><strong>12.Caffe下Matlab接口Demo测试</strong></p>
<p>在使用Matlab运行caffe库时，即运行文件”caffe-master/matlab/demo/classification_demo.m”。遇到的错误信息如下：</p>
<pre><code>Invalid MEX-<span class="built_in">file</span> <span class="string">'caffe-master/matlab/+caffe/private/caffe_.mexa64'</span>: libcudart.so.8.0: cannot <span class="built_in">open</span> shared object <span class="built_in">file</span>: No such <span class="built_in">file</span> <span class="operator">or</span> <span class="built_in">directory</span>
</code></pre><p>错误原因是由于Matlab找不到caffe<em>.mexa64所依赖的所有库文件的路径，此时可以使用ldd命令来查看caffe\</em>.mexa64内库文件的地址：</p>
<p>//1. 在Ubuntu系统的命令终端</p>
<pre><code><span class="keyword">ldd</span> *caffe_.mexa64
</code></pre><p>结果输出的是库文件对应的地址，与下文相对的缺失的库文件的地址可在此找到：</p>
<pre><code>libcudart<span class="class">.so</span>.<span class="number">8.0</span> =&gt; /usr/local/cuda-<span class="number">8.0</span>/lib64/libcudart<span class="class">.so</span>.<span class="number">8.0</span>
libcublas<span class="class">.so</span>.<span class="number">8.0</span> =&gt; /usr/local/cuda-<span class="number">8.0</span>/lib64/libcublas<span class="class">.so</span>.<span class="number">8.0</span>
libcurand<span class="class">.so</span>.<span class="number">8.0</span> =&gt; /usr/local/cuda-<span class="number">8.0</span>/lib64/libcurand<span class="class">.so</span>.<span class="number">8.0</span>
libcudnn<span class="class">.so</span>.<span class="number">5</span> =&gt; /usr/local/cuda-<span class="number">8.0</span>/lib64/libcudnn<span class="class">.so</span>.<span class="number">5</span>
</code></pre><p>//2. 在Matlab命令窗口输入</p>
<pre><code>!<span class="keyword">ldd</span> *caffe_.mexa64
</code></pre><p>结果在Matlab窗口的输出信息中发现：</p>
<pre><code>libcudart<span class="class">.so</span>.<span class="number">8.0</span> =&gt; not found
libcublas<span class="class">.so</span>.<span class="number">8.0</span> =&gt; not found
libcurand<span class="class">.so</span>.<span class="number">8.0</span> =&gt; not found 
libcudnn<span class="class">.so</span>.<span class="number">5</span> =&gt; not found
</code></pre><p>解决方法：通过如下命令将默认路径链接到真实路径下：</p>
<pre><code>sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/cuda-8.0/</span>lib64<span class="regexp">/libcudart.so.8.0 /</span>usr<span class="regexp">/local/</span>MATLAB<span class="regexp">/R2014a/</span>sys<span class="regexp">/os/</span>glnxa64/libcudart.so.8.0
sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/cuda-8.0/</span>lib64<span class="regexp">/libcublas.so.8.0 /</span>usr<span class="regexp">/local/</span>MATLAB<span class="regexp">/R2014a/</span>sys<span class="regexp">/os/</span>glnxa64/libcublas.so.8.0
sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/cuda-8.0/</span>lib64<span class="regexp">/libcurand.so.8.0 /</span>usr<span class="regexp">/local/</span>MATLAB<span class="regexp">/R2014a/</span>sys<span class="regexp">/os/</span>glnxa64/libcurand.so.8.0
sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/cuda-8.0/</span>lib64<span class="regexp">/libcudnn.so.5 /</span>usr<span class="regexp">/local/</span>MATLAB<span class="regexp">/R2014a/</span>sys<span class="regexp">/os/</span>glnxa64/libcudnn.so.5
</code></pre><p>重新启动Matlab使之生效。</p>
<p>另外，运行此例需要下载CaffeNet模型（Please download CaffeNet from Model Zoo before you run this demo.）<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="external">https://github.com/BVLC/caffe/wiki/Model-Zoo</a> </p>
<p>|| <em>name: BVLC CaffeNet Model</em></p>
<p>|| <em>caffemodel: bvlc_reference_caffenet.caffemodel</em></p>
<p>|| <em>caffemodel_url: <a href="https://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel" target="_blank" rel="external">download</a></em></p>
<p>|| <em>license: unrestricted</em></p>
<p>详细说明可参见”caffe-master/models/bvlc_reference_caffenet”…</p>
<p><strong>参考：</strong></p>
<p><a href="http://www.2cto.com/os/201607/528798.html" target="_blank" rel="external">ubuntu14.04+cuda8.0（GTX1080）+caffe安装</a></p>
<p><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu-16-04-nvidia-gtx-1080-cuda-8" target="_blank" rel="external">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></p>
<p><a href="http://www.jianshu.com/p/74e9c8697372" target="_blank" rel="external">深度学习框架torch/caffe/tensor/mxnet安装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Ubuntu配置——Chrome XX-Net Sogou Nodejs Hexo" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/30/Ubuntu配置——Chrome XX-Net Sogou Nodejs Hexo/" class="article-date">
  	<time datetime="2016-12-30T01:59:46.957Z" itemprop="datePublished">2016-12-30</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Ubuntu 配置——Chrome，XX-Net，Sogou，Nodejs，Hexo</p>
<p>1.安装Chrome</p>
<pre><code>sudo wget https://repo.fdzh<span class="preprocessor">.org</span>/chrome/google-chrome<span class="preprocessor">.list</span> -P /etc/apt/sources<span class="preprocessor">.list</span>.d/
</code></pre><p>然后导入谷歌软件的公钥，用于下面步骤中对下载软件进行验证。命令将返回“OK”。</p>
<pre><code>wget -<span class="keyword">q</span> -O - http<span class="variable">s:</span>//<span class="keyword">dl</span>.google.<span class="keyword">com</span>/linux/linux_signing_key.pub  | sudo apt-key <span class="built_in">add</span> -
</code></pre><p>然后执行如下命令：</p>
<pre><code>sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>
sudo apt-<span class="built_in">get</span> install google-chrome-stable
</code></pre><p>2.配置XX-Net</p>
<p>按照<a href="https://github.com/XX-net/XX-Net/wiki/%E4%BD%BF%E7%94%A8Chrome%E6%B5%8F%E8%A7%88%E5%99%A8" target="_blank" rel="external">说明文档</a>进行下载配置.</p>
<p>3.搜狗输入法</p>
<p>下载地址：<a href="http://pinyin.sogou.com/linux/" target="_blank" rel="external">http://pinyin.sogou.com/linux/</a></p>
<p>安装：</p>
<pre><code><span class="tag">sudo</span> <span class="tag">dpkg</span> <span class="tag">-i</span> <span class="tag">sogoupinyin_2</span><span class="class">.1</span><span class="class">.0</span><span class="class">.0082_amd64</span><span class="class">.deb</span>
</code></pre><p>若出现问题安装错误时，或是由于缺少依赖，因此可执行如下语句：</p>
<pre><code>sudo apt-<span class="keyword">get</span> install -f
</code></pre><p>然后再次执行安装命令即可：</p>
<pre><code><span class="tag">sudo</span> <span class="tag">dpkg</span> <span class="tag">-i</span> <span class="tag">sogoupinyin_2</span><span class="class">.1</span><span class="class">.0</span><span class="class">.0082_amd64</span><span class="class">.deb</span>
</code></pre><p>4.搭建hexo</p>
<p>4.1安装nodejs</p>
<p>一个是通过ubuntu自带的包管理进行安装。不过它自带的版本可能过低，所以需要添加源：</p>
<pre><code>sudo<span class="instruction"> add-apt-repository </span>ppa:chris-lea/node.js
sudo apt-get update
sudo apt-get install nodejs
</code></pre><p>创建一个nodejs到node的软链接:</p>
<pre><code>sudo ln -s <span class="regexp">/usr/</span>bin<span class="regexp">/nodejs /u</span>sr<span class="regexp">/bin/</span>node
</code></pre><p>4.2安装Git</p>
<pre><code>sudo apt-<span class="keyword">get</span> install git
</code></pre><p>4.3安装hexo</p>
<pre><code><span class="preprocessor"># 创建目录</span>
mkdir hexo
<span class="preprocessor"># 切换目录</span>
cd hexo
<span class="preprocessor"># 全局安装 Hexo，需要最高权限，记得输入root密码</span>
sudo apt-<span class="keyword">get</span> install npm
sudo npm install -g hexo-cli
<span class="preprocessor"># 初始化 Hexo</span>
hexo init
</code></pre><p>安装插件</p>
<pre><code>npm <span class="operator"><span class="keyword">install</span> hexo-generator-<span class="keyword">index</span> <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-generator-archive <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-generator-category <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-generator-tag <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-<span class="keyword">server</span> <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-deployer-heroku <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-deployer-rsync <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-deployer-openshift <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-renderer-marked <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-renderer-stylus <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-generator-feed <span class="comment">--save</span>
npm <span class="keyword">install</span> hexo-generator-sitemap <span class="comment">--save</span></span>
</code></pre><p>测试安装成功</p>
<pre><code>hexo <span class="keyword">server</span>
</code></pre><p>4.4配置本机全局git环境<br>首先请使用邮箱注册github账号，否则会影响下面操作，记住你注册的邮箱。</p>
<pre><code>git config --global user<span class="class">.email</span> <span class="string">"you@example.com"</span>
git config --global user<span class="class">.name</span> <span class="string">"Your Name"</span>
</code></pre><p>生成SSH秘钥</p>
<pre><code><span class="comment"># -C后面跟住你在github的用户名邮箱，这样公钥才会被github认可</span>
 ssh-keygen -t rsa -C you<span class="property">@example</span>.com
 <span class="comment"># 回车后，输入一个文件夹名字，存储新的SSH 秘钥</span>
<span class="regexp">/home/username/</span>.ssh/id_rsa
<span class="comment"># 查看 公钥内容 稍后加入Github 账户的 sshkey中</span>
 less ~/.ssh/id_rsa.pub
</code></pre><p>将id_rsa.pub中的文本拷贝到github设置SSh。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Ubuntu backup" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/28/Ubuntu backup/" class="article-date">
  	<time datetime="2016-12-28T12:41:49.855Z" itemprop="datePublished">2016-12-28</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Ubuntu Backup</p>
<ol>
<li>backup</li>
</ol>
<p>然后打开终端，输入以下命令：</p>
<pre><code>sudo su
<span class="built_in">cd</span> /  <span class="comment">##转到根目录</span>
</code></pre><p>然後，下面就是我用来备份我的系统的完整的命令：</p>
<pre><code>tar -cvpzf /media/gwang/<span class="type">Work</span>/backup.tgz --exclude=/<span class="keyword">proc</span> --exclude=/lost+found --exclude=/mnt --exclude=/sys --exclude=/media --exclude=/tmp /
</code></pre><p>PS:</p>
<p>tar 是用来备份的程序</p>
<p>c - 新建一个备份文档</p>
<p>v - 详细模式， tar程序将在屏幕上实时输出所有信息。</p>
<p>p - 保存权限，并应用到所有文件。</p>
<p>z - 采用‘gzip’压缩备份文件，以减小备份文件体积。</p>
<p>f - 说明备份文件存放的路径， /media/sda7/backup.tgz 是本例子中备份文件名。这个备份文件备份的位置是其它分区，也就是原来的WIN分区中。因为我的根目录的空间不足，所以只有备份在其它的地方了。</p>
<p>—excloude - 排除指定目录,使其不被备份</p>
<ol>
<li><p>Linux 中美妙的事情之一就是在系统正在运行的情况下可以进行还原操作，而不需要启动光盘或者其他任何乱七八糟的东西。当然，如果您的系统已经崩溃，那您必须选择使用live CD，但是结果还是一样。</p>
<p> tar -xvpzf /media/gwang/Work/backup.tgz -C /</p>
</li>
</ol>
<p>如果您使用的是bz2压缩的：</p>
<pre><code>tar -xvpjf /media/gwang/Work/backup<span class="class">.tar</span><span class="class">.bz2</span> -C /
</code></pre><p>如果系统已经崩溃可以使用Live usb登录，然后</p>
<pre><code>mkdir <span class="regexp">/tmp/</span>root
mount <span class="regexp">/dev/</span>sdaX <span class="regexp">/tmp/</span>root

tar -xvpjf <span class="regexp">/media/g</span>wang<span class="regexp">/Work/</span>backup.tar.bz2 -C <span class="regexp">/tmp/</span>root
</code></pre><p>当然，恢复前可以先</p>
<pre><code>rm -rf <span class="regexp">/tmp/root/</span>* 
</code></pre><p>这样就删除根目录下的所有文件.</p>
<p>这个只是在本机上还原，如果是还原到别的机子上记得修改fstab文件。（可能还需要安装grub）</p>
<p>恢复命令结束时，别忘了重新创建那些在备份时被排除在外的目录：</p>
<pre><code><span class="preprocessor"># mkdir proc</span>
<span class="preprocessor"># mkdir lost+found</span>
<span class="preprocessor"># mkdir mnt</span>
<span class="preprocessor"># mkdir sys</span>
<span class="preprocessor"># mkdir media</span>
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习硬件配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/深度学习硬件配置/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.565Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/27/深度学习硬件配置/">深度学习硬件配置</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.收到NVIDIA资助的显卡GeForce Titan X (12GB)</p>
<p>2.其他硬件设备：</p>
<pre><code>CPU： Intel i<span class="number">7</span>-<span class="number">6700</span>
主板：华硕B150M-PLUS (LGA<span class="number">1151</span>)
内存：<span class="number">4</span>GB x <span class="number">2</span>
硬盘：希捷<span class="number">1</span><span class="keyword">TB</span>
电源：鑫谷Segotep GP700G（金牌认证、宽幅） 额定<span class="number">600</span><span class="keyword">W</span> 
机箱：Tt小板机箱
显卡接口转换器：DVI-&gt;VGI
</code></pre><p>3.备注</p>
<pre><code>深度学习对CPU的要求并不是特别高，根据实际情况选择。
主板建议还是选择一个好的品牌，预算充足，可以考虑<span class="keyword">X</span><span class="number">99</span>平台。
显卡GTX<span class="number">1080</span>比老Titan <span class="keyword">X</span>的性价比要高。
内存建议<span class="number">32</span>G(<span class="number">16</span>x<span class="number">2</span>)，近期内存价格暴涨。
硬盘，最好配SSD，用来存放软件和数据集，提升IO效率。
电源根据显卡和整机功率需求选择(TitanX+CPU等差不多<span class="number">350</span><span class="keyword">W</span>)。
机箱可扩展，散热好即可。
</code></pre><p>4.组装<br>    主板安装CPU及散热器，将主板安装到机箱内(面板接口要仔细)，然后按照电源、硬盘，最后将电源线依次插入相应供电接口位置。</p>
<p>5.参考</p>
<p><a href="http://www.jianshu.com/p/0198ad851b16" target="_blank" rel="external">个人深度学习环境搭建：主机配置与组装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习开发包" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/深度学习开发包/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.564Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/27/深度学习开发包/">Deep Learning Libraries by Language</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Python"><strong>Python</strong></h4><ol>
<li><p><a href="http://deeplearning.net/software/theano" target="_blank" rel="external">Theano</a>  is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built.<br> a. <a href="http://keras.io/" target="_blank" rel="external">Keras</a> is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU.<br> b. <a href="http://deeplearning.net/software/pylearn2/" target="_blank" rel="external">Pylearn2</a> is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano.<br> c. <a href="https://github.com/Lasagne/Lasagne" target="_blank" rel="external">Lasagne</a> is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism , focus and restraint principles.<br> d. <a href="https://github.com/mila-udem/blocks" target="_blank" rel="external">Blocks</a> a framework that helps you build neural network models on top of Theano.</p>
</li>
<li><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">Caffe</a> is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google’s DeepDream is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.</p>
</li>
<li><p><a href="https://github.com/dnouri/nolearn" target="_blank" rel="external">nolearn</a> contains a number of wrappers and abstractions around existing neural network libraries, most notably Lasagne, along with a few machine learning utility modules.</p>
</li>
<li><p><a href="http://radimrehurek.com/gensim/" target="_blank" rel="external">Gensim</a> is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms.</p>
</li>
<li><p><a href="http://chainer.org/" target="_blank" rel="external">Chainer</a> bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the flexible framework for Deep Learning.</p>
</li>
<li><p><a href="https://github.com/nitishsrivastava/deepnet" target="_blank" rel="external">deepnet</a> is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets.</p>
</li>
<li><p><a href="https://github.com/hannes-brt/hebel" target="_blank" rel="external">Hebel</a> is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping.</p>
</li>
<li><p><a href="https://github.com/dmlc/cxxnet" target="_blank" rel="external">CXXNET</a> is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction.</p>
</li>
<li><p><a href="https://github.com/andersbll/deeppy" target="_blank" rel="external">DeepPy</a> is a Pythonic deep learning framework built on top of NumPy.</p>
</li>
<li><p><a href="https://github.com/vishwa-raman/DeepLearning" target="_blank" rel="external">DeepLearning</a> is deep learning library, developed with C++ and python.</p>
</li>
<li><p><a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> is Nervana’s Python based Deep Learning framework.</p>
</li>
</ol>
<h4 id="Matlab"><strong>Matlab</strong></h4><ol>
<li><p><a href="https://github.com/sdemyanov/ConvNet" target="_blank" rel="external">ConvNet</a> Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs.</p>
</li>
<li><p><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="external">DeepLearnToolBox</a> is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets.</p>
</li>
<li><p><a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">cuda-convnet</a> is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm.</p>
</li>
<li><p><a href="http://www.vlfeat.org/matconvnet/" target="_blank" rel="external">MatConvNet</a>  is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs</p>
</li>
</ol>
<hr>
<p><strong>More</strong>… <a href="http://www.teglor.com/b/deep-learning-libraries-language-cm569/" target="_blank" rel="external">http://www.teglor.com/b/deep-learning-libraries-language-cm569/</a></p>
<hr>
<p><strong>Author</strong>: <a href="http://www.teglor.com/b/deep-learning-libraries-language-cm569/" target="_blank" rel="external">teglor</a> </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-机器学习数学基础" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/机器学习数学基础/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.563Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/27/机器学习数学基础/">机器学习之数学基础</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <pre><code><span class="built_in">Math</span> <span class="keyword">for</span> Machine Learning
</code></pre><p>author：Hal Daume III</p>
<p>The  goal  of  this  document  is  to  provide  a  “refresher”  on  continuous  mathematics  for  computer  science students.  It is by no means a rigorous course on these topics.  The presentation,  motivation,  etc.,  are all from a machine learning perspective.  The hope, however, is that it’s useful in other contexts.  The two majortopics covered are linear algebra and calculus (probability is currently left off).</p>
<p><a href="http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf" target="_blank" rel="external">PDF</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Math/">Math</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-文本挖掘" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/文本挖掘/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.562Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="文本挖掘">文本挖掘</h4><hr>
<p><strong>1.背景</strong></p>
<p>随着互联网的大规模普及和企业信息化程度的提高,文本信息的快速积累使公司、政府和科研机构在信息处理和使用中面临前所未有的挑战。一方面,互联网和企业信息系统每天都不断产生大量文本数据,这些文本资源中蕴含着许多有价值的信息;而另一方面因为技术手段的落后,从大量数据资源中获取需要的信息十分困难。人们迫切需要研究出方便有效的工具去从大规模文本信息资源中提取符合需要的简洁、精炼、可理解的知识,文本挖掘就是为解决这个问题而产生的研究方向。</p>
<p>传统的自然语言理解是对文本进行较低层次的理解,主要进行基于词、语法和语义信息的分析,并通过词在句子中出现的次序发现有意义的信息。在这一层次遇到的问题多与句法和语义歧义性相关。对文本较高层次的理解主要集中在研究如何从各种形式的文本和文本集中抽取隐含的模式和知识。文本高层次理解的对象可以是仅包含简单句子的单个文本也可以是多个文本组成的文本集,但是现有的技术手段虽然基本上解决了单个句子的分析问题,但是还很难覆盖所有的语言现象,特别是对整个段落<br>或篇章的理解还无从下手。</p>
<p>在19世纪早期发展起来的以统计技术为基础的数据挖掘技术已经发展的较为成熟,并在大规模结构化关系数据库上应用取得成功。将数据挖掘的成果用于分析以自然语言描述的文本,这种方法被称为文本挖掘(Text Mining, TM)或文本知识发现(Knowledge Discovery in Text, KDT)。与传统自然语言处理(Natural Lnaguage Proeessing, NLP)关注词语和句子的理解不同,文本挖掘的主要目标是在大规模文本集中发现隐藏的有意义的知识,即对文本集的理解和文本间关系的理解。因此,文本挖掘是自然语言处理和数据挖掘技术发展到一定阶段的产物。</p>
<p>在现实世界中,可获取的大部信息是以文本形式存储在文本数据库中的,由来自各种数据源的大量文档组成,如新闻文档、研究论文、书籍、数字图书馆、电子邮件和Web页面。由于电子形式的文本信息飞速增涨,文本挖掘已经成为信息领域的研究热点。</p>
<p><strong>2.定义</strong></p>
<p>文本数据库中存储的数据可能是高度非结构化的,如Web网页;也可能是半结构化的,如Email消息和一些XML网页;而其它的则可能是良结构化的。良结构化文本数据的典型代表是图书馆数据库中的文档,这些文档可能包含结构字段,如标题、作者、出版日期、长度、分类等等,也可能包含大量非结构化文本成分,如摘要和内容。通常,具有较好结构的文本数据库可以使用关系数据库系统实现,而对非结构化的文本成分需要采用特殊的处理方法对其进行转化。</p>
<p>文本挖掘是一个交叉的研究领域,它涉及到数据挖掘、信息检索、自然语言处理、机器学习等多个领域的内容,不同的研究者从各自的研究领域出发,对文本挖掘的含义有不同的理解,不同应用目的文本挖掘项目也各有其侧重点。因此,对文本挖掘的定义也有多种,其中被普遍认可的文本挖掘定义如下:</p>
<p>定义: 文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程,同时运用这些知识更好地组织信息以便将来参考。</p>
<p>直观的说,当数据挖掘的对象完全由文本这种数据类型组成时,这个过程就称为文本挖掘。</p>
<p>文本挖掘也称为文本数据挖掘[Hearst97]或文本知识发现[Fedlmna95],文本挖掘的主要目的是从非结构化文本文档中提取有趣的、重要的模式和知识。可以看成是基于数据库的数据挖掘或知识发现的扩展F[ayyda96,Simoudis96]。</p>
<p>文本挖掘是从数据挖掘发展而来,因此其定义与我们熟知的数据挖掘定义相类似。但与传统的数据挖掘相比,文本挖掘有其独特之处,主要表现在:<strong>文档本身是半结构化或非结构化的,无确定形式并且缺乏机器可理解的语义</strong>;而数据挖掘的对象以数据库中的结构化数据为主,并利用关系表等存储结构来发现知识。因此,<strong>有些数据挖掘技术并不适用于文本挖掘,即使可用,也需要建立在对文本集预处理的基础之上。</strong></p>
<p><strong>3.文本挖掘过程</strong></p>
<p>文本知识发现主要由以下步骤组成：</p>
<pre><code><span class="comment">文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">文本预处理</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">文档中间形式</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">文本挖掘</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">模式</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">评估与表示</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">知识</span>
</code></pre><p>1)文本预处理:</p>
<p>选取任务相关的文本并将其转化成文本挖掘工具可以处理的中间形式。</p>
<pre><code><span class="comment">文本集</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征抽取</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征选择</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">文本特征矩阵</span>
</code></pre><p>通常包括两个主要步骤:</p>
<p>(a)特征抽取:建立文档集的特征表示,将文本转化成一种类似关系数据且能表现文本内容的结构化形式,如信息检索领域经常采用的向量空间模型就是这样一种结构化模型。</p>
<p>(b)特征选择:一般说来结构化文本的特征空间维数较高,需要对其进行缩减,只保留对表达文本内容作用较大的一些特征。</p>
<p>2)文本挖掘:</p>
<p>在完成文本预处理后,可以利用机器学习、数据挖掘以及模式识别等方法提取面向特定应用目标的知识或模式。</p>
<p>3)模式评估与表示<br>最后一个环节是利用已经定义好的评估指标对获取的知识或模式进行评价。如果评价结果符合要求,就存储该模式以备用户使用;否则返回到前面的某个环节重新调整和改进,然后再进行新一轮的发现。</p>
<p><strong>4.研究现状</strong></p>
<p>在文本挖掘过程中,文本的特征表示是整个挖掘过程的基础;而<strong>关联分析、文本分类、文本聚类</strong>是三种最主要也是最基本的功能。</p>
<p><strong>4.1文本特征表示</strong></p>
<p>传统数据挖掘所处理的数据是结构化的,其特征通常不超过几百个;而非结构化或半结构化的文本数据转换成特征向量后,特征数可能高达几万甚至几十万。所以,文本挖掘面临的首要问题是如何在计算机中合理的表示文本。这种表示法既要包含足够的信息以反映文本的特征,又不至于太过庞大使学习算法无法处理。这就涉及到文本特征的抽取和选择。</p>
<p>文本特征指的是关于文本的元数据,可以分为描述性特征,如文本的名称、日期、大小、类型以及语义性特征,如文本的作者、标题、机构、内容。描述性特征易于获得,而语义特征较难获得。在文本特征表示方面,内容特征是被研究得最多的问题。</p>
<p>当文本内容被简单地看成由它所包含的基本语言单位(字、词、词组或短语等)组成的集合时,这些基本的语言单位被称为<strong>项(Term)</strong>。如果用出现在文本中的<br>项表示文本,那么这些项就是文本的特征。</p>
<p>对文本内容的特征表示主要有布尔模型、向量空间模型、概率模型和基于知识的表示模型。因为<strong>布尔模型和向量空间模型</strong>易于理解且计算复杂度较低,所以成为文本表示的主要工具。</p>
<p><strong>(1)特征抽取</strong></p>
<p>中文文档中的词与词之间不像英文文档那样具有分隔符,因此中、英文文档内容特征的提取步骤略有不同。</p>
<pre><code><span class="comment">英文文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">消除停词</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">词干抽取</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征词集合</span>
<span class="comment">中文文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">消除停词</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">词语切分</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征词集合</span>
</code></pre><p><strong>消除停词</strong>:<br>文本集有时包含一些没有意义但使用频率极高的词。这些词在所有文本中的频率分布相近,从而增加了文本之间的相似程度,给文本挖掘带来一定困难。解决这个问题的方法是用这些词构造一个停词表或禁用词表(stop word list)[Ricardo1991],在特征抽取过程中删去停词表中出现的特征词。</p>
<p>常用的停词包括虚词和实词两种,如</p>
<p>(i)虚词:英文中的”a,the,of,for,with,in,at,…”<br>中文中的”的,地,得,把,被,就…”</p>
<p>(ii)实词:数据库会议上的论文中的“数据库”一词,可视为停词。</p>
<p><strong>词干抽取</strong>:</p>
<p>定义: 令V(s)是由彼此互为语法变形的词组成的非空词集,V(s)的规范形式称为词干(stem)。</p>
<p>例如,如果V(s)={connected,connecting,connection,connections},那么s=connect 是V(s)的词干。</p>
<p><strong>词干抽取(stemming)有四种不同的策略:词缀排除(affix rermoval)、词干表查询(table lookup)、后继变化(successor variety)和n-gram</strong>。其中词缀排除最直观、简单且易于实现。多数词的变形是因添加后缀引起的,所以在基于词缀排除策略的抽取算法中后缀排除最为重要,Porter算法[Porter80]是后缀排除算法中最常用的一种。</p>
<p>词干抽取将具有不同词缀的词合并成一个词,降低文本挖掘系统中特征词的总数,从而提高了挖掘系统的性能。</p>
<p>当然,也有两点需要注意:</p>
<p>(1)词干抽取对文本挖掘性能的提高仅在基于统计原理的各种分析和挖掘技术下有效。在进行涉及语义和语法的自然语言处理时,不适宜采用词干抽取技术。</p>
<p>(2)词干抽取对文本挖掘或信息检索准确性的影响至今没有令人信服的结论,因此许多搜索引擎和文本挖掘系统不使用任何词干抽取算法。</p>
<p><strong>汉语切分</strong>:</p>
<p>汉语的分词问题己经基本解决,并出现了多种分词方法。这些分词方法可以分为两类:一类是理解式分词法,即利用汉语的语法知识、语义知识及心理学知识进行分词;另一类是机械式分词法,一般以分词词典为依据,通过文本中的汉字串和词表中的词逐一匹配完成词语切分。第一类分词方法算法复杂,实际应用中经常采用的是第二类分词方法。机械式分词法主要有正向最大匹配法,逆向最<br>大匹配法,逐词遍历法。</p>
<p>由于词典的容量有限,在大规模真实文本处理中,会遇到许多词典中未出现的词,即未登录词。未登录现象是影响分词准确率的重要原因。为解决这个问题,人们提出利用N-gram语言模型进行词项划分[周01a,01b],从而摆脱基于词典的分词方法对词典的依赖。与基于词典的分词方法不同,基于N-gram技术得到的词项不一定具有实际意义。</p>
<p>例如:“文本挖掘”的所有N-gram项为:</p>
<pre><code><span class="number">1</span>-<span class="string">gram:</span>文,本,挖,掘
<span class="number">2</span>-<span class="string">gram:</span>文本,本挖,挖掘
<span class="number">3</span>-<span class="string">gram:</span>文本挖,本挖掘
<span class="number">4</span>-<span class="string">gram:</span>文本挖掘
</code></pre><p>其中除1-gram是单字外,2-gram中的“本挖”,3-gram中的“文本挖”,“本挖掘”都不具有实际意义。</p>
<p><strong>(2)特征选择</strong></p>
<p>特征选择也称特征子集选择或特征集缩减。经过特征抽取获得的特征词数量很多,有时达数万个特征。如此多的特征对许多文本挖掘方法,如文本分类、聚类、文本关联分析来说未必都是有意义的;而过大的特征空间还会严重影响文本挖掘的效率,因此选择适当的特征子集十分必要。</p>
<p>通常采用机器学习的方法进行文本特征选择。虽然机器学习中有许多选取特征子集的算法,但有些算法复杂且效率低下,不适于处理庞大的文本特征集。</p>
<p>国外对特征选择的研究较多[Mladenic99,Mladenic03,Lewis92,Liu96],特别是已有专门针对文本分类特征选择方法的比较研究[Yang97]。国内对这一问题的研究以跟踪研究为主,集中在将国外现有特征评估函数用于中文文本特征选择[周<br>02]及对其进行改进[李99]。</p>
<p><strong>4.2基于关键字的关联分析</strong></p>
<p>文本数据一旦被转化成结构化中间形式后,这种中间形式就作为文本挖掘过程的基础。</p>
<p>与关系数据库中关联规则的挖掘方法类似,基于关键词的关联规则产生过程包括两个阶段:</p>
<p><em>关联挖掘阶段</em>:<br>这一阶段产生所有的支持度大等于最小支持度闭值的关键词集,即频繁项集。</p>
<p><em>规则生成阶段</em>:<br>利用前一阶段产生的频繁项集构造满足最小置信度约束的关联规则。</p>
<p>Feldman等人实现了基于上述思想的文本知识发现系统KDT[Feldman96]、FACT[Feldman97],KDT系统在Reuter22173语料集中发现的关联规则示例:</p>
<pre><code>[<span class="constant">Iran,Nicaragua,Usa]</span>-&gt;<span class="constant">Reagan </span><span class="number">6</span>/<span class="number">1.00</span>
[gold,copper]-&gt;<span class="constant">Canada </span><span class="number">5</span>/<span class="number">0</span>.<span class="number">556</span>
[gold,silver]-&gt;<span class="constant">USA </span><span class="number">19</span>/<span class="number">0</span>.<span class="number">692</span>
</code></pre><p>根据不同的挖掘需要,可以利用不同的挖掘方法,如关联挖掘、最大模式挖掘或层次关联挖掘,完成相应的文本分析任务。</p>
<p><strong>4.3文本分类</strong></p>
<p>文本分类是文本挖掘中一项非常重要的任务,也是国内外研究较多的一种挖掘技术。在机器学习中分类称作有监督学习或有教师归纳,其目的是提出一个分类函数或分类模型(也称作分类器),该模型能把数据库中的数据项映射到给定类别中的一个。</p>
<p>一般来讲,文本分类需要四个步骤:</p>
<p>(1)获取训练文本集:训练文本集由一组经过预处理的文本特征向量组成,每个训练文本(或称训练样本)有一个类别标号;</p>
<p>(2)选择分类方法并训练分类模型:文本分类方法有统计方法、机器学习方法、神经网络方法等等。在对待分类样本进行分类前,要根据所选择的分类方法,利用训练集进行训练并得出分类模型;</p>
<p>(3)用导出的分类模型对其它待分类文本进行分类;</p>
<p>(4)根据分类结果评估分类模型。</p>
<p>另外需要注意的是,文本分类的效果一般和数据集本身的特点有关。有的数据集包含噪声,有的存在缺失值,有的分布稀疏,有的字段或属性间相关性强。目前,普遍认为不存在某种方法能适合于各种特点的数据[Yang99a,Yang99b]。</p>
<p>随着nIetmet技术的发展和普及,在线文本信息迅速增加,文本分类成为处理和组织大量文本数据的关键技术。而近二十多年来计算机软、硬件技术的发展和自然语言处理、人工智能等领域的研究进展为文本自动分类提供了技术条件和理论基础。迄今为止,文本分类研究已经取得了很大的进展,提出了一系列有效的方法,其中分类质量较好的有k最近邻(k-Nearest Neighbor,KNN),[Iwayama95,Yang97,Yang99a]、支持向量机(Support Vector Machine,SVM)[Joachims98]、朴素贝叶斯(Naive Bayes,NB)[Lewis94,Chakra97,Lewis98]。1998年文献[Liu98]提出了基于关联规则的分类方法CBA,此后陆续有人进行这方面的研究,如CAEP[Dong99]、JEP[Li00a,Li00c]、DeEPs[Li0Ob]、CMAR[Li01]和用于文本分类的ARC[Zaiane02]。</p>
<p>国内对中文文本自动分类的研究起步较晚,尽管己有一些研究成果[李04,姚03,邹99,周01a],但由于尚没有通用的标准语料和评价方法,很难对这些成果进行比较。而对基于关联规则的文本分类的研究在国内还未见到。</p>
<p><strong>4.4文本聚类</strong></p>
<p>文本聚类是根据文本数据的不同特征,将其划分为不同数据类的过程。其目的是要使同一类别的文本间的距离尽可能小,而不同类别的文本间的距离尽可能的大。主要的聚类方法有统计方法、机器学习方法、神经网络方法和面向数据库的方法。在统计方法中,聚类也称聚类分析,主要研究基于几何距离的聚类。在机器学习中聚类称作无监督学习或无教师归纳。聚类学习和分类学习的不同主要在于:分类学习的训练文本或对象具有类标号,而用于聚类的文本没有类标号,由聚类学习算法自动确定。</p>
<p>传统的聚类方法在处理高维和海量文本数据时的效率不很理想,原因是:<br>(1)传统的聚类方法对样本空间的搜索具有一定的盲目性;<br>(2)在高维很难找到适宜的相似度度量标准。</p>
<p>虽然,文本聚类用于海量文本数据时存在不足。但与文本分类相比,文本聚类可以直接用于不带类标号的文本集,避免了为获得训练文本的类标号所花费的代价。根据聚类算法无需带有类标号样本这一优势,Nigam等人提出从带有和不带有类标号的混合文本中学习分类模型的方法[Ngiam98]。其思想是利用聚类技术减少分类方法对有标号训练样本的需求,减轻手工标记样本类别所需的工作<br>量,这种方法也称为半监督学习。</p>
<p>文本聚类包括以下四个步骤:</p>
<p>(1)获取结构化的文本集。</p>
<p>结构化的文本集由一组经过预处理的文本特征向量组成。从文本集中选取的特征好坏直接影响到聚类的质量。如果选取的特征与聚类目标无关,那么就难以得到良好的聚类结果。对于聚类任务,合理的特征选择策略应是使同类文本在特征空间中相距较近,异类文本相距较远。</p>
<p>(2)执行聚类算法,获得聚类谱系图。聚类算法的目的是获取能够反映特征空间样本点之间的“抱团”性质。</p>
<p>(3)选取合适的聚类阈值。在得到聚类谱系图后,领域专家凭借经验,并结合具体的应用场合确定阈值。阈值确定后,就可以直接从谱系图中得到聚类结果。</p>
<p>目前,常见的聚类算法可以分成以下几类[Han01]:</p>
<p>(1)平面划分法:对包含n个样本的样本集构造样本集的k个划分,每个划分表示一个聚簇。常见的划分聚类算法有k-均值算法,k-中心点算法,CLARANS算法。</p>
<p>(2)层次聚类法:层次聚类法对给定的样本集进行层次分解。根据层次分解方向的不同可分为凝聚层次聚类和分裂层次聚类。凝聚法也称为自底向上的方法,如AGNES;分裂法也称自顶向下的方法,如DIANA、CURE、BIRCH、Chameleon。</p>
<p>(3)基于密度的方法:多数平面划分法使用距离度量样本间的相似程度,因此只能发现球状簇,难以发现任意形状簇。基于密度的聚类法根据样本点临近区域的密度进行聚类,使在给定区域内至少包含一定数据的样本点。DBSCAN就是一个具有代表性的基于密度的聚类算法。</p>
<p>(4)基于网格的方法:采用多分辨率的网格数据结构,将样本空间量化为数量有限的网格单元,所有聚类操作都在网格上进行,如STING算法。</p>
<p>(5)基于模型的方法:为每个簇假定一个模型,然后通过寻找样本对给定模型的最佳拟合进行聚类。</p>
<p>有些聚类算法集成多种算法的思想,因此难以将其划归到上述类别中的一类,如CLIQUE综合了密度和网格两种聚类方法。</p>
<p>文本聚类有着广泛的应用,比如可以用来:</p>
<p>(1)改进信息检索系统的查全率和查准率[Ricardo99];</p>
<p>(2)用于文本集浏览[Cutting92];</p>
<p>(3)搜索引擎返回的相关文本的组织[Zamir97];</p>
<p>(4)自动产生文本集的类层次结构[Koller97]。在带有类标号的文本集上发现自然聚类[Aggarwal99],然后利用自然聚类改进文本分类器。</p>
<p><strong>5.文本挖掘与相近领域的关系</strong></p>
<p><strong>5.1自然语言处理与文本挖掘的区别</strong></p>
<p>文本挖掘与自然语言处理有着千丝万缕的联系,但也存在明显的不同:</p>
<p>(1)<strong>文本挖掘通过归纳推理</strong>发现知识,而传统的自然语言处理多采用<strong>演绎推理</strong>的方法,很少使用归纳推理方法。</p>
<p>(2)文本挖掘在大规模文本集而不是少数文本中发现知识,其目的不在于改善对文本的理解而是发现文本中的关系。虽然自然语言处理的两个新兴领域:信息检索(Information Retrieval,IR)和信息提取(Information Extraction,IE)也是以大规模文本集为对象,但只要使用严格的演绎推理,那么就不能称作文本挖掘。主要原因是它们没有发现任何知识,只是发现符合某种约束条件的文本而不是知识本身。</p>
<pre><code>[<span class="link_label">比较</span>][<span class="link_reference">方法不同</span>][<span class="link_label">目标不同</span>][<span class="link_reference">对象范围不同</span>]
自然语言处理：[<span class="link_label">演绎推理方法</span>][<span class="link_reference">更好的理解文本</span>][<span class="link_label">以一篇或少数文本为研究对象，发现表示文本特点的关系</span>]
文本挖掘：[<span class="link_label">归纳推理方法</span>][<span class="link_reference">更好的使用文本</span>][<span class="link_label">以大量文本组成的文本集为研究对象，在文本集中发现文本间或文本集中词与词之间的关系</span>]
</code></pre><p>1)信息检索与文本挖掘</p>
<p>信息检索是与数据库技术并行发展多年的领域,其中以文本为对象的文本信息检索以非结构或半结构化数据为处理对象,研究大量文本的信息组织和检索问题。</p>
<p>文本信息检索主要发现与用户检索要求(如关键词)相关的文本。例如,基于关键词的文本检索使用相关度量计算文本与用户查询间的相关性并按相关程度高低排序获得的文档。</p>
<p>近年来,基于自然语言处理技术发展起来的智能检索技术包含了对歧义信息的检索处理,如“苹果”,究竟是指水果还是电脑品牌;“华人”与“中华人民共和国”的区分,这类检索通过歧义知识描述库、全文索引、上下文分析以及用户相关反馈等技术实现文本信息检索的智能化。与文本挖掘不同,智能信息检索仍然只是关注从文本集中更有效地识别和提取相关文档,而不发现任何新的信息或知识。</p>
<p>2)信息提取与文本挖掘</p>
<p>信息提取(IE)是指提取文本集中预定义的事件或任务信息的过程,例如关于恐怖事件信息的提取,可以包括事件时间,地点,恐怖分子,受害者,恐怖分子采用的手段等等。其目的在于发现文本中的结构模式。主要过程是先根据需要确定结构模式,然后从文本中提取相应的知识填进该结构模式。文本挖掘任务则与之正好相反,它需要自动发现那些IE中给定的模式。</p>
<p><strong>5.2文本挖掘与相关领域的交叉</strong></p>
<p>虽然以上介绍的研究领域与文本挖掘存在明显的不同,但它们在某种程度上也存在交叉。最典型的交叉就是通过技术和方法的互相借鉴为各自领域提供新的有效的方法,如许多文本挖掘系统中采用的预处理方法就是最先在信息检索领域中提出并使用的。除此之外,还有其它的例子,如:</p>
<p>(1)基于文本挖掘的汉语词性自动标注</p>
<p>利用文本挖掘研究词及词性的序列模式对词性的影响是非常有新意的研究,这与人在根据上下文对词性进行判断的方法是一致的,不但根据上下文的词、词性,而且可以根据二者的组合来判断某个词的词性。</p>
<p>国内从数据挖掘的角度对汉语文本词性标注规则的获取进行了研究[李01]。其方法是在统计语料规模较大的情况下,利用关联规则发现算法发现词性标注规则。只要规则的置信度足够高,获得的规则就可以用来处理兼类词的情况。该过程完全是自动的,而获取的规则在表达上是明确的,同时又是隐含在数据中、用户不易发现的。</p>
<p>(2)基于信息抽取的文本挖掘</p>
<p>为将非结构化的自然语言文档表示成结构化形式以便直接利用传统的数据挖掘技术来进行文本挖掘。已有多种结构化方法被提出,如前面提到的文本特征表示方法就是最典型的一种。此外,随着信息抽取技术的不断发展[Freitag98,Clifton99],它在文本挖掘领域扮演着日益重要的角色。信息抽取的主要任务是从自然语言文本集中查找特别的数据段,然后将非结构化文档转化为结构化的数据库,以便更容易地理解文本。基于信息抽取<br>的文本挖掘系统框架[Nahm01]:</p>
<pre><code>Text-&gt;{Information Extraction-&gt;DB-&gt;KDD}-&gt;<span class="keyword">Rule</span> Base
</code></pre><p>在这个系统中,IE模块负责在原始文本中捕获特别的数据段,并生成数据库提供给知识发现模块进一步挖掘。</p>
<p><strong>5.3文本挖掘技术在Email处理方面的应用</strong></p>
<p>由于Email文档和普通文档之间有许多相似之处,所以可以将挖掘普通文档涉及的技术和方法用于Email信息挖掘。目前,有许多关于利用文本挖掘技术有效地组织和分析Email信息的研究。例如,通过分析Email的语言和作者性别群进行计算机取证[Rajman97];将Email信息的结构特征和语言学特征与SVM结合进行作者身份鉴别。</p>
<p>文本挖掘技术除了用于组织Email信息外,还可以用于对Email消息进行分类。如:利用朴素贝叶斯算法[Rennie00]、Rocchio算法、SVM方法和Bayesian方法[sahami98,Andr00,Sakkis01]对Email信息进行分类和过滤。此外,文献[Cohen96]和[Lewis94]则提出两个基于规则的系统,这两个系统都是利用文本挖掘技术来分类Email信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-数据挖掘界领军人物谢邦昌" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/数据挖掘界领军人物谢邦昌/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.561Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>好文丨数据挖掘界领军人物谢邦昌：深度剖析Data Mining </p>
<p>谢邦昌</p>
<p>深度剖析Data Mining</p>
<p><strong>简介</strong><br>谢邦昌教授，是台北医学大学医务管理学系研究所暨大数据研究中心及管理学院主任，也是数据挖掘界领军人物及世界知名统计学家，他对数据挖掘的定义是：Data Mining是从巨大数据仓储中找出有用信息的一种过程与技术。</p>
<p><strong>Data Mining主要功能</strong></p>
<p>Data Mining实际应用功能可分为三大类六分项来说明：Classification和Clustering属于分类区隔类；Regression和Time-series属于推算预测类；Association和Sequence则属于序列规则类。</p>
<p>Classification是根据一些变量的数值做计算，再依照结果作分类。（计算的结果最后会被分类为几个少数的离散数值，例如将一组数据分为 “可能会响应” 或是 “可能不会响应” 两类）。</p>
<p>Classification常被用来处理如前所述之邮寄对象筛选的问题。我们会用一些根据历史经验已经分类好的数据来研究它们的特征，然后再根据这些特征对其他未经分类或是新的数据做预测。</p>
<p>这些我们用来寻找特征的已分类数据可能是来自我们的现有的客户数据，或是将一个完整数据库做部份取样，再经由实际的运作来测试；譬如利用一个大型邮寄对象数据库的部份取样来建立一个Classification Model，再利用这个Model来对数据库的其它数据或是新的数据作分类预测。</p>
<p>Clustering用在将数据分群，其目的在于将群间的差异找出来，同时也将群内成员的相似性找出来。Clustering与Classification不同的是，在分析前并不知道会以何种方式或根据来分类。所以必须要配合专业领域知识来解读这些分群的意义。</p>
<p>Regression是使用一系列的现有数值来预测一个连续数值的可能值。若将范围扩大亦可利用Logistic Regression来预测类别变量，特别在广泛运用现代分析技术如类神经网络或决策树理论等分析工具，推估预测的模式已不在止于传统线性的局限，在预测的功能上大大增加了选择工具的弹性与应用范围的广度。</p>
<p>Time-Series Forecasting与Regression功能类似，只是它是用现有的数值来预测未来的数值。两者最大差异在于Time-Series所分析的数值都与时间有关。Time-Series Forecasting的工具可以处理有关时间的一些特性，譬如时间的周期性、阶层性、季节性以及其它的一些特别因素（如过去与未来的关连性）。</p>
<p>Association是要找出在某一事件或是数据中会同时出现的东西。举例而言，如果A是某一事件的一种选择，则B也出现在该事件中的机率有多少。（例如：如果顾客买了火腿和柳橙汁，那么这个顾客同时也会买牛奶的机率是85%。）</p>
<p>Sequence Discovery与Association关系很密切，所不同的是Sequence Discovery中事件的相关是以时间因素来作区隔（例如：如果A股票在某一天上涨12%，而且当天股市加权指数下降，则B股票在两天之内上涨的机率是 68%） 。</p>
<p><strong>目前业界最常用的Data Mining分析工具</strong></p>
<p>Data Mining工具市场大致可分为三类：</p>
<ol>
<li><p>一般分析目的用的软件包：<br>SAS Enterprise Miner<br>Microsoft SQL Server 2005 – 2008<br>IBM Intelligent Miner<br>Unica PRW<br>SPSS Clementine<br>SGI MineSet<br>Oracle Darwin<br>Angoss KnowledgeSeeker<br>Statistica</p>
</li>
<li><p>针对特定功能或产业而研发的软件：<br>KD1（针对零售业）<br>Options &amp; Choices（针对保险业）<br>HNC（针对信用卡诈欺或呆帐侦测）<br>Unica Model 1（针对营销业）</p>
</li>
<li><p>整合DSS（Decision Support Systems）/OLAP/Data Mining的大型分析系统：<br>Cognos Scenario and Business Objects</p>
</li>
</ol>
<p><strong>对于刚刚接触Data Mining的人来说，怎样把它学好？</strong></p>
<p>先从问题着手，Domain Knowledge 是很重要的具体应重视三方面的问题：</p>
<ol>
<li><p>强调需求，重视过程和结果。虽然统计学和数据挖掘一样，都是在寻求实际数据解决方案的过程中成长起来的，然而统计学家更关注模型，运用数据仅仅是为了发现新的模型，而数据挖掘则更强调知识的价值，模型是用来发现知识的工具。强调需求，重视过程和结果才能实现统计创新。</p>
</li>
<li><p>借鉴机器学习的特点，提炼方法，以算法的形式体现方法。统计学早已脱离正态的传统框架发展方法。但是，由于统计最新的可以被直接使用的成果太少，不仅阻碍了人们对统计方法的运用，甚至造成对先进统计方法的不甚了解。数据挖掘的兴起，为统计学与信息技术的结合带来了发展的契机。计算机技术将成为继数学之后，又一推动统计学发展的强大工具。</p>
</li>
<li><p>发挥统计软件的优势。许多“傻瓜”统计软件的设计，更适合统计学家研究使用，任何一个初通统计的数据分析员要想通过软件来进行数据分析，都极有可能由于对数据涵义的不求甚解，导致脱离实际的统计模型的滥用，数据挖掘软件也是如此；Clementine、SQL Server 2005及SAS和S-plus被设计为可以通过编程来调节软件的默认属性，用这样的软件工作可以增强统计研究者的算法意识；最后，统计软件为统计研究的目的，在图形和可视化方面的互动操作，应该在数据挖掘的软件中体现这一思想，因为它可以帮助数据分析员理解高维数据复杂的结构。</p>
</li>
</ol>
<p>从数据挖掘在国际上的发展来看，数据挖掘的研究重点已从提出概念和发现方法，转向系统应用和方法创新上，研究注重多种发现策略和技术的集成，以及多种学科之间的相互渗透，数据挖掘技术迫切需要系统、科学的理论体系作为其发展的有力支撑。</p>
<p>最近，由经验统计方法和人工智能相结合而产生的衍生技术，如分类回归树（Classification And Regression Tree, 简称CART），卡方自动交互探测法（Chi-square Automatic Interaction Detector，简称CHAID）等前沿方法，以算法的形式展示了统计和信息技术结合发展的新方向。这些都预示着数据挖掘技术与统计学的集成已成为必然的趋势。</p>
<p>我们坚信，随着统计学与现代信息技术的融合，在方法上不断进行新的探索，一定会为统计学和数据挖掘未来的发展开辟一片新的天地。</p>
<p><strong>Web Mining 和Data Mining的区别</strong></p>
<p>如果将Web视为CRM的一个新的Channel，则Web Mining便可单纯看做Data Mining应用在网络数据的泛称。</p>
<p>该如何测量一个网站是否成功？哪些内容、优惠、广告是人气最旺的？主要访客是哪些人？什么原因吸引他们前来？如何从堆积如山之大量由网络所得数据中找出让网站运作更有效率的操作因素？以上种种皆属Web Mining 分析之范畴。</p>
<p>Web Mining 不仅只限于一般较为人所知的log file分析，除了计算网页浏览率以及访客人次外，举凡网络上的零售、财务服务、通讯服务、政府机关、医疗咨询、远距教学等等，只要由网络连结出的数据库够大够完整，所有Off-Line可进行的分析，Web Mining都可以做，甚或更可整合Off-Line及On-Line的数据库，实施更大规模的模型预测与推估，毕竟凭借因特网的便利性与渗透力再配合网络行为的可追踪性与高互动特质，一对一营销的理念是最有机会在网络世界里完全落实的。</p>
<p><strong>整体而言，Web Mining具有以下特性</strong></p>
<ol>
<li><p>资料收集容易且不引人注意，所谓凡走过必留下痕迹，当访客进入网站后的一切浏览行为与历程都是可以立即被纪录的；</p>
</li>
<li><p>以交互式个人化服务为终极目标，除了因应不同访客呈现专属设计的网页之外，不同的访客也会有不同的服务；</p>
</li>
<li><p>可整合外部来源数据让分析功能发挥地更深更广，除了log file、cookies、会员填表数据、在线调查数据、在线交易数据等由网络直接取得的资源外，结合实体世界累积时间更久、范围更广的资源，将使分析的结果更准确也更深入。</p>
</li>
<li><p>利用Data Mining技术建立更深入的访客数据剖析，并赖以架构精准的预测模式，以期呈现真正智能型个人化的网络服务，是Web Mining努力的方向。</p>
</li>
<li><p>Data Warehousing（资料仓储） 和Data Mining 之间的关系若将Data Warehousing比喻作矿坑，Data Mining就是深入矿坑采矿的工作。毕竟Data Mining不是一种无中生有的魔术，也不是点石成金的炼金术，若没有够丰富完整的数据，是很难期待Data Mining能挖掘出什么有意义的信息的。</p>
</li>
</ol>
<p>要将庞大的数据转换成为有用的信息，必须先有效率地收集信息。随着科技的进步，功能完善的数据库系统就成了最好的收集资料的工具。「数据仓储」，简单地说，就是搜集来自其它系统的有用数据，存放在一整合的储存区内。所以其实就是一个经过处理整合，且容量特别大的关系型数据库，用以储存决策支持系统（Design Support System）所需的数据，供决策支持或数据分析使用。从信息技术的角度来看，数据仓储的目标是在组织中，在正确的时间，将正确的数据交给正确的人。</p>
<p>许多人对于Data Warehousing和Data Mining时常混淆，不知如何分辨。其实，数据仓储是数据库技术的一个新主题，在数据科技日渐普及下，利用计算机系统帮助我们操作、计算和思考，让作业方式改变，决策方式也跟着改变。数据仓储本身是一个非常大的数据库，它储存着由组织作业数据库中整合而来的数据，特别是指从在线交易系统OLTP（On-Line Transactional Processing）所得来的数据。</p>
<p>将这些整合过的数据置放于数据仓储中，而公司的决策者则利用这些数据作决策；但是，这个转换及整合数据的过程，是建立一个数据仓储最大的挑战。因为将作业中的数据转换成有用的的策略性信息是整个数据仓储的重点。综上所述，数据仓储应该具有这些数据：整合性数据（integrated data）、详细和汇总性的数据(detailed and summarized data)、历史数据、解释数据的数据。</p>
<p>从数据仓储挖掘出对决策有用的信息与知识，是建立数据仓储与使用Data Mining的最大目的，两者的本质与过程是两码子事。</p>
<p>换句话说，数据仓储应先行建立完成，Data Mining才能有效率的进行，因为数据仓储本身所含数据是干净(不会有错误的数据参杂其中）、完备，且经过整合的。因此两者关系或许可解读为「 Data Mining是从巨大数据仓储中找出有用信息的一种过程与技术」。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-安装lpsolve库 for MATLAB" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/12/27/安装lpsolve库 for MATLAB/" class="article-date">
  	<time datetime="2016-12-27T05:56:58.560Z" itemprop="datePublished">2016-12-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/27/安装lpsolve库 for MATLAB/">安装lpsolve库 for MATLAB</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>lpsolve是sourceforge下的一个开源项目，它的介绍如下： </p>
<p><em>Mixed Integer Linear Programming (MILP) solver lp_solve solves pure linear, (mixed) integer/binary, semi-cont and special ordered sets (SOS) models.lp_solve is written in ANSI C and can be compiled on many different platforms like Linux and WINDOWS </em>.</p>
<p>lpsolve是一个混合整数线性规划求解器，可以求解纯线性、（混合）整数/二值、半连续和特殊有序集模型。并且经过实际验证，有极高的求解效率。 </p>
<p><a href="http://sourceforge.net/projects/lpsolve/?source=directory" target="_blank" rel="external">sourceforge主页</a></p>
<p>1.在Windows x64和Matlab环境下使用lpsolve</p>
<p>需要在网址<a href="https://sourceforge.net/projects/lpsolve/files/lpsolve/5.5.2.5/" target="_blank" rel="external">lpsolve_5.5.2.5</a> 提供的文件列表中下载文件lp_solve_5.5.2.5_MATLAB_exe_win64.zip。或者下载文件lp_solve_5.5.2.5_source.tar.gz自行编译dll。</p>
<p>注：在Matlab下运行示例报错：</p>
<pre><code>Error <span class="keyword">using</span> mxlpsolve
Failed <span class="keyword">to</span> initialise lpsolve <span class="keyword">library</span>.
</code></pre><p>参考Using lpsolve from MATLAB: <a href="http://web.mit.edu/lpsolve/doc/MATLAB.htm" target="_blank" rel="external">http://web.mit.edu/lpsolve/doc/MATLAB.htm</a>给出的说明，需要将编译得到的mxlpsolve.dll拷贝到 WINDOWS\system32文件夹下。</p>
<p>2.在MacOS下使用lpsolve</p>
<p>参考<a href="https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/" target="_blank" rel="external">Using lp_solve in Java with Mac OS X</a>的配置说明，下载源文件lp_solve_5.5.2.5_source.tar.gz，然后跳转到lp_solve_5.5/lpsolve55文件夹内，并执行ccc.osx：</p>
<pre><code><span class="keyword">cd</span> lp_solve_5.5/lpsolve55
<span class="keyword">sh</span> ccc.osx
</code></pre><p>生成的文件所在的文件夹：lpsolve55/bin/osx64/，将此文件夹内生成的两个文件liblpsolve55.dylib，liblpsolve55.a拷贝到/usr/local/lib文件夹内即可：</p>
<pre><code>sudo cp liblpsolve55<span class="class">.a</span> liblpsolve55<span class="class">.dylib</span> /usr/local/lib
</code></pre><p>测试demo：</p>
<pre><code><span class="keyword">cd</span> lp_solve_5.5/demo
<span class="keyword">sh</span> ccc
./demo
</code></pre><p>3.在MacOS的Matlab中需要文件mxlpsolve.mexmaci64</p>
<p>需要从lpsolve主页下载源文件lp_solve_5.5.2.5_MATLAB_source.tar.gz，解压缩后，在Matlab中执行文件Makefile.m，期间需要添加各种头文件：</p>
<pre><code>lp_Hash<span class="class">.h</span>
lp_lib<span class="class">.h</span>
lp_matrix<span class="class">.h</span>
lp_mipbb<span class="class">.h</span>
lp_SOS<span class="class">.h</span>
lp_types<span class="class">.h</span>
lp_utils.h
</code></pre><p>可下载lp_solve_5.5.2.5_dev_ux64.tar.gz并从中获取即可。</p>
<p>4.举例</p>
<p>mxlpsove.m是建模的核心函数，一个线性规划模型的所有配置和求解都是通过这个函数完成的。lp_maker.m和lp_solve.m是对mxlpsolve.m的高层包装，简化了模型建立和求解的过程。例如用lpsolve求解数学规划问题：</p>
<p>$$ \max 4x_1+2x_2+x_3\\<br>s.t.~ 2x_1+x_2\le 1\\<br>x_1+2x_3\le 2\\<br>x_1+x_2+x_3=1\\<br>0\le x_1\le1\\<br>0\le x_2\le1\\<br>0\le x_3\le2$$</p>
<p>相应的Matlab语句为：</p>
<pre><code>f = [<span class="number">4</span> <span class="number">2</span> <span class="number">1</span>];
A = [<span class="number">2</span> <span class="number">1</span> <span class="number">0</span>; <span class="number">1</span> <span class="number">0</span> <span class="number">2</span>; <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>];
b = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">1</span>];
l = [ <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>];
u = [ <span class="number">1</span> <span class="number">1</span> <span class="number">2</span>];
lp=mxlpsolve<span class="comment">('make_lp', 1, 3)</span>;
mxlpsolve<span class="comment">('set_verbose', lp, 3)</span>;
mxlpsolve<span class="comment">('set_obj_fn', lp, f)</span>;
mxlpsolve<span class="comment">('add_constraint', lp, A(1, :)</span>, <span class="number">1</span>, b<span class="comment">(1)</span>); 
mxlpsolve<span class="comment">('add_constraint', lp, A(2, :)</span>, <span class="number">1</span>, b<span class="comment">(2)</span>); 
mxlpsolve<span class="comment">('add_constraint', lp, A(3, :)</span>, <span class="number">0</span>, b<span class="comment">(3)</span>; 
mxlpsolve<span class="comment">('set_lowbo', lp, l)</span>; 
mxlpsolve<span class="comment">('set_upbo', lp, u)</span>; 
mxlpsolve<span class="comment">('write_lp', lp, 'a.lp')</span>; 
mxlpsolve<span class="comment">('get_mat', lp, 1, 2)</span> 
mxlpsolve<span class="comment">('solve', lp)</span> 
mxlpsolve<span class="comment">('get_objective', lp)</span> 
mxlpsolve<span class="comment">('get_variables', lp)</span> 
mxlpsolve<span class="comment">('get_constraints', lp)</span> 
mxlpsolve<span class="comment">('delete_lp', lp)</span>
</code></pre><p>重要函数说明：</p>
<p><strong>lp_solve</strong></p>
<pre><code>LP_SOLVE  Solves mixed <span class="built_in">integer</span> linear programming problems.

SYNOPSIS: [obj,x,duals] = lp_solve(f,a,b,e,vlb,vub,xint,scalemode,keep)

  solves the MILP problem

          max v = f<span class="comment">'*x</span>
            a*x &lt;&gt; b
              vlb &lt;= x &lt;= vub
              x(int) are <span class="built_in">integer</span>

ARGUMENTS: The first four arguments are required:

        f: n vector <span class="keyword">of</span> coefficients <span class="keyword">for</span> a linear objective <span class="keyword">function</span>.
        a: m <span class="keyword">by</span> n matrix representing linear constraints.
        b: m vector <span class="keyword">of</span> right sides <span class="keyword">for</span> the inequality constraints.
        e: m vector that determines the sense <span class="keyword">of</span> the inequalities:
                  e(i) = -<span class="number">1</span>  ==&gt; Less Than
                  e(i) =  <span class="number">0</span>  ==&gt; <span class="keyword">Equals</span>
                  e(i) =  <span class="number">1</span>  ==&gt; Greater Than
      vlb: n vector <span class="keyword">of</span> lower bounds. <span class="keyword">If</span> empty <span class="keyword">or</span> omitted,
           <span class="keyword">then</span> the lower bounds are <span class="keyword">set</span> <span class="keyword">to</span> zero.
      vub: n vector <span class="keyword">of</span> upper bounds. May be omitted <span class="keyword">or</span> empty.
     xint: vector <span class="keyword">of</span> <span class="built_in">integer</span> variables. May be omitted <span class="keyword">or</span> empty.
scalemode: scale flag. <span class="keyword">Off</span> <span class="keyword">when</span> <span class="number">0</span> <span class="keyword">or</span> omitted.
     keep: Flag <span class="keyword">for</span> keeping the lp problem after it<span class="comment">'s been solved.</span>
           <span class="keyword">If</span> omitted, the lp will be deleted <span class="keyword">when</span> solved.

   OUTPUT: A nonempty output <span class="keyword">is</span> returned <span class="keyword">if</span> a solution <span class="keyword">is</span> found:

      obj: Optimal value <span class="keyword">of</span> the objective <span class="keyword">function</span>.
        x: Optimal value <span class="keyword">of</span> the decision variables.
    duals: solution <span class="keyword">of</span> the dual problem.

    Example <span class="keyword">of</span> usage. <span class="keyword">To</span> create <span class="keyword">and</span> solve following lp-model:

max: -x1 + <span class="number">2</span> x2;
C1: <span class="number">2</span>x1 + x2 &lt; <span class="number">5</span>;
-<span class="number">4</span> x1 + <span class="number">4</span> x2 &lt;<span class="number">5</span>;

int x2,x1;
The following command can be used:

&gt;&gt; [obj, x]=lp_solve([-<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>; -<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [], [], [<span class="number">1</span>, <span class="number">2</span>])

obj =

         <span class="number">3</span>

x =

        <span class="number">1</span>
         <span class="number">2</span>
</code></pre><p><strong>lp_maker</strong>　　
　　</p>
<pre><code>LP_MAKER  Makes mixed <span class="built_in">integer</span> linear programming problems.
SYNOPSIS: lp_handle = lp_maker(f,a,b,e,vlb,vub,xint,scalemode,setminim)
      make the MILP problem
        max v = f<span class="comment">'*x</span>
          a*x &lt;&gt; b
            vlb &lt;= x &lt;= vub
            x(int) are <span class="built_in">integer</span>

   ARGUMENTS: The first four arguments are required:
            f: n vector <span class="keyword">of</span> coefficients <span class="keyword">for</span> a linear objective <span class="keyword">function</span>.
            a: m <span class="keyword">by</span> n matrix representing linear constraints.
            b: m vector <span class="keyword">of</span> right sides <span class="keyword">for</span> the inequality constraints.
            e: m vector that determines the sense <span class="keyword">of</span> the inequalities:
                      e(i) &lt; <span class="number">0</span>  ==&gt; Less Than
                      e(i) = <span class="number">0</span>  ==&gt; <span class="keyword">Equals</span>
                      e(i) &gt; <span class="number">0</span>  ==&gt; Greater Than
          vlb: n vector <span class="keyword">of</span> non-negative lower bounds. <span class="keyword">If</span> empty <span class="keyword">or</span> omitted,
               <span class="keyword">then</span> the lower bounds are <span class="keyword">set</span> <span class="keyword">to</span> zero.
          vub: n vector <span class="keyword">of</span> upper bounds. May be omitted <span class="keyword">or</span> empty.
         xint: vector <span class="keyword">of</span> <span class="built_in">integer</span> variables. May be omitted <span class="keyword">or</span> empty.
    scalemode: Autoscale flag. <span class="keyword">Off</span> <span class="keyword">when</span> <span class="number">0</span> <span class="keyword">or</span> omitted.
     setminim: <span class="keyword">Set</span> maximum lp <span class="keyword">when</span> this flag <span class="keyword">equals</span> <span class="number">0</span> <span class="keyword">or</span> omitted.

   OUTPUT: lp_handle <span class="keyword">is</span> an <span class="built_in">integer</span> handle <span class="keyword">to</span> the lp created.
Example <span class="keyword">of</span> usage. <span class="keyword">To</span> create following lp-model:

max: -x1 + <span class="number">2</span> x2;
C1: <span class="number">2</span>x1 + x2 &lt; <span class="number">5</span>;
-<span class="number">4</span> x1 + <span class="number">4</span> x2 &lt;<span class="number">5</span>;

int x2,x1;
The following command can be used:

&gt;&gt; lp=lp_maker([-<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>; -<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [], [], [<span class="number">1</span>, <span class="number">2</span>])

lp =

     <span class="number">0</span>
<span class="keyword">To</span> solve the model <span class="keyword">and</span> <span class="keyword">get</span> the solution:

&gt;&gt; mxlpsolve(<span class="comment">'solve', lp)</span>

ans =

     <span class="number">0</span>

&gt;&gt; mxlpsolve(<span class="comment">'get_objective', lp)</span>

ans =

     <span class="number">3</span>

&gt;&gt; mxlpsolve(<span class="comment">'get_variables', lp)</span>

ans =

     <span class="number">1</span>
     <span class="number">2</span>
</code></pre><p>注意：Don’t forget to free the handle and its associated memory when you are done:</p>
<pre><code><span class="prompt">&gt;&gt;</span> mxlpsolve(<span class="string">'delete_lp'</span>, lp);
</code></pre><p>5.参考</p>
<p><a href="https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/" target="_blank" rel="external">https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/</a></p>
<p><a href="http://web.mit.edu/lpsolve/doc/MATLAB.htm" target="_blank" rel="external">http://web.mit.edu/lpsolve/doc/MATLAB.htm</a></p>
<p><a href="http://www.cnblogs.com/kane1990/p/3428129.html" target="_blank" rel="external">http://www.cnblogs.com/kane1990/p/3428129.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Matlab/">Matlab</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Gang Wang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>