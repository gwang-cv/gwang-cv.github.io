<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Hello World</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hello World">
<meta property="og:url" content="http://gwang-cv.github.io/index.html">
<meta property="og:site_name" content="Hello World">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hello World">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Gang Wang</a></h1>
		</hgroup>

		
		<p class="header-subtitle">a computer vision researchGO</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>Über</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Home</a></li>
				        
							<li><a href="/archives">Archives</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
					        
								<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/DeepLearning/" style="font-size: 16.67px;">DeepLearning</a> <a href="/tags/ML/" style="font-size: 10px;">ML</a> <a href="/tags/Mac/" style="font-size: 20px;">Mac</a> <a href="/tags/Math/" style="font-size: 10px;">Math</a> <a href="/tags/Matlab/" style="font-size: 10px;">Matlab</a> <a href="/tags/Python/" style="font-size: 13.33px;">Python</a> <a href="/tags/Researcher/" style="font-size: 10px;">Researcher</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">Hello, I&#39;m Gang Wang. This is my blog, enjoy it.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Gang Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Gang Wang</h1>
			</hgroup>
			
			<p class="header-subtitle">a computer vision researchGO</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Home</a></li>
		        
					<li><a href="/archives">Archives</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
			        
						<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-文本挖掘" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/15/文本挖掘/" class="article-date">
  	<time datetime="2016-11-15T08:24:05.000Z" itemprop="datePublished">2016-11-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="文本挖掘">文本挖掘</h4><hr>
<p><strong>1.背景</strong></p>
<p>随着互联网的大规模普及和企业信息化程度的提高,文本信息的快速积累使公司、政府和科研机构在信息处理和使用中面临前所未有的挑战。一方面,互联网和企业信息系统每天都不断产生大量文本数据,这些文本资源中蕴含着许多有价值的信息;而另一方面因为技术手段的落后,从大量数据资源中获取需要的信息十分困难。人们迫切需要研究出方便有效的工具去从大规模文本信息资源中提取符合需要的简洁、精炼、可理解的知识,文本挖掘就是为解决这个问题而产生的研究方向。</p>
<p>传统的自然语言理解是对文本进行较低层次的理解,主要进行基于词、语法和语义信息的分析,并通过词在句子中出现的次序发现有意义的信息。在这一层次遇到的问题多与句法和语义歧义性相关。对文本较高层次的理解主要集中在研究如何从各种形式的文本和文本集中抽取隐含的模式和知识。文本高层次理解的对象可以是仅包含简单句子的单个文本也可以是多个文本组成的文本集,但是现有的技术手段虽然基本上解决了单个句子的分析问题,但是还很难覆盖所有的语言现象,特别是对整个段落<br>或篇章的理解还无从下手。</p>
<p>在19世纪早期发展起来的以统计技术为基础的数据挖掘技术已经发展的较为成熟,并在大规模结构化关系数据库上应用取得成功。将数据挖掘的成果用于分析以自然语言描述的文本,这种方法被称为文本挖掘(Text Mining, TM)或文本知识发现(Knowledge Discovery in Text, KDT)。与传统自然语言处理(Natural Lnaguage Proeessing, NLP)关注词语和句子的理解不同,文本挖掘的主要目标是在大规模文本集中发现隐藏的有意义的知识,即对文本集的理解和文本间关系的理解。因此,文本挖掘是自然语言处理和数据挖掘技术发展到一定阶段的产物。</p>
<p>在现实世界中,可获取的大部信息是以文本形式存储在文本数据库中的,由来自各种数据源的大量文档组成,如新闻文档、研究论文、书籍、数字图书馆、电子邮件和Web页面。由于电子形式的文本信息飞速增涨,文本挖掘已经成为信息领域的研究热点。</p>
<p><strong>2.定义</strong></p>
<p>文本数据库中存储的数据可能是高度非结构化的,如Web网页;也可能是半结构化的,如Email消息和一些XML网页;而其它的则可能是良结构化的。良结构化文本数据的典型代表是图书馆数据库中的文档,这些文档可能包含结构字段,如标题、作者、出版日期、长度、分类等等,也可能包含大量非结构化文本成分,如摘要和内容。通常,具有较好结构的文本数据库可以使用关系数据库系统实现,而对非结构化的文本成分需要采用特殊的处理方法对其进行转化。</p>
<p>文本挖掘是一个交叉的研究领域,它涉及到数据挖掘、信息检索、自然语言处理、机器学习等多个领域的内容,不同的研究者从各自的研究领域出发,对文本挖掘的含义有不同的理解,不同应用目的文本挖掘项目也各有其侧重点。因此,对文本挖掘的定义也有多种,其中被普遍认可的文本挖掘定义如下:</p>
<p>定义: 文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程,同时运用这些知识更好地组织信息以便将来参考。</p>
<p>直观的说,当数据挖掘的对象完全由文本这种数据类型组成时,这个过程就称为文本挖掘。</p>
<p>文本挖掘也称为文本数据挖掘[Hearst97]或文本知识发现[Fedlmna95],文本挖掘的主要目的是从非结构化文本文档中提取有趣的、重要的模式和知识。可以看成是基于数据库的数据挖掘或知识发现的扩展F[ayyda96,Simoudis96]。</p>
<p>文本挖掘是从数据挖掘发展而来,因此其定义与我们熟知的数据挖掘定义相类似。但与传统的数据挖掘相比,文本挖掘有其独特之处,主要表现在:<strong>文档本身是半结构化或非结构化的,无确定形式并且缺乏机器可理解的语义</strong>;而数据挖掘的对象以数据库中的结构化数据为主,并利用关系表等存储结构来发现知识。因此,<strong>有些数据挖掘技术并不适用于文本挖掘,即使可用,也需要建立在对文本集预处理的基础之上。</strong></p>
<p><strong>3.文本挖掘过程</strong></p>
<p>文本知识发现主要由以下步骤组成：</p>
<pre><code><span class="comment">文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">文本预处理</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">文档中间形式</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">文本挖掘</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">模式</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="title">[</span><span class="comment">评估与表示</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">知识</span>
</code></pre><p>1)文本预处理:</p>
<p>选取任务相关的文本并将其转化成文本挖掘工具可以处理的中间形式。</p>
<pre><code><span class="comment">文本集</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征抽取</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征选择</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">文本特征矩阵</span>
</code></pre><p>通常包括两个主要步骤:</p>
<p>(a)特征抽取:建立文档集的特征表示,将文本转化成一种类似关系数据且能表现文本内容的结构化形式,如信息检索领域经常采用的向量空间模型就是这样一种结构化模型。</p>
<p>(b)特征选择:一般说来结构化文本的特征空间维数较高,需要对其进行缩减,只保留对表达文本内容作用较大的一些特征。</p>
<p>2)文本挖掘:</p>
<p>在完成文本预处理后,可以利用机器学习、数据挖掘以及模式识别等方法提取面向特定应用目标的知识或模式。</p>
<p>3)模式评估与表示<br>最后一个环节是利用已经定义好的评估指标对获取的知识或模式进行评价。如果评价结果符合要求,就存储该模式以备用户使用;否则返回到前面的某个环节重新调整和改进,然后再进行新一轮的发现。</p>
<p><strong>4.研究现状</strong></p>
<p>在文本挖掘过程中,文本的特征表示是整个挖掘过程的基础;而<strong>关联分析、文本分类、文本聚类</strong>是三种最主要也是最基本的功能。</p>
<p><strong>4.1文本特征表示</strong></p>
<p>传统数据挖掘所处理的数据是结构化的,其特征通常不超过几百个;而非结构化或半结构化的文本数据转换成特征向量后,特征数可能高达几万甚至几十万。所以,文本挖掘面临的首要问题是如何在计算机中合理的表示文本。这种表示法既要包含足够的信息以反映文本的特征,又不至于太过庞大使学习算法无法处理。这就涉及到文本特征的抽取和选择。</p>
<p>文本特征指的是关于文本的元数据,可以分为描述性特征,如文本的名称、日期、大小、类型以及语义性特征,如文本的作者、标题、机构、内容。描述性特征易于获得,而语义特征较难获得。在文本特征表示方面,内容特征是被研究得最多的问题。</p>
<p>当文本内容被简单地看成由它所包含的基本语言单位(字、词、词组或短语等)组成的集合时,这些基本的语言单位被称为<strong>项(Term)</strong>。如果用出现在文本中的<br>项表示文本,那么这些项就是文本的特征。</p>
<p>对文本内容的特征表示主要有布尔模型、向量空间模型、概率模型和基于知识的表示模型。因为<strong>布尔模型和向量空间模型</strong>易于理解且计算复杂度较低,所以成为文本表示的主要工具。</p>
<p><strong>(1)特征抽取</strong></p>
<p>中文文档中的词与词之间不像英文文档那样具有分隔符,因此中、英文文档内容特征的提取步骤略有不同。</p>
<pre><code><span class="comment">英文文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">消除停词</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">词干抽取</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征词集合</span>
<span class="comment">中文文档集合</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">消除停词</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">词语切分</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">特征词集合</span>
</code></pre><p><strong>消除停词</strong>:<br>文本集有时包含一些没有意义但使用频率极高的词。这些词在所有文本中的频率分布相近,从而增加了文本之间的相似程度,给文本挖掘带来一定困难。解决这个问题的方法是用这些词构造一个停词表或禁用词表(stop word list)[Ricardo1991],在特征抽取过程中删去停词表中出现的特征词。</p>
<p>常用的停词包括虚词和实词两种,如</p>
<p>(i)虚词:英文中的”a,the,of,for,with,in,at,…”<br>中文中的”的,地,得,把,被,就…”</p>
<p>(ii)实词:数据库会议上的论文中的“数据库”一词,可视为停词。</p>
<p><strong>词干抽取</strong>:</p>
<p>定义: 令V(s)是由彼此互为语法变形的词组成的非空词集,V(s)的规范形式称为词干(stem)。</p>
<p>例如,如果V(s)={connected,connecting,connection,connections},那么s=connect 是V(s)的词干。</p>
<p><strong>词干抽取(stemming)有四种不同的策略:词缀排除(affix rermoval)、词干表查询(table lookup)、后继变化(successor variety)和n-gram</strong>。其中词缀排除最直观、简单且易于实现。多数词的变形是因添加后缀引起的,所以在基于词缀排除策略的抽取算法中后缀排除最为重要,Porter算法[Porter80]是后缀排除算法中最常用的一种。</p>
<p>词干抽取将具有不同词缀的词合并成一个词,降低文本挖掘系统中特征词的总数,从而提高了挖掘系统的性能。</p>
<p>当然,也有两点需要注意:</p>
<p>(1)词干抽取对文本挖掘性能的提高仅在基于统计原理的各种分析和挖掘技术下有效。在进行涉及语义和语法的自然语言处理时,不适宜采用词干抽取技术。</p>
<p>(2)词干抽取对文本挖掘或信息检索准确性的影响至今没有令人信服的结论,因此许多搜索引擎和文本挖掘系统不使用任何词干抽取算法。</p>
<p><strong>汉语切分</strong>:</p>
<p>汉语的分词问题己经基本解决,并出现了多种分词方法。这些分词方法可以分为两类:一类是理解式分词法,即利用汉语的语法知识、语义知识及心理学知识进行分词;另一类是机械式分词法,一般以分词词典为依据,通过文本中的汉字串和词表中的词逐一匹配完成词语切分。第一类分词方法算法复杂,实际应用中经常采用的是第二类分词方法。机械式分词法主要有正向最大匹配法,逆向最<br>大匹配法,逐词遍历法。</p>
<p>由于词典的容量有限,在大规模真实文本处理中,会遇到许多词典中未出现的词,即未登录词。未登录现象是影响分词准确率的重要原因。为解决这个问题,人们提出利用N-gram语言模型进行词项划分[周01a,01b],从而摆脱基于词典的分词方法对词典的依赖。与基于词典的分词方法不同,基于N-gram技术得到的词项不一定具有实际意义。</p>
<p>例如:“文本挖掘”的所有N-gram项为:</p>
<pre><code><span class="number">1</span>-<span class="string">gram:</span>文,本,挖,掘
<span class="number">2</span>-<span class="string">gram:</span>文本,本挖,挖掘
<span class="number">3</span>-<span class="string">gram:</span>文本挖,本挖掘
<span class="number">4</span>-<span class="string">gram:</span>文本挖掘
</code></pre><p>其中除1-gram是单字外,2-gram中的“本挖”,3-gram中的“文本挖”,“本挖掘”都不具有实际意义。</p>
<p><strong>(2)特征选择</strong></p>
<p>特征选择也称特征子集选择或特征集缩减。经过特征抽取获得的特征词数量很多,有时达数万个特征。如此多的特征对许多文本挖掘方法,如文本分类、聚类、文本关联分析来说未必都是有意义的;而过大的特征空间还会严重影响文本挖掘的效率,因此选择适当的特征子集十分必要。</p>
<p>通常采用机器学习的方法进行文本特征选择。虽然机器学习中有许多选取特征子集的算法,但有些算法复杂且效率低下,不适于处理庞大的文本特征集。</p>
<p>国外对特征选择的研究较多[Mladenic99,Mladenic03,Lewis92,Liu96],特别是已有专门针对文本分类特征选择方法的比较研究[Yang97]。国内对这一问题的研究以跟踪研究为主,集中在将国外现有特征评估函数用于中文文本特征选择[周<br>02]及对其进行改进[李99]。</p>
<p><strong>4.2基于关键字的关联分析</strong></p>
<p>文本数据一旦被转化成结构化中间形式后,这种中间形式就作为文本挖掘过程的基础。</p>
<p>与关系数据库中关联规则的挖掘方法类似,基于关键词的关联规则产生过程包括两个阶段:</p>
<p><em>关联挖掘阶段</em>:<br>这一阶段产生所有的支持度大等于最小支持度闭值的关键词集,即频繁项集。</p>
<p><em>规则生成阶段</em>:<br>利用前一阶段产生的频繁项集构造满足最小置信度约束的关联规则。</p>
<p>Feldman等人实现了基于上述思想的文本知识发现系统KDT[Feldman96]、FACT[Feldman97],KDT系统在Reuter22173语料集中发现的关联规则示例:</p>
<pre><code>[<span class="constant">Iran,Nicaragua,Usa]</span>-&gt;<span class="constant">Reagan </span><span class="number">6</span>/<span class="number">1.00</span>
[gold,copper]-&gt;<span class="constant">Canada </span><span class="number">5</span>/<span class="number">0</span>.<span class="number">556</span>
[gold,silver]-&gt;<span class="constant">USA </span><span class="number">19</span>/<span class="number">0</span>.<span class="number">692</span>
</code></pre><p>根据不同的挖掘需要,可以利用不同的挖掘方法,如关联挖掘、最大模式挖掘或层次关联挖掘,完成相应的文本分析任务。</p>
<p><strong>4.3文本分类</strong></p>
<p>文本分类是文本挖掘中一项非常重要的任务,也是国内外研究较多的一种挖掘技术。在机器学习中分类称作有监督学习或有教师归纳,其目的是提出一个分类函数或分类模型(也称作分类器),该模型能把数据库中的数据项映射到给定类别中的一个。</p>
<p>一般来讲,文本分类需要四个步骤:</p>
<p>(1)获取训练文本集:训练文本集由一组经过预处理的文本特征向量组成,每个训练文本(或称训练样本)有一个类别标号;</p>
<p>(2)选择分类方法并训练分类模型:文本分类方法有统计方法、机器学习方法、神经网络方法等等。在对待分类样本进行分类前,要根据所选择的分类方法,利用训练集进行训练并得出分类模型;</p>
<p>(3)用导出的分类模型对其它待分类文本进行分类;</p>
<p>(4)根据分类结果评估分类模型。</p>
<p>另外需要注意的是,文本分类的效果一般和数据集本身的特点有关。有的数据集包含噪声,有的存在缺失值,有的分布稀疏,有的字段或属性间相关性强。目前,普遍认为不存在某种方法能适合于各种特点的数据[Yang99a,Yang99b]。</p>
<p>随着nIetmet技术的发展和普及,在线文本信息迅速增加,文本分类成为处理和组织大量文本数据的关键技术。而近二十多年来计算机软、硬件技术的发展和自然语言处理、人工智能等领域的研究进展为文本自动分类提供了技术条件和理论基础。迄今为止,文本分类研究已经取得了很大的进展,提出了一系列有效的方法,其中分类质量较好的有k最近邻(k-Nearest Neighbor,KNN),[Iwayama95,Yang97,Yang99a]、支持向量机(Support Vector Machine,SVM)[Joachims98]、朴素贝叶斯(Naive Bayes,NB)[Lewis94,Chakra97,Lewis98]。1998年文献[Liu98]提出了基于关联规则的分类方法CBA,此后陆续有人进行这方面的研究,如CAEP[Dong99]、JEP[Li00a,Li00c]、DeEPs[Li0Ob]、CMAR[Li01]和用于文本分类的ARC[Zaiane02]。</p>
<p>国内对中文文本自动分类的研究起步较晚,尽管己有一些研究成果[李04,姚03,邹99,周01a],但由于尚没有通用的标准语料和评价方法,很难对这些成果进行比较。而对基于关联规则的文本分类的研究在国内还未见到。</p>
<p><strong>4.4文本聚类</strong></p>
<p>文本聚类是根据文本数据的不同特征,将其划分为不同数据类的过程。其目的是要使同一类别的文本间的距离尽可能小,而不同类别的文本间的距离尽可能的大。主要的聚类方法有统计方法、机器学习方法、神经网络方法和面向数据库的方法。在统计方法中,聚类也称聚类分析,主要研究基于几何距离的聚类。在机器学习中聚类称作无监督学习或无教师归纳。聚类学习和分类学习的不同主要在于:分类学习的训练文本或对象具有类标号,而用于聚类的文本没有类标号,由聚类学习算法自动确定。</p>
<p>传统的聚类方法在处理高维和海量文本数据时的效率不很理想,原因是:<br>(1)传统的聚类方法对样本空间的搜索具有一定的盲目性;<br>(2)在高维很难找到适宜的相似度度量标准。</p>
<p>虽然,文本聚类用于海量文本数据时存在不足。但与文本分类相比,文本聚类可以直接用于不带类标号的文本集,避免了为获得训练文本的类标号所花费的代价。根据聚类算法无需带有类标号样本这一优势,Nigam等人提出从带有和不带有类标号的混合文本中学习分类模型的方法[Ngiam98]。其思想是利用聚类技术减少分类方法对有标号训练样本的需求,减轻手工标记样本类别所需的工作<br>量,这种方法也称为半监督学习。</p>
<p>文本聚类包括以下四个步骤:</p>
<p>(1)获取结构化的文本集。</p>
<p>结构化的文本集由一组经过预处理的文本特征向量组成。从文本集中选取的特征好坏直接影响到聚类的质量。如果选取的特征与聚类目标无关,那么就难以得到良好的聚类结果。对于聚类任务,合理的特征选择策略应是使同类文本在特征空间中相距较近,异类文本相距较远。</p>
<p>(2)执行聚类算法,获得聚类谱系图。聚类算法的目的是获取能够反映特征空间样本点之间的“抱团”性质。</p>
<p>(3)选取合适的聚类阈值。在得到聚类谱系图后,领域专家凭借经验,并结合具体的应用场合确定阈值。阈值确定后,就可以直接从谱系图中得到聚类结果。</p>
<p>目前,常见的聚类算法可以分成以下几类[Han01]:</p>
<p>(1)平面划分法:对包含n个样本的样本集构造样本集的k个划分,每个划分表示一个聚簇。常见的划分聚类算法有k-均值算法,k-中心点算法,CLARANS算法。</p>
<p>(2)层次聚类法:层次聚类法对给定的样本集进行层次分解。根据层次分解方向的不同可分为凝聚层次聚类和分裂层次聚类。凝聚法也称为自底向上的方法,如AGNES;分裂法也称自顶向下的方法,如DIANA、CURE、BIRCH、Chameleon。</p>
<p>(3)基于密度的方法:多数平面划分法使用距离度量样本间的相似程度,因此只能发现球状簇,难以发现任意形状簇。基于密度的聚类法根据样本点临近区域的密度进行聚类,使在给定区域内至少包含一定数据的样本点。DBSCAN就是一个具有代表性的基于密度的聚类算法。</p>
<p>(4)基于网格的方法:采用多分辨率的网格数据结构,将样本空间量化为数量有限的网格单元,所有聚类操作都在网格上进行,如STING算法。</p>
<p>(5)基于模型的方法:为每个簇假定一个模型,然后通过寻找样本对给定模型的最佳拟合进行聚类。</p>
<p>有些聚类算法集成多种算法的思想,因此难以将其划归到上述类别中的一类,如CLIQUE综合了密度和网格两种聚类方法。</p>
<p>文本聚类有着广泛的应用,比如可以用来:</p>
<p>(1)改进信息检索系统的查全率和查准率[Ricardo99];</p>
<p>(2)用于文本集浏览[Cutting92];</p>
<p>(3)搜索引擎返回的相关文本的组织[Zamir97];</p>
<p>(4)自动产生文本集的类层次结构[Koller97]。在带有类标号的文本集上发现自然聚类[Aggarwal99],然后利用自然聚类改进文本分类器。</p>
<p><strong>5.文本挖掘与相近领域的关系</strong></p>
<p><strong>5.1自然语言处理与文本挖掘的区别</strong></p>
<p>文本挖掘与自然语言处理有着千丝万缕的联系,但也存在明显的不同:</p>
<p>(1)<strong>文本挖掘通过归纳推理</strong>发现知识,而传统的自然语言处理多采用<strong>演绎推理</strong>的方法,很少使用归纳推理方法。</p>
<p>(2)文本挖掘在大规模文本集而不是少数文本中发现知识,其目的不在于改善对文本的理解而是发现文本中的关系。虽然自然语言处理的两个新兴领域:信息检索(Information Retrieval,IR)和信息提取(Information Extraction,IE)也是以大规模文本集为对象,但只要使用严格的演绎推理,那么就不能称作文本挖掘。主要原因是它们没有发现任何知识,只是发现符合某种约束条件的文本而不是知识本身。</p>
<pre><code>[<span class="link_label">比较</span>][<span class="link_reference">方法不同</span>][<span class="link_label">目标不同</span>][<span class="link_reference">对象范围不同</span>]
自然语言处理：[<span class="link_label">演绎推理方法</span>][<span class="link_reference">更好的理解文本</span>][<span class="link_label">以一篇或少数文本为研究对象，发现表示文本特点的关系</span>]
文本挖掘：[<span class="link_label">归纳推理方法</span>][<span class="link_reference">更好的使用文本</span>][<span class="link_label">以大量文本组成的文本集为研究对象，在文本集中发现文本间或文本集中词与词之间的关系</span>]
</code></pre><p>1)信息检索与文本挖掘</p>
<p>信息检索是与数据库技术并行发展多年的领域,其中以文本为对象的文本信息检索以非结构或半结构化数据为处理对象,研究大量文本的信息组织和检索问题。</p>
<p>文本信息检索主要发现与用户检索要求(如关键词)相关的文本。例如,基于关键词的文本检索使用相关度量计算文本与用户查询间的相关性并按相关程度高低排序获得的文档。</p>
<p>近年来,基于自然语言处理技术发展起来的智能检索技术包含了对歧义信息的检索处理,如“苹果”,究竟是指水果还是电脑品牌;“华人”与“中华人民共和国”的区分,这类检索通过歧义知识描述库、全文索引、上下文分析以及用户相关反馈等技术实现文本信息检索的智能化。与文本挖掘不同,智能信息检索仍然只是关注从文本集中更有效地识别和提取相关文档,而不发现任何新的信息或知识。</p>
<p>2)信息提取与文本挖掘</p>
<p>信息提取(IE)是指提取文本集中预定义的事件或任务信息的过程,例如关于恐怖事件信息的提取,可以包括事件时间,地点,恐怖分子,受害者,恐怖分子采用的手段等等。其目的在于发现文本中的结构模式。主要过程是先根据需要确定结构模式,然后从文本中提取相应的知识填进该结构模式。文本挖掘任务则与之正好相反,它需要自动发现那些IE中给定的模式。</p>
<p><strong>5.2文本挖掘与相关领域的交叉</strong></p>
<p>虽然以上介绍的研究领域与文本挖掘存在明显的不同,但它们在某种程度上也存在交叉。最典型的交叉就是通过技术和方法的互相借鉴为各自领域提供新的有效的方法,如许多文本挖掘系统中采用的预处理方法就是最先在信息检索领域中提出并使用的。除此之外,还有其它的例子,如:</p>
<p>(1)基于文本挖掘的汉语词性自动标注</p>
<p>利用文本挖掘研究词及词性的序列模式对词性的影响是非常有新意的研究,这与人在根据上下文对词性进行判断的方法是一致的,不但根据上下文的词、词性,而且可以根据二者的组合来判断某个词的词性。</p>
<p>国内从数据挖掘的角度对汉语文本词性标注规则的获取进行了研究[李01]。其方法是在统计语料规模较大的情况下,利用关联规则发现算法发现词性标注规则。只要规则的置信度足够高,获得的规则就可以用来处理兼类词的情况。该过程完全是自动的,而获取的规则在表达上是明确的,同时又是隐含在数据中、用户不易发现的。</p>
<p>(2)基于信息抽取的文本挖掘</p>
<p>为将非结构化的自然语言文档表示成结构化形式以便直接利用传统的数据挖掘技术来进行文本挖掘。已有多种结构化方法被提出,如前面提到的文本特征表示方法就是最典型的一种。此外,随着信息抽取技术的不断发展[Freitag98,Clifton99],它在文本挖掘领域扮演着日益重要的角色。信息抽取的主要任务是从自然语言文本集中查找特别的数据段,然后将非结构化文档转化为结构化的数据库,以便更容易地理解文本。基于信息抽取<br>的文本挖掘系统框架[Nahm01]:</p>
<pre><code>Text-&gt;{Information Extraction-&gt;DB-&gt;KDD}-&gt;<span class="keyword">Rule</span> Base
</code></pre><p>在这个系统中,IE模块负责在原始文本中捕获特别的数据段,并生成数据库提供给知识发现模块进一步挖掘。</p>
<p><strong>5.3文本挖掘技术在Email处理方面的应用</strong></p>
<p>由于Email文档和普通文档之间有许多相似之处,所以可以将挖掘普通文档涉及的技术和方法用于Email信息挖掘。目前,有许多关于利用文本挖掘技术有效地组织和分析Email信息的研究。例如,通过分析Email的语言和作者性别群进行计算机取证[Rajman97];将Email信息的结构特征和语言学特征与SVM结合进行作者身份鉴别。</p>
<p>文本挖掘技术除了用于组织Email信息外,还可以用于对Email消息进行分类。如:利用朴素贝叶斯算法[Rennie00]、Rocchio算法、SVM方法和Bayesian方法[sahami98,Andr00,Sakkis01]对Email信息进行分类和过滤。此外,文献[Cohen96]和[Lewis94]则提出两个基于规则的系统,这两个系统都是利用文本挖掘技术来分类Email信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-天下第一铭[汤晓鸥]" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/08/天下第一铭[汤晓鸥]/" class="article-date">
  	<time datetime="2016-11-08T07:33:18.000Z" itemprop="datePublished">2016-11-08</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/08/天下第一铭[汤晓鸥]/">天下第一铭--汤晓鸥【转载】</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>天下第一铭</strong></p>
<p>作者：汤晓鸥</p>
<p>  2003年3月8日，陪秋梅过了最后一个属于我们二人世界的妇女节（一直觉得妇女节比情人节重要），第二天，我们安静的二人世界就变成了吵闹的三口之家。新来的第三者白白胖胖，叫铭铭，是在香港威尔士亲王医院10层楼的产科病房出生的。铭铭出生的那天，11层楼住进了一个特殊的病人，说是肺病。可是我们那几天太高兴了，根本没注意。过了两天医生叫我们去他办公室问我们要不要提前出院，因为楼上有个传染病人。我们觉得还有很多东西要和护士学，肺炎也没什么可怕的，不想早出院。等我们回到病房，发现一个层楼的新任妈妈都在收拾行李,已经走的差不多了。我们才明白问题很严重。回家后的两个多月，再没敢带铭铭出门一步。后来才知道11层的病人是香港第一例SARS.</p>
<p> 铭铭的全名叫汤之铭，是佛教大师南怀瑾先生起的名字。一直觉得是老先生取的名字保佑了铭铭。名字是老先生根据2000多年前的一部畅销书《大学》里面的典故起的。经历了SARS的苦其心志，又有老先生的保佑, 我一直觉得天将降大任于铭铭，常和秋梅讲，铭铭将来很可能成就“天下第一铭”。真是想什么来什么，果然四个月大时，就显灵了。</p>
<p> 那时我父母第一次从国内来看铭铭，第二天就要到了，可是我们一直在为一件很头疼的事伤脑筋，铭铭已经14天没大便了，看了几次医生，都说孩子活蹦乱跳的没问题，可能消化太好，听医生讲14 天可能是香港地区的纪录了，不知是不是华南地区纪录。不管怎样说，我儿子有了他自己的第一个地区级纪录了，可惜后来再没能破此纪录，最多一次才四天，可能上次太难受了，看来铭铭也不傻，不愿为虚名太苦了自己。好在我父母来的头一天，问题解决了。父母来了后抱着铭铭说，这孩子没照片上看着胖了，怎么这么轻，我当时后悔不已，不该逼铭铭做他不愿意做的事，否则铭铭至少比头一天重一倍。这孩子其实用心良苦。</p>
<p> 铭铭六个月大的时候，妈妈的假期结束了，不得不回北京工作了。铭铭当然毫不犹豫地决定跟妈妈走（主要是从他的哭声中判断的），这样我又开始了对微软亚洲研究院的经久不息的访问。可能是访问实在太频了，结果我访问的媒体计算组的主任，时任研究院副院长的张宏江问我愿不愿意接管他的媒体计算组，还没等我们开始谈条件，没过多久，研究院重组，宏江成了新成立的工程院院长，另一位副院长Harry(沈向洋)成了研究院新院长。Harry好像觉得我来管媒体计算组不大合适。我也没问为啥。过了没多久，一个周三的下午，Harry突然来电邮说想和我谈谈。原来Harry想找我接管他自己的视觉计算组，又觉得对不起媒体计算组，所以干脆将两个组合并成一个，问我愿不愿带。我第二天就答应了，Harry也怕夜长梦多，隔天我们就把很多细节敲定了，没有经过任何面试，我就在几天之内成了研究院的人了。周六，我就买了房子。那一周，感觉上像两个恋人生怕对方反悔而匆匆领了结婚证。</p>
<p>我当然不会反悔，我对研究院其实爱慕已久，研究院在我心里很像铭铭，大有天下第一铭的气势。我一直觉得Bill一生中做了两个了不起的决定，第一是和IBM签了DOS协议，第二就是建立了微软亚洲研究院。当然，有些同学可能不同意这种说法，我有时也想，和世界上最大的计算机公司签约怎么能和同世界上最大的国家签约相比呢，所以也许建立了亚洲研究院应该更重要。</p>
<p> 北京的学校差不多集中了中国十几亿人中最优秀的人才。研究院是中国唯一的一所由跨国公司成立的从事基础研究的地方。和国外一流研究机构相比，研究院近水楼台；和国内的一流研究院比，亚洲研究院具有国际一流的理念和管理模式；和IBM 及Google 在中国的研究院比，亚洲研究院从事基础研究而不是产品开发。这样独树一帜的地位，天下无双。</p>
<p> 其后果自然是人才的高度集中。其程度让我想起了中国科大和麻省理工学院(MIT)。三个地方的人都挺好，却不太一样。说起来上世纪80年代的科大最难进，因为她只看高考成绩，没什么别的好说的。这样的后果是人才比较同质化，大家的长处都差不多，学生都很像运动员，会比赛，但缺少解决实际问题的能力。MIT就好申请多了，允许书面申请，这样即使某一方面较弱也可以申诉，强调自己的强项。课外活动有超常的地方也可以加分不少。微软亚洲研究院就更好进了，不但有书面申请，还可以当面申诉（面试），有机会全面表现自己。当然，全面表现的后果也很严重，就是进去以后要全面兑现。研究能力，编程能力，写作能力，吹牛能力，缺一不可。</p>
<p>  视觉计算组的同事就具有这样的特性，感觉和他们在一起，没有什么题目做不出来的。所以我又想起了铭铭，总感觉和铭铭在一起的时间太少，想把每一分钟都记录下来，结果照了大量照片。于是很自私地号召大家做照片管理方面的研究，就有了我们在SIGCHI注1上的第一篇长论文。接着为了更方便地把照片中的人像一次从多张照片中分割出来，又做了多图分割的题目。为了快速方便地查找图像，我们做了实时图像检索技术。为了找到更多有趣的应用，又用人脸检测和照片管理技术做了一个将真人头像植入卡通图片的技术，于是很容易的用铭铭的照片将“小兵张嘎”动画图片系列变成了“小兵汤嘎”。我在研究院和一些高校做报告时经常把我们的研究课题总结为“下一代”图象处理技术，因为我们的技术多是应用在我们“下一代”儿童的照片上。</p>
<p>铭铭的照片经常用在视觉计算组的各种实验数据里，成了组里最受欢迎的形象模特</p>
<p>  我们做的一些好玩的技术已经开始影响微软的图像管理和搜索产品开发。在计算机研究领域有个矛盾，要想在实际产品中应用，一项技术必需简单实用，要想发表文章，这项技术又必需显得复杂深奥。要想既像Google那样做出实用产品，又像MIT那样在顶级会议发表文章，就要付出更多辛苦。作为一个做基础研究的地方，我们对在顶级会议发表文章的重视程度和MIT没有什么区别。在过去三年中，我们在一流的计算机视觉会议（ICCV注2, CVPR注3, ECCV注4）发表了60多篇论文。至少在数量上已差不多“天下第一铭”。我常讲做研究就像比武论剑一样,要论剑就要到华山论剑,如果你一定要去太行山论剑, 去挺进大别山，那别人只能当你是游击队, 永远也别想成正规军。在计算机视觉领域，农村是永远也包围不了城市的。华山以外，很难论出好剑。</p>
<p> 发这些论文的另外一个好处是吸引了很多好学生，这些年我见过很多非常优秀的学生，有些已不能用优秀来形容，只能说是天才。晓刚注5是我见到的第一个天才学生，在硕士阶段就发表了五篇CVPR/ICCV。他的才华和人品如此出众，以至于我毫不犹豫地将妹妹嫁给了他。后来我的另一个天才级学生达华注6发表了更多的文章，可是我已经没有妹妹可以再嫁了。好在最近的一个天才级学生靖宇注7，来的时候就有女朋友了。靖宇编程打字的速度是如此之快，以至于我看不清他在键盘上快速移动的手。这三个学生共同特点是都收到MIT 和斯坦福的全额奖学金。晓刚和达华去了MIT, 靖宇选择了斯坦福。我有种感觉，将来他们都会非常成功，成为各自领域的“天下第一铭”。我有种感觉，他们会越来越多。我更有种感觉，铭铭不属于他们。</p>
<p>  铭铭让我自豪的地方也很多。比如铭铭长的很漂亮。这不是我一个人说了算，你可以去问晓晓，桃桃，月月，同同，扬扬，希希……我家院里每个四五岁大的小女孩儿都认为铭铭是她最好的朋友。铭铭四岁前所结交的女朋友（在幼儿园结识的不算）已超过他爸爸四十年艰苦努力的成果（在研究院结识的不算）。</p>
<p>可惜铭铭对学习的态度就像功夫熊猫阿波对面条的感觉，毫无兴趣。铭铭对面条的感觉倒像阿波对功夫的感觉，兴趣盎然。铭铭的人生理想和同龄孩子很不一样，不是做医生，警察，或宇航员，而是“吃饭，睡觉，做佳菲猫”。而且说到做到，铭铭唯一喜欢的课程是厨艺课。厨艺课老师Mariana也觉得铭铭是五岁孩子中厨艺最精湛的了。可惜和同龄孩子一起的时候，极少有比厨艺的时候，反倒是认字，背诗经常被拿出来做表演项目。为了培养铭铭对体育的兴趣，对艺术的热爱，及对中华民族的自豪感，秋梅和我一起带铭铭去看了奥运会开幕式。对于这场人类历史上最精彩最完美最盛大的演出，铭铭印象最深刻的是我在现场餐厅为他买的两根烤香肠。想起来那一定是这世界上代价最高的两根香肠了。</p>
<p> 也许铭铭的血液里真的是流淌着面条汤？希望铭铭长大时，可以选择的已不只华山这一条路，总不能人人都上华山，太挤了，希望有更多的山可以上，有更多的路可以走。总得给铭铭这样不爱学习又厨艺精湛的孩子一条出路吧，但愿那条路不像面条一样弯延曲折。</p>
<p> 秋梅近来常怪我乱讲天下第一铭，给讲坏了。我只好苦笑，怪自己当初求上帝的时候忘了说是正着数还是倒着数了。我就安慰秋梅说“在认字，背诗，音乐，数学，中文，英文，这几个小的方面，铭铭是比别人差一点，好吧，不只一点，差一节，一大节，我们可能也不用太担心，或许铭铭是想后发制人。”</p>
<p>   秋梅温柔地看了我一眼，冷冷地说，“制谁呀！你看后面还有人么？”</p>
<hr>
<p><strong>作者介绍</strong> </p>
<p>汤晓鸥教授，是汤之铭的爸爸。1990年于中国科学技术大学获学士学位，1996年于麻省理工学院(MIT)获博士学位。现于香港中文大学信息工程系任终身教授。2005到2007年，于微软亚洲研究院担任视觉计算组主任。现任IEEE ICCV’09程序委员会主席 (Program Chair)及IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)编委 (Associate Editor)。他的研究领域包括计算机视觉、模式识别、及视频处理。</p>
<p>晓鸥在亚洲研究院期间，被一致推选为研究院文工团团长，兼团委书记，连续三年出任研究院年度文艺晚会主持人，他的演艺生涯开始于研究院，也是在研究院达到顶峰，为此，他为自己起了个艺名叫“小o”。小o的名言是：“看事物要一分为二，任何事物都有两个方面，有可笑的一面，同时也有更可笑的一面”。他就是这样看着铭铭一天天长大。</p>
<hr>
<p>注1，SIGCHI: Special Interest Group for Computer Human Interaction，是世界上人机交互领域最大的专业组织，这是一个多学科交叉的学术组织，包括计算机科学家、软件工程师、心理学家、交互设计人员、图形设计人员、社会学家和人类学家等等。大家共同理念是”设计有用且可用的技术是一个多学科交叉的过程，这一过程的恰当实施可以改变人们的生活”。<br>注2，ICCV: International Conference on Computer Vision，由IEEE主办的国际计算机视觉大会。作为世界顶级的学术会议，首届国际计算机视觉大会于1987年在伦敦揭幕，其后两年举办一届。2005年第10届ICCV在北京举行。<br>注3，CVPR: Computer Vision and Pattern Recognition, 由IEEE主办的国际计算机视觉与模式识别大会，它是计算机视觉领域最顶级的三大学术会议之一。<br>注4，ECCV: European Conference on Computer Vision，两年举办一次，是计算机视觉领域三大顶级学术会议之一。<br>注5，王晓刚：中国科大本科毕业，少年班第一名，郭沫若奖学金获得者，于香港中文大学取得硕士学位，现于麻省理工学院攻读博士学位。<br>注6，林达华：中国科大本科毕业，于香港中文大学取得硕士学位，获香港中文大学工程院优秀硕士论文奖（每年度全院只选一人），现于麻省理工学院攻读博士学位。<br>注7，崔靖宇：清华大学本科及硕士毕业，随汤晓鸥在研究院做了一年半的实习生，获微软学者奖学金，现于斯坦福大学攻读博士学位。</p>
<p>转载自：<a href="http://blog.sina.com.cn/s/blog_4caedc7a0100bgu9.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_4caedc7a0100bgu9.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Researcher/">Researcher</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Researcher/">Researcher</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-安装lpsolve库 for MATLAB" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/03/安装lpsolve库 for MATLAB/" class="article-date">
  	<time datetime="2016-11-03T01:42:53.000Z" itemprop="datePublished">2016-11-03</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/03/安装lpsolve库 for MATLAB/">安装lpsolve库 for MATLAB</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>lpsolve是sourceforge下的一个开源项目，它的介绍如下： </p>
<p><em>Mixed Integer Linear Programming (MILP) solver lp_solve solves pure linear, (mixed) integer/binary, semi-cont and special ordered sets (SOS) models.lp_solve is written in ANSI C and can be compiled on many different platforms like Linux and WINDOWS </em>.</p>
<p>lpsolve是一个混合整数线性规划求解器，可以求解纯线性、（混合）整数/二值、半连续和特殊有序集模型。并且经过实际验证，有极高的求解效率。 </p>
<p><a href="http://sourceforge.net/projects/lpsolve/?source=directory" target="_blank" rel="external">sourceforge主页</a></p>
<p>1.在Windows x64和Matlab环境下使用lpsolve</p>
<p>需要在网址<a href="https://sourceforge.net/projects/lpsolve/files/lpsolve/5.5.2.5/" target="_blank" rel="external">lpsolve_5.5.2.5</a> 提供的文件列表中下载文件lp_solve_5.5.2.5_MATLAB_exe_win64.zip。或者下载文件lp_solve_5.5.2.5_source.tar.gz自行编译dll。</p>
<p>注：在Matlab下运行示例报错：</p>
<pre><code>Error <span class="keyword">using</span> mxlpsolve
Failed <span class="keyword">to</span> initialise lpsolve <span class="keyword">library</span>.
</code></pre><p>参考Using lpsolve from MATLAB: <a href="http://web.mit.edu/lpsolve/doc/MATLAB.htm" target="_blank" rel="external">http://web.mit.edu/lpsolve/doc/MATLAB.htm</a>给出的说明，需要将编译得到的mxlpsolve.dll拷贝到 WINDOWS\system32文件夹下。</p>
<p>2.在MacOS下使用lpsolve</p>
<p>参考<a href="https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/" target="_blank" rel="external">Using lp_solve in Java with Mac OS X</a>的配置说明，下载源文件lp_solve_5.5.2.5_source.tar.gz，然后跳转到lp_solve_5.5/lpsolve55文件夹内，并执行ccc.osx：</p>
<pre><code><span class="keyword">cd</span> lp_solve_5.5/lpsolve55
<span class="keyword">sh</span> ccc.osx
</code></pre><p>生成的文件所在的文件夹：lpsolve55/bin/osx64/，将此文件夹内生成的两个文件liblpsolve55.dylib，liblpsolve55.a拷贝到/usr/local/lib文件夹内即可：</p>
<pre><code>sudo cp liblpsolve55<span class="class">.a</span> liblpsolve55<span class="class">.dylib</span> /usr/local/lib
</code></pre><p>测试demo：</p>
<pre><code><span class="keyword">cd</span> lp_solve_5.5/demo
<span class="keyword">sh</span> ccc
./demo
</code></pre><p>3.在MacOS的Matlab中需要文件mxlpsolve.mexmaci64</p>
<p>需要从lpsolve主页下载源文件lp_solve_5.5.2.5_MATLAB_source.tar.gz，解压缩后，在Matlab中执行文件Makefile.m，期间需要添加各种头文件：</p>
<pre><code>lp_Hash<span class="class">.h</span>
lp_lib<span class="class">.h</span>
lp_matrix<span class="class">.h</span>
lp_mipbb<span class="class">.h</span>
lp_SOS<span class="class">.h</span>
lp_types<span class="class">.h</span>
lp_utils.h
</code></pre><p>可下载lp_solve_5.5.2.5_dev_ux64.tar.gz并从中获取即可。</p>
<p>4.举例</p>
<p>mxlpsove.m是建模的核心函数，一个线性规划模型的所有配置和求解都是通过这个函数完成的。lp_maker.m和lp_solve.m是对mxlpsolve.m的高层包装，简化了模型建立和求解的过程。例如用lpsolve求解数学规划问题：</p>
<p>$$ \max 4x_1+2x_2+x_3\\<br>s.t.~ 2x_1+x_2\le 1\\<br>x_1+2x_3\le 2\\<br>x_1+x_2+x_3=1\\<br>0\le x_1\le1\\<br>0\le x_2\le1\\<br>0\le x_3\le2$$</p>
<p>相应的Matlab语句为：</p>
<pre><code>f = [<span class="number">4</span> <span class="number">2</span> <span class="number">1</span>];
A = [<span class="number">2</span> <span class="number">1</span> <span class="number">0</span>; <span class="number">1</span> <span class="number">0</span> <span class="number">2</span>; <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>];
b = [<span class="number">1</span>; <span class="number">2</span>; <span class="number">1</span>];
l = [ <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>];
u = [ <span class="number">1</span> <span class="number">1</span> <span class="number">2</span>];
lp=mxlpsolve<span class="comment">('make_lp', 1, 3)</span>;
mxlpsolve<span class="comment">('set_verbose', lp, 3)</span>;
mxlpsolve<span class="comment">('set_obj_fn', lp, f)</span>;
mxlpsolve<span class="comment">('add_constraint', lp, A(1, :)</span>, <span class="number">1</span>, b<span class="comment">(1)</span>); 
mxlpsolve<span class="comment">('add_constraint', lp, A(2, :)</span>, <span class="number">1</span>, b<span class="comment">(2)</span>); 
mxlpsolve<span class="comment">('add_constraint', lp, A(3, :)</span>, <span class="number">0</span>, b<span class="comment">(3)</span>; 
mxlpsolve<span class="comment">('set_lowbo', lp, l)</span>; 
mxlpsolve<span class="comment">('set_upbo', lp, u)</span>; 
mxlpsolve<span class="comment">('write_lp', lp, 'a.lp')</span>; 
mxlpsolve<span class="comment">('get_mat', lp, 1, 2)</span> 
mxlpsolve<span class="comment">('solve', lp)</span> 
mxlpsolve<span class="comment">('get_objective', lp)</span> 
mxlpsolve<span class="comment">('get_variables', lp)</span> 
mxlpsolve<span class="comment">('get_constraints', lp)</span> 
mxlpsolve<span class="comment">('delete_lp', lp)</span>
</code></pre><p>重要函数说明：</p>
<p><strong>lp_solve</strong></p>
<pre><code>LP_SOLVE  Solves mixed <span class="built_in">integer</span> linear programming problems.

SYNOPSIS: [obj,x,duals] = lp_solve(f,a,b,e,vlb,vub,xint,scalemode,keep)

  solves the MILP problem

          max v = f<span class="comment">'*x</span>
            a*x &lt;&gt; b
              vlb &lt;= x &lt;= vub
              x(int) are <span class="built_in">integer</span>

ARGUMENTS: The first four arguments are required:

        f: n vector <span class="keyword">of</span> coefficients <span class="keyword">for</span> a linear objective <span class="keyword">function</span>.
        a: m <span class="keyword">by</span> n matrix representing linear constraints.
        b: m vector <span class="keyword">of</span> right sides <span class="keyword">for</span> the inequality constraints.
        e: m vector that determines the sense <span class="keyword">of</span> the inequalities:
                  e(i) = -<span class="number">1</span>  ==&gt; Less Than
                  e(i) =  <span class="number">0</span>  ==&gt; <span class="keyword">Equals</span>
                  e(i) =  <span class="number">1</span>  ==&gt; Greater Than
      vlb: n vector <span class="keyword">of</span> lower bounds. <span class="keyword">If</span> empty <span class="keyword">or</span> omitted,
           <span class="keyword">then</span> the lower bounds are <span class="keyword">set</span> <span class="keyword">to</span> zero.
      vub: n vector <span class="keyword">of</span> upper bounds. May be omitted <span class="keyword">or</span> empty.
     xint: vector <span class="keyword">of</span> <span class="built_in">integer</span> variables. May be omitted <span class="keyword">or</span> empty.
scalemode: scale flag. <span class="keyword">Off</span> <span class="keyword">when</span> <span class="number">0</span> <span class="keyword">or</span> omitted.
     keep: Flag <span class="keyword">for</span> keeping the lp problem after it<span class="comment">'s been solved.</span>
           <span class="keyword">If</span> omitted, the lp will be deleted <span class="keyword">when</span> solved.

   OUTPUT: A nonempty output <span class="keyword">is</span> returned <span class="keyword">if</span> a solution <span class="keyword">is</span> found:

      obj: Optimal value <span class="keyword">of</span> the objective <span class="keyword">function</span>.
        x: Optimal value <span class="keyword">of</span> the decision variables.
    duals: solution <span class="keyword">of</span> the dual problem.

    Example <span class="keyword">of</span> usage. <span class="keyword">To</span> create <span class="keyword">and</span> solve following lp-model:

max: -x1 + <span class="number">2</span> x2;
C1: <span class="number">2</span>x1 + x2 &lt; <span class="number">5</span>;
-<span class="number">4</span> x1 + <span class="number">4</span> x2 &lt;<span class="number">5</span>;

int x2,x1;
The following command can be used:

&gt;&gt; [obj, x]=lp_solve([-<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>; -<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [], [], [<span class="number">1</span>, <span class="number">2</span>])

obj =

         <span class="number">3</span>

x =

        <span class="number">1</span>
         <span class="number">2</span>
</code></pre><p><strong>lp_maker</strong>　　
　　</p>
<pre><code>LP_MAKER  Makes mixed <span class="built_in">integer</span> linear programming problems.
SYNOPSIS: lp_handle = lp_maker(f,a,b,e,vlb,vub,xint,scalemode,setminim)
      make the MILP problem
        max v = f<span class="comment">'*x</span>
          a*x &lt;&gt; b
            vlb &lt;= x &lt;= vub
            x(int) are <span class="built_in">integer</span>

   ARGUMENTS: The first four arguments are required:
            f: n vector <span class="keyword">of</span> coefficients <span class="keyword">for</span> a linear objective <span class="keyword">function</span>.
            a: m <span class="keyword">by</span> n matrix representing linear constraints.
            b: m vector <span class="keyword">of</span> right sides <span class="keyword">for</span> the inequality constraints.
            e: m vector that determines the sense <span class="keyword">of</span> the inequalities:
                      e(i) &lt; <span class="number">0</span>  ==&gt; Less Than
                      e(i) = <span class="number">0</span>  ==&gt; <span class="keyword">Equals</span>
                      e(i) &gt; <span class="number">0</span>  ==&gt; Greater Than
          vlb: n vector <span class="keyword">of</span> non-negative lower bounds. <span class="keyword">If</span> empty <span class="keyword">or</span> omitted,
               <span class="keyword">then</span> the lower bounds are <span class="keyword">set</span> <span class="keyword">to</span> zero.
          vub: n vector <span class="keyword">of</span> upper bounds. May be omitted <span class="keyword">or</span> empty.
         xint: vector <span class="keyword">of</span> <span class="built_in">integer</span> variables. May be omitted <span class="keyword">or</span> empty.
    scalemode: Autoscale flag. <span class="keyword">Off</span> <span class="keyword">when</span> <span class="number">0</span> <span class="keyword">or</span> omitted.
     setminim: <span class="keyword">Set</span> maximum lp <span class="keyword">when</span> this flag <span class="keyword">equals</span> <span class="number">0</span> <span class="keyword">or</span> omitted.

   OUTPUT: lp_handle <span class="keyword">is</span> an <span class="built_in">integer</span> handle <span class="keyword">to</span> the lp created.
Example <span class="keyword">of</span> usage. <span class="keyword">To</span> create following lp-model:

max: -x1 + <span class="number">2</span> x2;
C1: <span class="number">2</span>x1 + x2 &lt; <span class="number">5</span>;
-<span class="number">4</span> x1 + <span class="number">4</span> x2 &lt;<span class="number">5</span>;

int x2,x1;
The following command can be used:

&gt;&gt; lp=lp_maker([-<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>; -<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [], [], [<span class="number">1</span>, <span class="number">2</span>])

lp =

     <span class="number">0</span>
<span class="keyword">To</span> solve the model <span class="keyword">and</span> <span class="keyword">get</span> the solution:

&gt;&gt; mxlpsolve(<span class="comment">'solve', lp)</span>

ans =

     <span class="number">0</span>

&gt;&gt; mxlpsolve(<span class="comment">'get_objective', lp)</span>

ans =

     <span class="number">3</span>

&gt;&gt; mxlpsolve(<span class="comment">'get_variables', lp)</span>

ans =

     <span class="number">1</span>
     <span class="number">2</span>
</code></pre><p>注意：Don’t forget to free the handle and its associated memory when you are done:</p>
<pre><code><span class="prompt">&gt;&gt;</span> mxlpsolve(<span class="string">'delete_lp'</span>, lp);
</code></pre><p>5.参考</p>
<p><a href="https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/" target="_blank" rel="external">https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/</a></p>
<p><a href="http://web.mit.edu/lpsolve/doc/MATLAB.htm" target="_blank" rel="external">http://web.mit.edu/lpsolve/doc/MATLAB.htm</a></p>
<p><a href="http://www.cnblogs.com/kane1990/p/3428129.html" target="_blank" rel="external">http://www.cnblogs.com/kane1990/p/3428129.html</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Matlab/">Matlab</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Clarifai API体验" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/02/Clarifai API体验/" class="article-date">
  	<time datetime="2016-11-02T07:24:53.000Z" itemprop="datePublished">2016-11-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/02/Clarifai API体验/">Clarifai API体验</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>参考官方给出的文档：<a href="https://developer.clarifai.com/guide/tag#guide-tag-responses" target="_blank" rel="external">https://developer.clarifai.com/guide/tag#guide-tag-responses</a>，在本机进行简单的体验。</p>
<p>首先注册账户，然后创建Application，获取ID及Secret，并在Application页面下生成“Access Token”码：XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX</p>
<p>〇、客户端API参考：<a href="https://github.com/Clarifai/clarifai-python" target="_blank" rel="external">https://github.com/Clarifai/clarifai-python</a></p>
<pre><code>pip install clarifai==<span class="number">2.0</span>.10
clarifai config
<span class="label">CLARIFAI_APP_ID:</span>
<span class="label">CLARIFAI_APP_SECRET:</span>
</code></pre><p>在python中实现测试：</p>
<pre><code><span class="keyword">from</span> clarifai.rest <span class="keyword">import</span> ClarifaiApp
app = ClarifaiApp()
model = app.models.get(<span class="string">'general-v1.3'</span>)
<span class="built_in">print</span> model.predict_by_url(<span class="string">'https://samples.clarifai.com/metro-north.jpg'</span>)
<span class="comment"># or local image</span>
<span class="built_in">print</span> model.predict_by_filename(<span class="string">'/Users/USER/my_image.jpeg'</span>)
</code></pre><p>一、打开MAC终端，输入Request命令：</p>
<ol>
<li><p>测试在线图片</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> --data-urlencode <span class="string">"url=https://samples.clarifai.com/metro-north.jpg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
<li><p>测试本地图片</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> -<span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
<li><p>测试多幅图像</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> -<span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image1.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image2.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image3.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image4.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
</ol>
<p>二、读取Response结果</p>
<p>在python中使用json来解析文本结果：</p>
<pre><code># -*- coding: utf-<span class="number">8</span> -*-
<span class="keyword">import</span> json
<span class="keyword">for</span> <span class="built_in">line</span> in <span class="built_in">open</span>(<span class="string">"response.txt"</span>):
    <span class="built_in">print</span> <span class="built_in">line</span>
<span class="built_in">str</span>=json.loads(<span class="built_in">line</span>)
results=<span class="built_in">str</span>[<span class="string">'results'</span>]
strNum=len(<span class="built_in">str</span>[<span class="string">'results'</span>])
<span class="keyword">for</span> ind in range(<span class="number">0</span>,strNum):
    <span class="built_in">print</span> ind
    <span class="built_in">print</span> results[ind][<span class="string">'result'</span>][<span class="string">'tag'</span>][<span class="string">'classes'</span>]
</code></pre><p>输出解析结果（图像的tag类别）：</p>
<pre><code><span class="number">0</span>
[<span class="string">u'sketch'</span>, <span class="string">u'illustration'</span>, <span class="string">u'cute'</span>, <span class="string">u'man'</span>, <span class="string">u'no person'</span>, <span class="string">u'funny'</span>, <span class="string">u'fun'</span>, <span class="string">u'vector'</span>, <span class="string">u'character'</span>, <span class="string">u'graphic design'</span>, <span class="string">u'business'</span>, <span class="string">u'child'</span>, <span class="string">u'art'</span>, <span class="string">u'retro'</span>, <span class="string">u'love'</span>, <span class="string">u'moon'</span>, <span class="string">u'Halloween'</span>, <span class="string">u'graphic'</span>, <span class="string">u'design'</span>, <span class="string">u'isolated'</span>]
<span class="number">1</span>
[<span class="string">u'illustration'</span>, <span class="string">u'vector'</span>, <span class="string">u'sketch'</span>, <span class="string">u'retro'</span>, <span class="string">u'design'</span>, <span class="string">u'sketch'</span>, <span class="string">u'business'</span>, <span class="string">u'symbol'</span>, <span class="string">u'man'</span>, <span class="string">u'family'</span>, <span class="string">u'no person'</span>, <span class="string">u'graphic'</span>, <span class="string">u'humor'</span>, <span class="string">u'people'</span>, <span class="string">u'outdoors'</span>, <span class="string">u'image'</span>, <span class="string">u'art'</span>, <span class="string">u'nature'</span>, <span class="string">u'house'</span>, <span class="string">u'animal'</span>]
<span class="number">2</span>
[<span class="string">u'vector'</span>, <span class="string">u'vector'</span>, <span class="string">u'illustration'</span>, <span class="string">u'no person'</span>, <span class="string">u'internet'</span>, <span class="string">u'technology'</span>, <span class="string">u'design'</span>, <span class="string">u'symbol'</span>, <span class="string">u'graphic design'</span>, <span class="string">u'data'</span>, <span class="string">u'flat'</span>, <span class="string">u'data'</span>, <span class="string">u'business'</span>, <span class="string">u'stripe'</span>, <span class="string">u'set'</span>, <span class="string">u'design'</span>, <span class="string">u'square'</span>, <span class="string">u'science'</span>, <span class="string">u'education'</span>, <span class="string">u'creativity'</span>]
<span class="number">3</span>
[<span class="string">u'sleeve'</span>, <span class="string">u'illustration'</span>, <span class="string">u'isolated'</span>, <span class="string">u'polo'</span>, <span class="string">u'shirt'</span>, <span class="string">u'vector'</span>, <span class="string">u'design'</span>, <span class="string">u'image'</span>, <span class="string">u'wear'</span>, <span class="string">u'garment'</span>, <span class="string">u'sale'</span>, <span class="string">u'man'</span>, <span class="string">u'fashion'</span>, <span class="string">u'shopping'</span>, <span class="string">u'casual'</span>, <span class="string">u'front'</span>, <span class="string">u'shop'</span>, <span class="string">u'apparel'</span>, <span class="string">u'graphic'</span>, <span class="string">u'flat'</span>]
<span class="number">4</span>
[<span class="string">u'illustration'</span>, <span class="string">u'vector'</span>, <span class="string">u'sketch'</span>, <span class="string">u'Halloween'</span>, <span class="string">u'cute'</span>, <span class="string">u'animal'</span>, <span class="string">u'skittish'</span>, <span class="string">u'funny'</span>, <span class="string">u'design'</span>, <span class="string">u'art'</span>, <span class="string">u'graphic'</span>, <span class="string">u'fun'</span>, <span class="string">u'ghost'</span>, <span class="string">u'scary'</span>, <span class="string">u'no person'</span>, <span class="string">u'vicious'</span>, <span class="string">u'desktop'</span>, <span class="string">u'retro'</span>, <span class="string">u'image'</span>, <span class="string">u'business'</span>]
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-DL学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/24/DL学习笔记/" class="article-date">
  	<time datetime="2016-10-24T01:23:03.000Z" itemprop="datePublished">2016-10-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/24/DL学习笔记/">DL学习笔记</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="（一）DeepLearning_(DL)概述">（一）DeepLearning (DL)概述</h3><h4 id="1-_什么是DL？">1. 什么是DL？</h4><p>机器学习ML的框架：</p>
<p>(1)数据：\({(x_i,y_i)}, 1\le i \le m\)</p>
<p>(2)模型：\(\mathcal{F}={f(x;\theta)}, \theta\in\Theta\)</p>
<p>i. 线性： \(y=f(x)=w^Tx+b\) 【x——&gt;y】</p>
<p>ii. 广义线性：\(y=f(x)=w^T\phi(x)+b\)  【x——&gt;[\(\bar{x}=\phi(x)\)]——&gt;y，其中的\(\phi\)为模式识别中的特征Feature，这一步也称为特征学习。】</p>
<p>iii. 非线性：人工神经网络（ANN）</p>
<p>(3)准则：损失函数\(L(y,f(x))\)</p>
<p>经验风险：\(R(\theta)=\frac{1}{m}\sum_{i=1}^m{L(y,f(x_i,\theta))}\)</p>
<p>正则化项：\(|w|_2^2\)</p>
<p>Minimizing： \(R(\theta)+\lambda|w|_2^2\)，或稀疏正则项L1.</p>
<p>因此转化为一个最优化问题。</p>
<p>神经网络ANN中 \(y=\sigma(\sum_i{w_i x_i+b})\) 相当于从P维到Q维的一个映射函数。则DL就是解决这个深度前馈神经网络的算法。</p>
<h4 id="2-_存在的困难及挑战">2. 存在的困难及挑战</h4><p>可训练的<strong>参数太多</strong>；【参数过多带来的问题具体包括：计算资源要大，数据要多(否则出现过拟合)，算法效率要高】</p>
<p>多层网络以后的优化问题变为<strong>非凸优化</strong>问题；</p>
<p><strong>梯度弥散</strong>问题，即从网络层由上往下的参数调节变得非常困难；</p>
<p>解释困难(可通过一些可视化的方法一定程度上来进行解释)；</p>
<h4 id="3-算法历史">3.算法历史</h4><p>1958年，感知机Perception：一个神经细胞的处理能力较差，与或运算无法实现。</p>
<p>1986年，神经网络的概念出现：BP算法，对浅层网络做了很多的工作。一方面受限于当时的硬件和软件问题。</p>
<p>1998年，CNN卷积神经网络，在手写体识别中取得了成功。—LeCun</p>
<p>2006年，DBN深度置信网络，—Hinton</p>
<h4 id="4-为什么学习DL">4.为什么学习DL</h4><p>有效！【语音识别，目标识别，NLP，CV….】</p>
<h4 id="5-领域概述">5.领域概述</h4><p>学术机构：</p>
<p>TorontoU，Hinton，1975年EdinburghU’s PHD;<br>NewYorkU，LeCun，1987年PHD;<br>MentrealU，Bengio，1991年McGillU’s PHD;<br>StanfordU, Ng，2003年UCBerkeley’s PHD;</p>
<p>学术会议：<br>NIPS，ICML，ICRL，…</p>
<p>参考: <a href="http://v.youku.com/v_show/id_XNjU1MzU4NDIw.html?f=21508721&amp;o=1" target="_blank" rel="external">深度学习课程-概述</a></p>
<h3 id="（二）FNN_&amp;_BP">（二）FNN &amp; BP</h3><h4 id="一-前馈神经网络FNN">一.前馈神经网络FNN</h4><h5 id="1-神经元、神经层、神经网">1.神经元、神经层、神经网</h5><p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/SingleNeuron.png" alt=""></p>
<pre><code>神经元
x_1 -w_1-\
..  -w_i<span class="comment">--〇z---&gt;a</span>
x_p -w_p-/
</code></pre><p>一个神经元的输出是一个线性函数与一个非线性函数的复合：\([z=\sum w_ix_i+b,~a=f(z)]=&gt;a=f(\sum w_ix_i+b)\). </p>
<p>其中激活函数包括：sigmod: \(\sigma(x)=\frac{1}{1+e^x};~\tan(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}};~|x|;~\)等</p>
<pre><code><span class="comment">神经层</span>
<span class="comment">x_1</span> <span class="literal">-</span><span class="comment">w_1</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">\</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">a_1</span> 
              <span class="comment">X</span>
<span class="string">.</span><span class="string">.</span>  <span class="literal">-</span><span class="comment">w_i</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">X</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="string">.</span><span class="string">.</span><span class="string">.</span>
              <span class="comment">X</span>
<span class="comment">x_p</span> <span class="literal">-</span><span class="comment">w_p</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">/</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">a_p</span>
</code></pre><p>神经层上的每个神经元的输入时相同的，但权值是不同的。我们用 \( a^{(l)}_i \)表示第 \( l \)层第 \( i \)单元的激活值（输出值）。</p>
<p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/Network331.png" alt=""></p>
<pre><code>神经网
-<span class="ruby">〇-\    -〇-\
</span>-<span class="ruby">〇--〇-<span class="regexp">/-〇--〇-...
</span></span>-<span class="ruby">〇--〇-\-〇--〇-...
</span>-<span class="ruby">〇-<span class="regexp">/    -〇-/</span>
</span>n_1  n_2  n_3   ...   n_L
</code></pre><h5 id="2-记号">2.记号</h5><p>超参数(并不是学习出来的，而是认为根据需要设定的参数，也称元参数)：层数L，第l层的神经元个数\(n^{(l)}\)，神经元非线性函数\(f_l()\).</p>
<p>要学习的参数：连接权weight参数\(w_i^{(1)},w_i^{(2)},..w_i^{(L)}\)，两层之间的连接权。 偏bias参数\(b^{(1)},b^{(2)},..b^{(L)}\).</p>
<p>第l层神经元的状态\(z^{(l)},1\le l\le L\)；第l层神经元的激活activation：\(a^{(l)}\)</p>
<h5 id="3-前馈计算">3.前馈计算</h5><p>(1)基本公式</p>
<p>\(z^{(l+1)}=w^l a^l+b^l  \)</p>
<p>\(a^l=f_l(z^l) \) </p>
<p>\(h_{w,b}(x)=a^{(l+1)}=f(z^{(l+1)})\)</p>
<p>(2)前馈计算(\(W=(w^1,…,w^l),b=(b^1,..,b^l)\))</p>
<p>\(l=1, a^l=x\) </p>
<p>计算步骤：\(a^1-&gt;z^2-&gt;a^2-&gt;…-&gt;z^L-&gt;a^L\) </p>
<p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/Network3322.png" alt=""></p>
<p>“目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是\(n<em>l\)层的神经网络，第1层是输入层，第 \(n_l\)层是输出层，中间的每个层 \(l\)与层 \(l+1\)紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 \(L_2\)层的所有激活值，然后是第 \(L_3\)层的激活值，以此类推，直到第 \(L</em>{n_l}\)层。这是一个前馈神经网络的例子，因为这种联接图没有闭环或回路。”——<a href="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="external">UFLDL</a></p>
<h5 id="4-应用于ML">4.应用于ML</h5><p>数据D：\((x_i,y_i),1\le i\le N\)</p>
<p>模型M：\(y=h(x|w,b)\)</p>
<p>准则C: \(\sum_i|y_i-a^l(x|w,b)|^2+\lambda|w|_F^2) \)=min</p>
<p>其中，第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过度拟合: \(||w||<em>F^2=\sum_i\sum_jw</em>{ij}^2\)</p>
<p>权重衰减参数 \( \lambda \)用于控制公式中两项的相对重要性。</p>
<p>使用梯度下降法进行求解这个优化问题。</p>
<p>将上式写为：\(\sum_i J(x_i,y_i;w,b)+\lambda|w|_F^2\)=min</p>
<p>目标函数关于待求参数的导数：</p>
<p>其中：$$\frac{\partial|w|_F^2}{\partial w}=2w$$</p>
<p>然后重点求:$$\frac{\partial \sum J(.)}{\partial w};~~\frac{\partial \sum J(.)}{\partial b}$$</p>
<p>迭代公式：<br>$$w^{t+1}=w^t-\alpha \frac{\partial \sum J(.)}{\partial w}$$<br>$$b^{t+1}=b^t-\alpha \frac{\partial \sum J(.)}{\partial b}$$<br>其中\(\alpha\)是学习速率。</p>
<h4 id="二-BP算法">二.BP算法</h4><p>“我们的目标是针对参数 \( W \)和 \( b \)来求其函数 \( J(W,b) \)的最小值。为了求解神经网络，我们需要将每一个参数 \( W^{(l)}<em>{ij} \)和 \( b^{(l)}_i \)初始化为一个很小的、接近零的随机值（比如说，使用正态分布 \( {Normal}(0,\epsilon^2) \)生成的随机值，其中 \( \epsilon \)设置为 \( 0.01 \) ），之后对目标函数使用诸如批量梯度下降法的最优化算法。因为 \( J(W, b) \)是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。最后，需要再次强调的是，要将参数进行随机初始化，而不是全部置为  0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，对于所有 \( i\)，\( W^{(1)}</em>{ij}\)都会取相同的值，那么对于任何输入 \( x \)都会有：\( a^{(2)}_1 = a^{(2)}_2 = a^{(2)}_3 = \ldots \)）。随机初始化的目的是使对称失效。”——<a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external">UFLDL</a></p>
<h5 id="1-多元函数的偏导数">1.多元函数的偏导数</h5><p>(1)<br>$$X=[x_1,…,x_p]^T\in R^p,~ y=f(X)=f(x_1,…,x_p)$$<br>$$\nabla_x f(x)=\nabla_x y=\frac{\partial y}{\partial x}=[\frac{\partial f(x_1)}{\partial x},…,\frac{\partial f(x_p)}{\partial x}]^T\in R^p$$<br>(2)<br>$$x\in R^p, y=[..]^T\in R^q$$<br>$$y=[f_1(x),..,f_q(x)]^T$$<br>$$\nabla_xy=\nabla_x f(x)=\frac{\partial y}{\partial x}=[\frac{\partial f(x)}{\partial x_i}]^T\in R^{p\times q}$$<br>(3) 导数法则：链式法则</p>
<h5 id="2-BP算法">2.BP算法</h5><p>BP思路：给定一个样例 \( (x,y)\)，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \( h_{W,b}(x) \)的输出值。之后，针对第 \( l \)层的每一个节点 \( i\)，我们计算出其“残差” \( \delta^{(l)}_i\)，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 \( \delta^{(n_l)}_i \)（第 \( n_l \)层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点（第 \( l+1 \)层节点）残差的加权平均值计算 \( \delta^{(l)}_i\)，这些节点以 \( a^{(l)}_i \)作为输入。</p>
<p>BP算法步骤：</p>
<p>(i)进行前馈传导计算，利用前向传导公式，得到 \( L<em>2, L_3, \ldots \) 直到输出层 \( L</em>{n_l} \)的激活值。</p>
<p>(ii)对于第 \( n_l \)层（输出层）的每个输出单元 \( i\)，我们根据以下公式计算残差：</p>
<p>$$\delta_i^{(n_l)}= \frac{\partial J}{\partial z_i^{n_l}}=-(y_i-a_i^{(n_l)})\cdot f’(z_i^{(n_l)})$$</p>
<p>(iii)令\(\delta^l=\frac{\partial J}{\partial z^l}\)，则对 \(  l = n_l-1, n_l-2, n_l-3, \ldots, 2 \)的各个层，第 \(  l \)层的第 \(  i \)个节点的残差计算方法如下：（矩阵向量形式）</p>
<p>$$\begin{equation}\begin{split}\delta^{l}&amp;=\frac{\partial J}{\partial z^{l}}=\frac{\partial a^l}{\partial z^l}\frac{\partial z^{l+1}}{\partial a^l}\frac{\partial J}{\partial z^{l+1}}\\<br>&amp;=diag(f’(z^l))\cdot((w^l)^T\cdot\delta^{l+1})\\<br>&amp;=(f’_l(z^l))\odot((w^l)^T\cdot\delta^{l+1})\\<br>&amp;=((w^l)^T\cdot\delta^{l+1})\odot(f’_l(z^l))<br>\end{split}\end{equation}$$<br>以上逐次从后向前求导的过程即为“反向传导（BP）”的本意所在.</p>
<p>(iv)计算所需的偏导数：<br>$$\frac{\partial J}{\partial w^l}=\delta^{l+1}\cdot(a^l)^T$$<br>$$\frac{\partial J}{\partial b^l}=\delta^{l+1}$$</p>
<p>其中，假设 \( f(z) \)是sigmoid函数，并且我们已经在前向传导运算中得到了 \( a^{(l)}_i\)。那么，使用我们早先推导出的 \( f’(z)\)表达式，就可以计算得到 \( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)\)。</p>
<h5 id="3-梯度下降法求解过程">3.梯度下降法求解过程</h5><p>(1) 对所有 \(  l\)，令 \(  \Delta W^{(l)} := 0 \),  \(  \Delta b^{(l)} := 0 \)（设置为全零矩阵或全零向量）</p>
<p>(2) For     \(  i = 1 \) to \(  m\)，使用反向传播算法计算: </p>
<p>\(\nabla_{W^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \nabla_{b^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \Delta W^{(l)} := \Delta W^{(l)} + \nabla_{W^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \Delta b^{(l)} := \Delta b^{(l)} + \nabla_{b^{(l)}} J(W,b;x,y) \)</p>
<p>(3) 更新参数：<br>$$ \begin{align}<br>W^{(l)} &amp;= W^{(l)} - \alpha \left[ \left(\frac{1}{m} \Delta W^{(l)} \right) + \lambda W^{(l)}\right] \\<br>b^{(l)} &amp;= b^{(l)} - \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]<br>\end{align}$$</p>
<p>重复梯度下降法的迭代步骤来减小代价函数 \( J(W,b)\) ，以训练我们的神经网络。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Clarifai" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/Clarifai/" class="article-date">
  	<time datetime="2016-10-21T13:40:07.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/Clarifai/">Clarifai图像自动化标签</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Clarifai 公司：www.clarifai.com</p>
<p>Clarifai创始人Matt Zeiler是New York University (NYU) Rob Fergus教授门下的学生。从上个世纪开始，NYU就一直是neural computation的重镇。现在Deep net的前身ConvNet，就是出自 NYU 的 Yann LeCun教授组.</p>
<p>ImageNet Large Scale Visual Recognition Competition 2013 (ILSVRC2013)</p>
<p>其中Matt Zeiler (<a href="http://Clarifai.com" target="_blank" rel="external">http://Clarifai.com</a>) 的算法排名第一，在不用额外训练数据的情况下，跑到了error rate 0.1174这样的成绩。</p>
<p>这个成绩是这样解读的：任选一张图片，扔给算法，算法返回5个结果。如果5个结果中，有一个猜对了物体类别，就算正确。换言之，如果允许猜5次，Clarifai已经有接近90%的准确率了。这里的物体类别包括了英语中两万多个名词，几乎涵盖了各大类别。</p>
<p><a href="https://developer.clarifai.com/guide/" target="_blank" rel="external">Clarifai API</a></p>
<p>参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/19821292" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/19821292</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Ubuntu16.04+Titan X+CUDA8.0+cudnn5" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/" class="article-date">
  	<time datetime="2016-10-21T07:49:55.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/">Ubuntu16.04+Titan X+CUDA8.0+cudnn5.1+Caffe</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>1.安装Ubuntu16.04 LTS x64</strong></p>
<p>利用工具rufus制作USB系统盘(官方下载64位版本: <a href="http://www.ubuntu.com/download/alternative-downloads" target="_blank" rel="external">ubuntu-16.04-desktop-amd64.iso</a>)，因为已有Win7系统，此处选择“Install Ubuntu alongside Windows Boot Manager”，分区采用默认选择，语言选择English，安装完毕。</p>
<p><em>注：此时显示器VGA接口接到主板集成显卡接口上。</em></p>
<p><strong>2.更新源</strong></p>
<pre><code>cd /etc/apt/
sudo cp sources<span class="class">.list</span> sources<span class="class">.list</span><span class="class">.bak</span>
sudo gedit sources.list
</code></pre><p>在sources.list文件头部添加如下源：</p>
<pre><code>deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
</code></pre><p>然后更新源和安装的包:</p>
<pre><code>sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>
sudo apt-<span class="built_in">get</span> upgrade
</code></pre><p><strong>3.安装NVIDIA显卡驱动</strong></p>
<p>采用ppa安装方式，没选择最新的nvidia-370，我选择了nvidia-367。</p>
<pre><code>sudo<span class="instruction"> add-apt-repository </span>ppa:graphics-drivers/ppa
sudo apt-get update
sudo apt-get install nvidia-367
sudo apt-get install mesa-common-dev
sudo apt-get install freeglut3-dev
sudo reboot
</code></pre><p><em>将显示器VGA接口换到NVIDIA显卡上。</em></p>
<p><strong>4.修改分辨率</strong></p>
<p>启动到界面之后发现分辨率只有1366x768，显示器适合1920x1080，采用xrandr并修改xorg.conf来解决。</p>
<pre><code>sudo gedit /etc/X11/xorg<span class="class">.conf</span>
修改如下：
HorizSync <span class="number">31.0</span> - <span class="number">84.0</span>
VertRefresh <span class="number">56.0</span>-<span class="number">77.0</span>
</code></pre><p>注销系统再次登录后，选择适合的桌面分辨率即可。</p>
<p><strong>5.安装CUDA8.0</strong></p>
<p>到官网下载<a href="https://developer.nvidia.com/cuda-release-candidate-download" target="_blank" rel="external">cuda_8.0.44_linux.run</a>，复制到根目录下。</p>
<pre><code>sudo sh cuda_8.0.44_linux.<span class="command">run</span> <span class="comment">--tmpdir=/tmp/</span>
</code></pre><p>遇到问题：incomplete installation，然后执行</p>
<pre><code>sudo apt-<span class="built_in">get</span> install freeglut3-<span class="built_in">dev</span> build-essential libx11-<span class="built_in">dev</span> libxmu-<span class="built_in">dev</span> libxi-<span class="built_in">dev</span> libgl1-mesa-glx libglu1-mesa libglu1-mesa-<span class="built_in">dev</span>
sudo sh cuda_8.0.44_linux.run -silent -driver
</code></pre><p>注：此时安装过程中提示是否要安装NVIDIA驱动时选择no。其他选择yes或默认即可。</p>
<pre><code>Install NVIDIA Accelerated Graphics Driver <span class="keyword">for</span> Linux-x86_64 <span class="number">361.62</span>? <span class="params">(y)</span>es/<span class="params">(n)</span>o/<span class="params">(q)</span>uit: n
</code></pre><p>安装完毕后声明环境变量：</p>
<pre><code><span class="title">sudo</span> gedit ~/.bashrc
</code></pre><p>在.bashrc尾部添加如下内容：</p>
<pre><code>export <span class="constant">PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/bin</span><span class="variable">${</span><span class="constant">PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">PATH</span>}}
export <span class="constant">LD_LIBRARY_PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/lib</span>64<span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span>}}
</code></pre><p>测试下安装是否成功：</p>
<p>测试1：</p>
<pre><code>cd NVIDI<span class="built_in">A_CUDA</span>-<span class="number">8.0</span>_Samples/
nvidia-smi
</code></pre><p>输出：</p>
<pre><code>Tue Oct 18 15:20:34 2016       
+-----------------------------------------------------------------------------+
|<span class="string"> NVIDIA-SMI 367.44                 Driver Version: 367.44                    </span>|
|<span class="string">-------------------------------+----------------------+----------------------+
</span>|<span class="string"> GPU  Name        Persistence-M</span>|<span class="string"> Bus-Id        Disp.A </span>|<span class="string"> Volatile Uncorr. ECC </span>|
|<span class="string"> Fan  Temp  Perf  Pwr:Usage/Cap</span>|<span class="string">         Memory-Usage </span>|<span class="string"> GPU-Util  Compute M. </span>|
|<span class="string">===============================+======================+======================</span>|
|<span class="string">   0  GeForce GTX TIT...  Off  </span>|<span class="string"> 0000:01:00.0      On </span>|<span class="string">                  N/A </span>|
|<span class="string"> 22%   48C    P5    27W / 250W </span>|<span class="string">    169MiB / 12205MiB </span>|<span class="string">      1%      Default </span>|
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
|<span class="string"> Processes:                                                       GPU Memory </span>|
|<span class="string">  GPU       PID  Type  Process name                               Usage      </span>|
|<span class="string">=============================================================================</span>|
|<span class="string">    0      2421    G   /usr/lib/xorg/Xorg                             105MiB </span>|
|<span class="string">    0     10062    G   compiz                                          63MiB </span>|
+-----------------------------------------------------------------------------+
</code></pre><p>测试2：</p>
<pre><code>cd <span class="number">1</span>_Utilities/deviceQuery
make
<span class="attribute">...</span><span class="attribute">...</span><span class="built_in">..
</span><span class="built_in">.</span>/deviceQuery 
</code></pre><p>输出：</p>
<pre><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "GeForce GTX TITAN X"
  CUDA Driver Version / Runtime Version          8.0 / 8.0
  CUDA Capability Major/Minor version number:    5.2
  Total amount of global memory:                 12205 MBytes (12798197760 bytes)
  (24) Multiprocessors, (128) CUDA Cores/MP:     3072 CUDA Cores
  GPU Max Clock rate:                            1076 MHz (1.08 GHz)
  Memory Clock rate:                             3505 Mhz
  Memory Bus Width:                              384-bit
  L2 <span class="operator"><span class="keyword">Cache</span> <span class="keyword">Size</span>:                                 <span class="number">3145728</span> bytes
  Maximum Texture Dimension <span class="keyword">Size</span> (x,y,z)         <span class="number">1</span>D=(<span class="number">65536</span>), <span class="number">2</span>D=(<span class="number">65536</span>, <span class="number">65536</span>), <span class="number">3</span>D=(<span class="number">4096</span>, <span class="number">4096</span>, <span class="number">4096</span>)
  Maximum Layered <span class="number">1</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">1</span>D=(<span class="number">16384</span>), <span class="number">2048</span> layers
  Maximum Layered <span class="number">2</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">2</span>D=(<span class="number">16384</span>, <span class="number">16384</span>), <span class="number">2048</span> layers
  Total amount <span class="keyword">of</span> constant memory:               <span class="number">65536</span> bytes
  Total amount <span class="keyword">of</span> shared memory per block:       <span class="number">49152</span> bytes
  Total <span class="built_in">number</span> <span class="keyword">of</span> registers available per block: <span class="number">65536</span>
  Warp <span class="keyword">size</span>:                                     <span class="number">32</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per multiprocessor:  <span class="number">2048</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per block:           <span class="number">1024</span>
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a thread block (x,y,z): (<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">64</span>)
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a grid <span class="keyword">size</span>    (x,y,z): (<span class="number">2147483647</span>, <span class="number">65535</span>, <span class="number">65535</span>)
  Maximum memory pitch:                          <span class="number">2147483647</span> bytes
  Texture alignment:                             <span class="number">512</span> bytes
  <span class="keyword">Concurrent</span> copy <span class="keyword">and</span> kernel execution:          Yes <span class="keyword">with</span> <span class="number">2</span> copy <span class="keyword">engine</span>(s)
  Run <span class="keyword">time</span> <span class="keyword">limit</span> <span class="keyword">on</span> kernels:                     Yes
  Integrated GPU sharing Host Memory:            <span class="keyword">No</span>
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class="keyword">for</span> Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI <span class="keyword">Domain</span> ID / Bus ID / location ID:   <span class="number">0</span> / <span class="number">1</span> / <span class="number">0</span>
  Compute <span class="keyword">Mode</span>:
     &lt; <span class="keyword">Default</span> (multiple host threads can <span class="keyword">use</span> ::cudaSetDevice() <span class="keyword">with</span> device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver <span class="keyword">Version</span> = <span class="number">8.0</span>, CUDA Runtime <span class="keyword">Version</span> = <span class="number">8.0</span>, NumDevs = <span class="number">1</span>, Device0 = GeForce GTX TITAN X
Result = PASS</span>
</code></pre><p>测试3：</p>
<pre><code>cd <span class="built_in">..</span><span class="subst">/</span><span class="built_in">..</span>/<span class="number">5</span>_Simulations/nbody<span class="subst">/</span>
make
<span class="attribute">...</span><span class="attribute">...</span><span class="attribute">...</span>
<span class="built_in">.</span>/nbody <span class="attribute">-benchmark</span> <span class="attribute">-numbodies</span><span class="subst">=</span><span class="number">256000</span> <span class="attribute">-device</span><span class="subst">=</span><span class="number">0</span>
</code></pre><p>输出：</p>
<pre><code>mark -numbodies=<span class="number">256000</span> -device=<span class="number">0</span>
Run <span class="string">"nbody -benchmark [-numbodies=&lt;numBodies&gt;]"</span> <span class="keyword">to</span> measure performance.
-fullscreen       (<span class="command">run</span> n-body simulation <span class="keyword">in</span> fullscreen mode)
-fp64             (use double precision floating point values <span class="keyword">for</span> simulation)
-hostmem          (stores simulation data <span class="keyword">in</span> host memory)
-benchmark        (<span class="command">run</span> benchmark <span class="keyword">to</span> measure performance) 
-numbodies=&lt;N&gt;    (<span class="type">number</span> <span class="keyword">of</span> bodies (&gt;= <span class="number">1</span>) <span class="keyword">to</span> <span class="command">run</span> <span class="keyword">in</span> simulation) 
-device=&lt;d&gt;       (<span class="keyword">where</span> d=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2.</span>... <span class="keyword">for</span> <span class="keyword">the</span> CUDA device <span class="keyword">to</span> use)
-numdevices=&lt;i&gt;   (<span class="keyword">where</span> i=(<span class="type">number</span> <span class="keyword">of</span> CUDA devices &gt; <span class="number">0</span>) <span class="keyword">to</span> use <span class="keyword">for</span> simulation)
-compare          (compares simulation results <span class="property">running</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> default GPU <span class="keyword">and</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-cpu              (<span class="command">run</span> n-body simulation <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-tipsy=&lt;<span class="type">file</span>.bin&gt; (load a tipsy model <span class="type">file</span> <span class="keyword">for</span> simulation)

NOTE: The CUDA Samples are <span class="keyword">not</span> meant <span class="keyword">for</span> performance measurements. Results may vary when GPU Boost <span class="keyword">is</span> enabled.

&gt; Windowed mode
&gt; Simulation data stored <span class="keyword">in</span> video memory
&gt; Single precision floating point simulation
&gt; <span class="number">1</span> Devices used <span class="keyword">for</span> simulation
gpuDeviceInit() CUDA Device [<span class="number">0</span>]: <span class="string">"GeForce GTX TITAN X
&gt; Compute 5.2 CUDA device: [GeForce GTX TITAN X]
number of bodies = 256000
256000 bodies, total time for 10 iterations: 3104.433 ms
= 211.105 billion interactions per second
= 4222.091 single-precision GFLOP/s at 20 flops per interaction</span>
</code></pre><p><strong>6.安装OpenCV 3.1.0</strong></p>
<p>从官网下载zip源代码，解压到根目录下。<br>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> -y remove ffmpeg x264 libx264-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libopencv-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install build-essential checkinstall cmake pkg-config yasm
sudo apt-<span class="built_in">get</span> -y install libtiff4-<span class="built_in">dev</span> libjpeg-<span class="built_in">dev</span> libjasper-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libavcodec-<span class="built_in">dev</span> libavformat-<span class="built_in">dev</span> libswscale-<span class="built_in">dev</span> libdc1394-<span class="number">22</span>-<span class="built_in">dev</span> libxine-<span class="built_in">dev</span> libgstreamer0.10-<span class="built_in">dev</span> libgstreamer-plugins-base0.10-<span class="built_in">dev</span> libv4l-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install python-<span class="built_in">dev</span> python-numpy
sudo apt-<span class="built_in">get</span> -y install libtbb-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libqt4-<span class="built_in">dev</span> libgtk2.0-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libfaac-<span class="built_in">dev</span> libmp3lame-<span class="built_in">dev</span> libopencore-amrnb-<span class="built_in">dev</span> libopencore-amrwb-<span class="built_in">dev</span> libtheora-<span class="built_in">dev</span> libvorbis-<span class="built_in">dev</span> libxvidcore-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install x264 v4l-utils ffmpeg
sudo apt-<span class="built_in">get</span> -y install libgtk2.0-<span class="built_in">dev</span>

cd opencv-<span class="number">3.1</span>.0
mkdir build   
cd build/
cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON ..
make -j4
sudo make install
</code></pre><p>遇到的错误：Errors</p>
<pre><code><span class="keyword">error</span>: ‘NppiGraphcutState’ has <span class="keyword">not</span> been declared
<span class="keyword">error</span>: ‘NppiGraphcutState’ <span class="keyword">does</span> <span class="keyword">not</span> <span class="property">name</span> a type
...
</code></pre><p>解决方法：(由于CUDA版本高于8.0，所以需要做如下修改。在源文件中找到“graphcuts.cpp”)</p>
<p>将：</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)</span>
</code></pre><p>改为:</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)  || (CUDART_VERSION &gt;= 8000)</span>
</code></pre><p>because graphcuts is not supported directly with CUDA8 anymore.</p>
<p>安装成功后配置环境：</p>
<pre><code>sudo <span class="keyword">sh</span> -c 'echo <span class="string">"/usr/local/lib"</span> &gt; /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/opencv.<span class="keyword">conf</span>'
sudo ldconfig
</code></pre><p>测试OpenCV安装是否成功：</p>
<pre><code><span class="built_in">mkdir</span> DisplayImage  
<span class="built_in">cd</span> DisplayImage 
gedit DisplayImage.cpp 
</code></pre><p>添加代码：</p>
<pre><code><span class="built_in">#</span><span class="preprocessor"><span class="keyword">include</span> &lt;stdio.h&gt;</span>  
<span class="built_in">#</span><span class="preprocessor"><span class="keyword">include</span> &lt;opencv2/opencv.hpp&gt;</span>  
using namespace cv;  

int main<span class="params">(int argc, char** argv)</span>  
{  
     <span class="keyword">if</span><span class="params">(argc!= <span class="number">2</span>)</span>  
     {  
               printf<span class="params">(<span class="string">"usage:DisplayImage.out &lt;Image_Path&gt;\n"</span>)</span>;  
               return -<span class="number">1</span>;  
     }  

     Mat image;  
     image= imread<span class="params">(argv[<span class="number">1</span>], <span class="number">1</span>)</span>;  

&lt;span style=<span class="string">"white-space:pre"</span>&gt;    &lt;/span&gt;<span class="keyword">if</span><span class="params">(!image.data)</span>  
&lt;span style=<span class="string">"white-space:pre"</span>&gt;    &lt;/span&gt;{  
               printf<span class="params">(<span class="string">"Noimage data\n"</span>)</span>;  
               return -<span class="number">1</span>;  
     }  

     namedWindow<span class="params">(<span class="string">"DisplayImage"</span>,CV_WINDOW_AUTOSIZE)</span>;  
     imshow<span class="params">(<span class="string">"DisplayImage"</span>,image)</span>;  

     waitKey<span class="params">(<span class="number">0</span>)</span>;  
     return <span class="number">0</span>;  
}  
</code></pre><p>创建CMake文件：</p>
<pre><code>gedit CMakeLists<span class="class">.txt</span>  
</code></pre><p>添加内容：</p>
<pre><code><span class="function"><span class="title">cmake_minimum_required</span><span class="params">(VERSION <span class="number">2.8</span>)</span></span>  
<span class="function"><span class="title">project</span><span class="params">(DisplayImage)</span></span>  
<span class="function"><span class="title">find_package</span><span class="params">(OpenCV REQUIRED)</span></span>  
<span class="function"><span class="title">add_executable</span><span class="params">(DisplayImage DisplayImage.cpp)</span></span>  
<span class="function"><span class="title">target_link_libraries</span><span class="params">(DisplayImage ${OpenCV_LIBS})</span></span> 
</code></pre><p>编译：</p>
<pre><code>cmake .  
<span class="built_in">make</span> 
</code></pre><p>执行：</p>
<pre><code>./DisplayImage lena<span class="class">.jpg</span>  
</code></pre><p><strong>7.安装cudnn 5.1</strong></p>
<p>从官网下载<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cudnn-8.0-linux-x64-v5.1.tgz</a> for CUDA 8.0. 解压到当前目录：</p>
<pre><code><span class="tag">tar</span> <span class="tag">-zxvf</span> <span class="tag">cudnn-8</span><span class="class">.0-linux-x64-v5</span><span class="class">.1</span><span class="class">.tgz</span>
</code></pre><p>解压后的文件如下：</p>
<pre><code>cuda/include/cudnn<span class="class">.h</span>
cuda/lib64/libcudnn<span class="class">.so</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5.1</span>.<span class="number">5</span>
cuda/lib64/libcudnn_static.a
</code></pre><p>然后执行：</p>
<pre><code>sudo cp cuda<span class="regexp">/include/</span>cudnn.h <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/
sudo cp cuda<span class="regexp">/lib64/</span>libcudnn* <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/cudnn.h
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/libcudnn*
</code></pre><p><strong>8.安装MATLAB 2014a</strong></p>
<p>需要注意的是Ubuntu16.04 LTS的gcc版本为5.4，而Matlab2014a支持的是gcc4.7。</p>
<p>用Crack文件中的install替换matlab2014安装目录下/java/jar/下的install文件，然后执行install程序。</p>
<p>注意：选择“不联网安装”；当出现密钥时，随意输入20个数字12345-67890-12345-67890即可；需要激活时选择不要联网激活，用Crack目录下的“license_405329_R2014a.lic”文件激活。</p>
<p>安装完成之后，将Crack/linux目录下的libmwservices.so文件拷贝到/usr/local/MATLAB/R2014a/bin/glnxa64。</p>
<pre><code>$ sudo cp libmwservices.so <span class="regexp">/usr/</span>local<span class="regexp">/MATLAB/</span>R2014a<span class="regexp">/bin/g</span>lnxa64
</code></pre><p>打开Matlab并激活：</p>
<pre><code><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/MATLAB/R2014a/bin
sudo ./matlab <span class="comment"># sudo不可缺少，否则选择激活文件后报错</span>
</code></pre><p><strong>9.Python</strong></p>
<p>选用Ubuntu16.04默认的安装和配置，python版本2.7.12.</p>
<p><strong>10.BLAS安装与配置</strong></p>
<p>BLAS（基础线性代数集合）是一个应用程序接口的标准。caffe官网上推荐了三种实现：ATLAS, MKL, OpenBLAS。其中ATLAS可以直接通过命令行安装。MKL是微软开发的商业工具包，面向科研和学生免费开放。申请学生版的<a href="https://software.intel.com/en-us/qualify-for-free-software/student" target="_blank" rel="external">Parallel Studio XE Cluster Edition</a>，下载parallel_studio_xe_2017.tgz。注意接收邮件中的序列号。</p>
<pre><code>tar zxvf parallel_studio_xe_2017.tgz   <span class="comment">#解压下载文件</span>
 chmod <span class="number">777</span> parallel_studio_xe_2017 -R   <span class="comment">#获取文件权限</span>
<span class="built_in">cd</span> parallel_studio_xe_2017/
 sudo ./install_GUI.sh
</code></pre><p>安装完成之后，进行相关文件的链接：</p>
<pre><code>sudo gedit /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/intel_mkl.<span class="keyword">conf</span>
</code></pre><p>添加库文件:</p>
<pre><code><span class="regexp">/opt/i</span>ntel<span class="regexp">/lib/i</span>ntel64
<span class="regexp">/opt/i</span>ntel<span class="regexp">/mkl/</span>lib<span class="regexp">/intel64</span>
</code></pre><p>编译链接使lib文件生效：</p>
<pre><code><span class="title">sudo</span> ldconfig    
</code></pre><p>如果选择安装ATLAS，在终端输入<code>sudo apt-get install libatlas-base-dev</code>即可。</p>
<p><strong>11.Caffe的安装与配置</strong></p>
<p>Caffe是由BVLC开发的一个深度学习框架，主要由贾扬清在UC Berkeley攻读PhD期间完成。参考官网上的<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">教程</a>以及Github上针对Ubuntu15.04和16.04的<a href="https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide" target="_blank" rel="external">教程</a>。从官方下载caffe源包<a href="https://github.com/BVLC/caffe" target="_blank" rel="external">caffe-master</a>。</p>
<p>安装库文件：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libboost-<span class="built_in">all</span>-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span> protobuf-compiler
</code></pre><p>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install -y build-essential cmake git pkg-config
sudo apt-<span class="built_in">get</span> install -y libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libopencv-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> protobuf-compiler   libatlas-base-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> install --no-install-recommends libboost-<span class="built_in">all</span>-<span class="built_in">dev</span>
</code></pre><p>Python接口依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install  the <span class="keyword">python</span>-dev
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-pip
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-devsudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-numpy <span class="keyword">python</span>-scipy # (Python <span class="number">2.7</span> development <span class="keyword">files</span>)
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-devsudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-numpy <span class="keyword">python3</span>-scipy # (Python <span class="number">3.5</span> development <span class="keyword">files</span>)
</code></pre><p>在python文件夹内，使用root执行依赖项的检查与安装：</p>
<pre><code>sudo <span class="keyword">su</span>
<span class="keyword">cd</span> caffe-master/python
<span class="keyword">for</span> req <span class="keyword">in</span> $(<span class="keyword">cat</span> requirements.txt); <span class="keyword">do</span> pip install <span class="label">$req</span>; done
</code></pre><p>Makefile.config：</p>
<pre><code>cd ~/caffe-master
cp Makefile<span class="class">.config</span><span class="class">.example</span> Makefile.config
</code></pre><p>配置如下：</p>
<pre><code><span class="preprocessor">## Refer to http://caffe.berkeleyvision.org/installation.html</span>
<span class="preprocessor"># Contributions simplifying and improving our build system are welcome!</span>

<span class="preprocessor"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>
<span class="constant"> USE_CUDNN </span>:= <span class="number">1</span>

<span class="preprocessor"># CPU-only switch (uncomment to build without GPU support).</span>
<span class="preprocessor"># CPU_ONLY := 1</span>

<span class="preprocessor"># uncomment to disable IO dependencies and corresponding data layers</span>
 <span class="constant"> USE_OPENCV </span>:= <span class="number">1</span>
<span class="preprocessor"># USE_LEVELDB := 0</span>
<span class="preprocessor"># USE_LMDB := 0</span>

<span class="preprocessor"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span>
<span class="preprocessor">#    You should not set this flag if you will be reading LMDBs with any</span>
<span class="preprocessor">#    possibility of simultaneous read and write</span>
<span class="preprocessor"># ALLOW_LMDB_NOLOCK := 1</span>

<span class="preprocessor"># Uncomment if you're using OpenCV 3</span>
<span class="constant"> OPENCV_VERSION </span>:= <span class="number">3</span>

<span class="preprocessor"># To customize your choice of compiler, uncomment and set the following.</span>
<span class="preprocessor"># N.B. the default for Linux is g++ and the default for OSX is clang++</span>
<span class="preprocessor"># CUSTOM_CXX := g++</span>

<span class="preprocessor"># CUDA directory contains bin/ and lib/ directories that we need.</span>
CUDA_DIR := /usr/local/cuda
<span class="preprocessor"># On Ubuntu 14.04, if cuda tools are installed via</span>
<span class="preprocessor"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>
<span class="preprocessor"># CUDA_DIR := /usr</span>

<span class="preprocessor"># CUDA architecture setting: going with all of them.</span>
<span class="preprocessor"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span>
CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \
        -gencode arch=compute_20,code=sm_21 \
        -gencode arch=compute_30,code=sm_30 \
        -gencode arch=compute_35,code=sm_35 \
        -gencode arch=compute_50,code=sm_50 \
        -gencode arch=compute_50,code=compute_50

<span class="preprocessor"># BLAS choice:</span>
<span class="preprocessor"># atlas for ATLAS (default)</span>
<span class="preprocessor"># mkl for MKL</span>
<span class="preprocessor"># open for OpenBlas</span>
BLAS := mkl
<span class="preprocessor"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span>
<span class="preprocessor"># Leave commented to accept the defaults for your choice of BLAS</span>
<span class="preprocessor"># (which should work)!</span>
<span class="preprocessor"># BLAS_INCLUDE := /path/to/your/blas</span>
<span class="preprocessor"># BLAS_LIB := /path/to/your/blas</span>

<span class="preprocessor"># Homebrew puts openblas in a directory that is not on the standard search path</span>
<span class="preprocessor"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span>
<span class="preprocessor"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span>

<span class="preprocessor"># This is required only if you will compile the matlab interface.</span>
<span class="preprocessor"># MATLAB directory should contain the mex binary in /bin.</span>
 <span class="constant"> MATLAB_DIR </span>:= /usr/local/MATLAB/R2014a
<span class="preprocessor"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span>

<span class="preprocessor"># NOTE: this is required only if you will compile the python interface.</span>
<span class="preprocessor"># We need to be able to find Python.h and numpy/arrayobject.h.</span>
PYTHON_INCLUDE := /usr/include/python2.7 \
        /usr/local/lib/python2.7/dist-packages/numpy/core/include
<span class="preprocessor"># Anaconda Python distribution is quite popular. Include path:</span>
<span class="preprocessor"># Verify anaconda location, sometimes it's in root.</span>
<span class="preprocessor"># ANACONDA_HOME := $(HOME)/anaconda</span>
<span class="preprocessor"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>
        # $(ANACONDA_HOME)/include/python2.7 \
        # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

<span class="preprocessor"># Uncomment to use Python 3 (default is Python 2)</span>
<span class="preprocessor"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>
<span class="preprocessor"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>
<span class="preprocessor">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span>

<span class="preprocessor"># We need to be able to find libpythonX.X.so or .dylib.</span>
PYTHON_LIB := /usr/lib
<span class="preprocessor"># PYTHON_LIB := $(ANACONDA_HOME)/lib</span>

<span class="preprocessor"># Homebrew installs numpy in a non standard path (keg only)</span>
<span class="preprocessor"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span>
<span class="preprocessor"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span>

<span class="preprocessor"># Uncomment to support layers written in Python (will link against Python libs)</span>
 <span class="constant"> WITH_PYTHON_LAYER </span>:= <span class="number">1</span>

<span class="preprocessor"># Whatever else you find you need goes here.</span>
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include  /usr/include/hdf5/serial
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/share/OpenCV/<span class="number">3</span>rdparty/lib/

<span class="preprocessor"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span>
<span class="preprocessor"># INCLUDE_DIRS += $(shell brew --prefix)/include</span>
<span class="preprocessor"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span>

<span class="preprocessor"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span>
<span class="preprocessor"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span>
<span class="preprocessor"># USE_PKG_CONFIG := 1</span>

<span class="preprocessor"># N.B. both build and distribute dirs are cleared on `make clean`</span>
BUILD_DIR := build
DISTRIBUTE_DIR := distribute

<span class="preprocessor"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span>
<span class="preprocessor"># DEBUG := 1</span>

<span class="preprocessor"># The ID of the GPU that 'make runtest' will use to run unit tests.</span>
TEST_GPUID := <span class="number">0</span>

<span class="preprocessor"># enable pretty build (comment to see full commands)</span>
Q ?= @
</code></pre><p>在Makefile中配置：</p>
<pre><code><span class="label">LIBRARIES</span> += glog gflags protobuf <span class="keyword">boost_system </span><span class="keyword">boost_filesystem </span>m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs
</code></pre><p>hdf5的配置：官方说这对于Ubuntu 16.04是必须的。libhdf5的版本号需要根据实际来修改下。</p>
<pre><code>find . -<span class="keyword">type</span> f -exec sed -i -<span class="keyword">e</span> 's^<span class="string">"hdf5.h"</span>^<span class="string">"hdf5/serial/hdf5.h"</span>^<span class="keyword">g</span>' -<span class="keyword">e</span> 's^<span class="string">"hdf5_hl.h"</span>^<span class="string">"hdf5/serial/hdf5_hl.h"</span>^<span class="keyword">g</span>' '{}' \;
<span class="keyword">cd</span> /usr/lib/x86_64-linux-gnu
sudo ln -s libhdf5_serial.<span class="keyword">so</span>.10.1.0 libhdf5.<span class="keyword">so</span>
sudo ln -s libhdf5_serial_hl.<span class="keyword">so</span>.10.0.2 libhdf5_hl.<span class="keyword">so</span>
</code></pre><p>编译：</p>
<pre><code><span class="keyword">cd</span> ~/caffe-master
<span class="keyword">make</span> <span class="keyword">all</span> -j4
<span class="keyword">make</span> test -j4
<span class="keyword">make</span> runtest -j4
<span class="keyword">make</span> pycaffe -j4
<span class="keyword">make</span> matcaffe -j4
</code></pre><p>编译接口matcaffe时，有如下警告：</p>
<pre><code>Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0-6ubuntu1~16.04.2)'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
MEX completed successfully.
</code></pre><p>若OpenCV安装不正确则会在caffe编译过程中遇到如下错误：</p>
<pre><code><span class="regexp">/usr/</span>bin/<span class="string">ld:</span> cannot find -lopencv_imgcodecs
<span class="string">collect2:</span> <span class="string">error:</span> ld returned <span class="number">1</span> exit status
<span class="string">Makefile:</span><span class="number">566</span>: recipe <span class="keyword">for</span> target <span class="string">'.build_release/lib/libcaffe.so.1.0.0-rc3'</span> failed
<span class="string">make:</span> *** [.build_release<span class="regexp">/lib/</span>libcaffe.so.1.0.0-rc3] Error <span class="number">1</span>
</code></pre><p>MNIST测试：</p>
<pre><code>sh data/mnist/get_mnist<span class="class">.sh</span>  #数据预处理
sh examples/mnist/create_mnist<span class="class">.sh</span> #重建lmdb文件。Caffe支持多种数据格式: <span class="function"><span class="title">Image</span><span class="params">(.jpg, .png等)</span></span>,leveldb,lmdb,HDF5. 生成mnist-train-lmdb 和 mnist-train-lmdb文件夹，这里包含了lmdb格式的数据集
sh examples/mnist/train_lenet<span class="class">.sh</span> #训练mnist
</code></pre><p>输出：</p>
<pre><code>I<span class="number">1019 21:48</span>:<span class="number">30.078994</span> 20063 caffe.cpp:217] Using GPUs 0
I<span class="number">1019 21:48</span>:<span class="number">30.092034</span> 20063 caffe.cpp:222] GPU 0: GeForce GTX TITAN X
...
....
.....
I<span class="number">1019 21:48</span>:<span class="number">49.415398</span> 20063 solver.cpp:317] Iteration 10000, loss = <span class="number">0.00242468</span>
I<span class="number">1019 21:48</span>:<span class="number">49.415410</span> 20063 solver.cpp:337] Iteration 10000, Testing net (#0)
I<span class="number">1019 21:48</span>:<span class="number">49.479605</span> 20063 solver.cpp:404] Test net output #0: accuracy = 0.9914
I<span class="number">1019 21:48</span>:<span class="number">49.479625</span> 20063 solver.cpp:404] Test net output #1: loss = <span class="number">0.0284448</span> (* 1 = <span class="number">0.0284448</span> loss)
I<span class="number">1019 21:48</span>:<span class="number">49.479629</span> 20063 solver.cpp:322] Optimization Done.
I<span class="number">1019 21:48</span>:<span class="number">49.479632</span> 20063 caffe.cpp:254] Optimization Done.
</code></pre><p><strong>参考：</strong></p>
<p><a href="http://www.2cto.com/os/201607/528798.html" target="_blank" rel="external">ubuntu14.04+cuda8.0（GTX1080）+caffe安装</a></p>
<p><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu-16-04-nvidia-gtx-1080-cuda-8" target="_blank" rel="external">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></p>
<p><a href="http://www.jianshu.com/p/74e9c8697372" target="_blank" rel="external">深度学习框架torch/caffe/tensor/mxnet安装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习硬件配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/深度学习硬件配置/" class="article-date">
  	<time datetime="2016-10-21T04:03:58.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/深度学习硬件配置/">深度学习硬件配置</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.收到NVIDIA资助的显卡GeForce Titan X (12GB)</p>
<p>2.其他硬件设备：</p>
<pre><code>CPU： Intel i<span class="number">7</span>-<span class="number">6700</span>
主板：华硕B150M-PLUS (LGA<span class="number">1151</span>)
内存：<span class="number">4</span>GB x <span class="number">2</span>
硬盘：希捷<span class="number">1</span><span class="keyword">TB</span>
电源：鑫谷Segotep GP700G（金牌认证、宽幅） 额定<span class="number">600</span><span class="keyword">W</span> 
机箱：Tt小板机箱
显卡接口转换器：DVI-&gt;VGI
</code></pre><p>3.备注</p>
<pre><code>深度学习对CPU的要求并不是特别高，根据实际情况选择。
主板建议还是选择一个好的品牌，预算充足，可以考虑<span class="keyword">X</span><span class="number">99</span>平台。
显卡GTX<span class="number">1080</span>比老Titan <span class="keyword">X</span>的性价比要高。
内存建议<span class="number">32</span>G(<span class="number">16</span>x<span class="number">2</span>)，近期内存价格暴涨。
硬盘，最好配SSD，用来存放软件和数据集，提升IO效率。
电源根据显卡和整机功率需求选择(TitanX+CPU等差不多<span class="number">350</span><span class="keyword">W</span>)。
机箱可扩展，散热好即可。
</code></pre><p>4.组装<br>    主板安装CPU及散热器，将主板安装到机箱内(面板接口要仔细)，然后按照电源、硬盘，最后将电源线依次插入相应供电接口位置。</p>
<p>5.参考</p>
<p><a href="http://www.jianshu.com/p/0198ad851b16" target="_blank" rel="external">个人深度学习环境搭建：主机配置与组装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MacOS安装torch7" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/07/MacOS安装torch7/" class="article-date">
  	<time datetime="2016-04-07T08:07:25.000Z" itemprop="datePublished">2016-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/07/MacOS安装torch7/">MacOS安装torch7</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>手动安装：</p>
<pre><code>$ # <span class="keyword">in</span> a terminal, run the commands
$ git clone <span class="string">https:</span><span class="comment">//github.com/torch/distro.git ~/torch --recursive</span>
$ cd ~/torch; bash install-deps;
$ ./install.sh

－－－－－－－－－－－－－－－－－－－－－－－－－

$ source <span class="regexp">~/torch/</span>install<span class="regexp">/bin/</span>torch-activate

－－－－－－－－－－－－－－－－－－－－－－－－－
$ th

   ______             __   |  Torch7                                         
 <span class="regexp">/_  __/</span>__  ________<span class="regexp">/ /</span>   |  Scientific computing <span class="keyword">for</span> Lua. 
   <span class="regexp">/ /</span> <span class="regexp">/ _ \/</span> __<span class="regexp">/ __/</span> _ \  |  Type ? for help                                
 /_/  \___/_/  \__/_//_/  |  https:<span class="comment">//github.com/torch         </span>
                         |  <span class="string">http:</span><span class="comment">//torch.ch                  </span>

th&gt; torch.Tensor{<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>}
 <span class="number">5</span>
<span class="number">6</span>
 <span class="number">7</span>
[torch.DoubleTensor of size <span class="number">3</span>]
</code></pre><p>退出 th&gt;</p>
<pre><code>os.<span class="function"><span class="title">exit</span><span class="params">()</span></span>
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MacOS安装caffe" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/07/MacOS安装caffe/" class="article-date">
  	<time datetime="2016-04-06T17:03:37.000Z" itemprop="datePublished">2016-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/07/MacOS安装caffe/">MacOS安装caffe</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.需要安装CUDA，不管有没有N卡<br>2.安装OpenBLAS，并在caffe设置文件中设置为：</p>
<pre><code><span class="preprocessor"># BLAS choice:</span>
<span class="preprocessor"># atlas for ATLAS (default)</span>
<span class="preprocessor"># mkl for MKL</span>
<span class="preprocessor"># open for OpenBlas</span>
BLAS := open
<span class="preprocessor"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span>
<span class="preprocessor"># Leave commented to accept the defaults for your choice of BLAS</span>
<span class="preprocessor"># (which should work)!</span>
BLAS_INCLUDE := /opt/OpenBLAS/include
BLAS_LIB := /opt/OpenBLAS/lib
</code></pre><p>3.安装依赖库，可用<code>brew list</code>检查安装是否完全<br>4.添加路径：</p>
<pre><code>export DYLD_FALLBACK_LIBRARY_PATH=<span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span><span class="string">lib:</span><span class="regexp">/usr/</span>local/lib  

export DYLD_FALLBACK_LIBRARY_PATH=<span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span><span class="string">lib:</span>$HOME<span class="regexp">/anaconda/</span><span class="string">lib:</span><span class="regexp">/usr/</span>local<span class="regexp">/lib:/</span>usr<span class="regexp">/lib:/</span>opt<span class="regexp">/OpenBLAS/</span><span class="string">lib:</span><span class="regexp">/opt/</span>OpenBLAS
</code></pre><p>5.下载caffe源文件，编译：<code>make all</code>时报错：</p>
<pre><code>    PROTOC src/caffe/proto/caffe.proto  
    make: protoc: No such <span class="built_in">file</span> <span class="operator">or</span> <span class="built_in">directory</span> 

解决： 

    sudo chown yourname /usr/<span class="built_in">local</span>  

    brew link yourlibpackage
</code></pre><p>6.Build: 继续编译：<code>make test</code>， <code>make runtest</code></p>
<pre><code>添加路径：

    <span class="built_in">export</span> DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib

    <span class="comment">#python</span>
    <span class="keyword">for</span> req <span class="keyword">in</span> $(cat python/requirements.txt); <span class="keyword">do</span> pip install <span class="variable">$req</span>; <span class="keyword">done</span>
    make pycaffe
    <span class="built_in">export</span> PYTHONPATH=~/technologies/caffe/python/:<span class="variable">$PYTHONPATH</span>
    <span class="built_in">cd</span> ..
</code></pre><p>make runtest的结果：</p>
<pre><code>[----------] <span class="number">10</span> tests from PowerLayerTest/<span class="number">0</span>, where TypeParam = N5caffe9CPUDeviceIfEE
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPower</span>
[       OK ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPower</span> (<span class="number">2</span> ms)
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerZeroGradient</span>
[       OK ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerZeroGradient</span> (<span class="number">1</span> ms)
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerTwoGradient</span>
...
[----------] <span class="number">10</span> tests from PowerLayerTest/<span class="number">0</span> (<span class="number">16</span> ms total)
</code></pre><p>Bugs:</p>
<pre><code>    <span class="keyword">library</span> <span class="keyword">not</span> found <span class="keyword">for</span> -lboost_python

解决： 

    brew install boost-python
</code></pre><hr>
<h3 id="测试">测试</h3><p>MNIST</p>
<p>下载mnist数据，如下代码，报错，需安装<code>brew install wget</code></p>
<pre><code>./<span class="typedef"><span class="keyword">data</span>/mnist/get_mnist.sh </span>
</code></pre><p>转数据：</p>
<pre><code>./examples/mnist/create_mnist.sh
<span class="variable">Creating</span> lmdb...
<span class="variable">Done</span>.

./examples/mnist/train_lenet.sh
</code></pre><p>问题：</p>
<pre><code>Cannot use GPU in CPU-only Caffe:<span class="instruction"> check </span>mode.
</code></pre><p>解决： 修改lenet_solver.prototxt 最后一行的GPU改为CPU，继续执行<code>./examples/mnist/train_lenet.sh</code></p>
<pre><code>I<span class="number">0407 00:51</span>:<span class="number">59.023721</span> <span class="number">2035871744</span> caffe.cpp:178] Use CPU.
I<span class="number">0407 00:51</span>:<span class="number">59.024705</span> <span class="number">2035871744</span> solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_test.prototxt"
I<span class="number">0407 00:51</span>:<span class="number">59.024927</span> <span class="number">2035871744</span> solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I<span class="number">0407 00:51</span>:<span class="number">59.026877</span> <span class="number">2035871744</span> net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I<span class="number">0407 00:51</span>:<span class="number">59.026897</span> <span class="number">2035871744</span> net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy

...结果...

I<span class="number">0407 00:57</span>:<span class="number">52.664399</span> <span class="number">2035871744</span> solver.cpp:317] Iteration 10000, loss = <span class="number">0.0032766</span>
I<span class="number">0407 00:57</span>:<span class="number">52.664429</span> <span class="number">2035871744</span> solver.cpp:337] Iteration 10000, Testing net (#0)
I<span class="number">0407 00:57</span>:<span class="number">54.761032</span> <span class="number">2035871744</span> solver.cpp:404]     Test net output #0: accuracy = 0.9901
I<span class="number">0407 00:57</span>:<span class="number">54.761072</span> <span class="number">2035871744</span> solver.cpp:404]     Test net output #1: loss = <span class="number">0.0290336</span> (* 1 = <span class="number">0.0290336</span> loss)
I<span class="number">0407 00:57</span>:<span class="number">54.761081</span> <span class="number">2035871744</span> solver.cpp:322] Optimization Done.
I<span class="number">0407 00:57</span>:<span class="number">54.761087</span> <span class="number">2035871744</span> caffe.cpp:222] Optimization Done.
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Gang Wang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>