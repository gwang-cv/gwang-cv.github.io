<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Hello World</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hello World">
<meta property="og:url" content="http://gwang-cv.github.io/index.html">
<meta property="og:site_name" content="Hello World">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hello World">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Gang Wang</a></h1>
		</hgroup>

		
		<p class="header-subtitle">a computer vision researchGO</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>About</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Home</a></li>
				        
							<li><a href="/archives">Archives</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
					        
								<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
					        
								<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/DeepLearning/" style="font-size: 16.67px;">DeepLearning</a> <a href="/tags/ML/" style="font-size: 10px;">ML</a> <a href="/tags/Mac/" style="font-size: 20px;">Mac</a> <a href="/tags/Math/" style="font-size: 10px;">Math</a> <a href="/tags/Python/" style="font-size: 13.33px;">Python</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">Hello, I&#39;m Gang Wang. This is my blog, enjoy it.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Gang Wang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="https://sites.google.com/site/2013gwang/logss.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Gang Wang</h1>
			</hgroup>
			
			<p class="header-subtitle">a computer vision researchGO</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Home</a></li>
		        
					<li><a href="/archives">Archives</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/gwang-cv" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/messiwang" title="weibo">weibo</a>
			        
						<a class="google" target="_blank" href="https://sites.google.com/site/2013gwang/" title="google">google</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-Clarifai API体验" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/02/Clarifai API体验/" class="article-date">
  	<time datetime="2016-11-02T07:24:53.000Z" itemprop="datePublished">2016-11-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/02/Clarifai API体验/">Clarifai API体验</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>参考官方给出的文档：<a href="https://developer.clarifai.com/guide/tag#guide-tag-responses" target="_blank" rel="external">https://developer.clarifai.com/guide/tag#guide-tag-responses</a>，在本机进行简单的体验。</p>
<p>首先注册账户，然后创建Application，获取ID及Secret，并在Application页面下生成“Access Token”码：XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX</p>
<p>〇、客户端API参考：<a href="https://github.com/Clarifai/clarifai-python" target="_blank" rel="external">https://github.com/Clarifai/clarifai-python</a></p>
<pre><code>pip install clarifai==<span class="number">2.0</span>.10
clarifai config
<span class="label">CLARIFAI_APP_ID:</span>
<span class="label">CLARIFAI_APP_SECRET:</span>
</code></pre><p>在python中实现测试：</p>
<pre><code><span class="keyword">from</span> clarifai.rest <span class="keyword">import</span> ClarifaiApp
app = ClarifaiApp()
model = app.models.get(<span class="string">'general-v1.3'</span>)
<span class="built_in">print</span> model.predict_by_url(<span class="string">'https://samples.clarifai.com/metro-north.jpg'</span>)
<span class="comment"># or local image</span>
<span class="built_in">print</span> model.predict_by_filename(<span class="string">'/Users/USER/my_image.jpeg'</span>)
</code></pre><p>一、打开MAC终端，输入Request命令：</p>
<ol>
<li><p>测试在线图片</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> --data-urlencode <span class="string">"url=https://samples.clarifai.com/metro-north.jpg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
<li><p>测试本地图片</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> -<span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
<li><p>测试多幅图像</p>
<pre><code>curl "https://api.clarifai.com/v1/tag/" \
      -<span class="ruby"><span class="constant">X</span> <span class="constant">POST</span> -<span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image1.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image2.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image3.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">F</span> <span class="string">"encoded_data=@/Users/USER/my_image4.jpeg"</span> \
</span>      -<span class="ruby"><span class="constant">H</span> <span class="string">"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX"</span></span>
</code></pre></li>
</ol>
<p>二、读取Response结果</p>
<p>在python中使用json来解析文本结果：</p>
<pre><code># -*- coding: utf-<span class="number">8</span> -*-
<span class="keyword">import</span> json
<span class="keyword">for</span> <span class="built_in">line</span> in <span class="built_in">open</span>(<span class="string">"response.txt"</span>):
    <span class="built_in">print</span> <span class="built_in">line</span>
<span class="built_in">str</span>=json.loads(<span class="built_in">line</span>)
results=<span class="built_in">str</span>[<span class="string">'results'</span>]
strNum=len(<span class="built_in">str</span>[<span class="string">'results'</span>])
<span class="keyword">for</span> ind in range(<span class="number">0</span>,strNum):
    <span class="built_in">print</span> ind
    <span class="built_in">print</span> results[ind][<span class="string">'result'</span>][<span class="string">'tag'</span>][<span class="string">'classes'</span>]
</code></pre><p>输出解析结果（图像的tag类别）：</p>
<pre><code><span class="number">0</span>
[<span class="string">u'sketch'</span>, <span class="string">u'illustration'</span>, <span class="string">u'cute'</span>, <span class="string">u'man'</span>, <span class="string">u'no person'</span>, <span class="string">u'funny'</span>, <span class="string">u'fun'</span>, <span class="string">u'vector'</span>, <span class="string">u'character'</span>, <span class="string">u'graphic design'</span>, <span class="string">u'business'</span>, <span class="string">u'child'</span>, <span class="string">u'art'</span>, <span class="string">u'retro'</span>, <span class="string">u'love'</span>, <span class="string">u'moon'</span>, <span class="string">u'Halloween'</span>, <span class="string">u'graphic'</span>, <span class="string">u'design'</span>, <span class="string">u'isolated'</span>]
<span class="number">1</span>
[<span class="string">u'illustration'</span>, <span class="string">u'vector'</span>, <span class="string">u'sketch'</span>, <span class="string">u'retro'</span>, <span class="string">u'design'</span>, <span class="string">u'sketch'</span>, <span class="string">u'business'</span>, <span class="string">u'symbol'</span>, <span class="string">u'man'</span>, <span class="string">u'family'</span>, <span class="string">u'no person'</span>, <span class="string">u'graphic'</span>, <span class="string">u'humor'</span>, <span class="string">u'people'</span>, <span class="string">u'outdoors'</span>, <span class="string">u'image'</span>, <span class="string">u'art'</span>, <span class="string">u'nature'</span>, <span class="string">u'house'</span>, <span class="string">u'animal'</span>]
<span class="number">2</span>
[<span class="string">u'vector'</span>, <span class="string">u'vector'</span>, <span class="string">u'illustration'</span>, <span class="string">u'no person'</span>, <span class="string">u'internet'</span>, <span class="string">u'technology'</span>, <span class="string">u'design'</span>, <span class="string">u'symbol'</span>, <span class="string">u'graphic design'</span>, <span class="string">u'data'</span>, <span class="string">u'flat'</span>, <span class="string">u'data'</span>, <span class="string">u'business'</span>, <span class="string">u'stripe'</span>, <span class="string">u'set'</span>, <span class="string">u'design'</span>, <span class="string">u'square'</span>, <span class="string">u'science'</span>, <span class="string">u'education'</span>, <span class="string">u'creativity'</span>]
<span class="number">3</span>
[<span class="string">u'sleeve'</span>, <span class="string">u'illustration'</span>, <span class="string">u'isolated'</span>, <span class="string">u'polo'</span>, <span class="string">u'shirt'</span>, <span class="string">u'vector'</span>, <span class="string">u'design'</span>, <span class="string">u'image'</span>, <span class="string">u'wear'</span>, <span class="string">u'garment'</span>, <span class="string">u'sale'</span>, <span class="string">u'man'</span>, <span class="string">u'fashion'</span>, <span class="string">u'shopping'</span>, <span class="string">u'casual'</span>, <span class="string">u'front'</span>, <span class="string">u'shop'</span>, <span class="string">u'apparel'</span>, <span class="string">u'graphic'</span>, <span class="string">u'flat'</span>]
<span class="number">4</span>
[<span class="string">u'illustration'</span>, <span class="string">u'vector'</span>, <span class="string">u'sketch'</span>, <span class="string">u'Halloween'</span>, <span class="string">u'cute'</span>, <span class="string">u'animal'</span>, <span class="string">u'skittish'</span>, <span class="string">u'funny'</span>, <span class="string">u'design'</span>, <span class="string">u'art'</span>, <span class="string">u'graphic'</span>, <span class="string">u'fun'</span>, <span class="string">u'ghost'</span>, <span class="string">u'scary'</span>, <span class="string">u'no person'</span>, <span class="string">u'vicious'</span>, <span class="string">u'desktop'</span>, <span class="string">u'retro'</span>, <span class="string">u'image'</span>, <span class="string">u'business'</span>]
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-DL学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/24/DL学习笔记/" class="article-date">
  	<time datetime="2016-10-24T01:23:03.000Z" itemprop="datePublished">2016-10-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/24/DL学习笔记/">DL学习笔记</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h3 id="（一）DeepLearning_(DL)概述">（一）DeepLearning (DL)概述</h3><h4 id="1-_什么是DL？">1. 什么是DL？</h4><p>机器学习ML的框架：</p>
<p>(1)数据：\({(x_i,y_i)}, 1\le i \le m\)</p>
<p>(2)模型：\(\mathcal{F}={f(x;\theta)}, \theta\in\Theta\)</p>
<p>i. 线性： \(y=f(x)=w^Tx+b\) 【x——&gt;y】</p>
<p>ii. 广义线性：\(y=f(x)=w^T\phi(x)+b\)  【x——&gt;[\(\bar{x}=\phi(x)\)]——&gt;y，其中的\(\phi\)为模式识别中的特征Feature，这一步也称为特征学习。】</p>
<p>iii. 非线性：人工神经网络（ANN）</p>
<p>(3)准则：损失函数\(L(y,f(x))\)</p>
<p>经验风险：\(R(\theta)=\frac{1}{m}\sum_{i=1}^m{L(y,f(x_i,\theta))}\)</p>
<p>正则化项：\(|w|_2^2\)</p>
<p>Minimizing： \(R(\theta)+\lambda|w|_2^2\)，或稀疏正则项L1.</p>
<p>因此转化为一个最优化问题。</p>
<p>神经网络ANN中 \(y=\sigma(\sum_i{w_i x_i+b})\) 相当于从P维到Q维的一个映射函数。则DL就是解决这个深度前馈神经网络的算法。</p>
<h4 id="2-_存在的困难及挑战">2. 存在的困难及挑战</h4><p>可训练的<strong>参数太多</strong>；【参数过多带来的问题具体包括：计算资源要大，数据要多(否则出现过拟合)，算法效率要高】</p>
<p>多层网络以后的优化问题变为<strong>非凸优化</strong>问题；</p>
<p><strong>梯度弥散</strong>问题，即从网络层由上往下的参数调节变得非常困难；</p>
<p>解释困难(可通过一些可视化的方法一定程度上来进行解释)；</p>
<h4 id="3-算法历史">3.算法历史</h4><p>1958年，感知机Perception：一个神经细胞的处理能力较差，与或运算无法实现。</p>
<p>1986年，神经网络的概念出现：BP算法，对浅层网络做了很多的工作。一方面受限于当时的硬件和软件问题。</p>
<p>1998年，CNN卷积神经网络，在手写体识别中取得了成功。—LeCun</p>
<p>2006年，DBN深度置信网络，—Hinton</p>
<h4 id="4-为什么学习DL">4.为什么学习DL</h4><p>有效！【语音识别，目标识别，NLP，CV….】</p>
<h4 id="5-领域概述">5.领域概述</h4><p>学术机构：</p>
<p>TorontoU，Hinton，1975年EdinburghU’s PHD;<br>NewYorkU，LeCun，1987年PHD;<br>MentrealU，Bengio，1991年McGillU’s PHD;<br>StanfordU, Ng，2003年UCBerkeley’s PHD;</p>
<p>学术会议：<br>NIPS，ICML，ICRL，…</p>
<p>参考: <a href="http://v.youku.com/v_show/id_XNjU1MzU4NDIw.html?f=21508721&amp;o=1" target="_blank" rel="external">深度学习课程-概述</a></p>
<h3 id="（二）FNN_&amp;_BP">（二）FNN &amp; BP</h3><h4 id="一-前馈神经网络FNN">一.前馈神经网络FNN</h4><h5 id="1-神经元、神经层、神经网">1.神经元、神经层、神经网</h5><p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/SingleNeuron.png" alt=""></p>
<pre><code>神经元
x_1 -w_1-\
..  -w_i<span class="comment">--〇z---&gt;a</span>
x_p -w_p-/
</code></pre><p>一个神经元的输出是一个线性函数与一个非线性函数的复合：\([z=\sum w_ix_i+b,~a=f(z)]=&gt;a=f(\sum w_ix_i+b)\). </p>
<p>其中激活函数包括：sigmod: \(\sigma(x)=\frac{1}{1+e^x};~\tan(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}};~|x|;~\)等</p>
<pre><code><span class="comment">神经层</span>
<span class="comment">x_1</span> <span class="literal">-</span><span class="comment">w_1</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">\</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">a_1</span> 
              <span class="comment">X</span>
<span class="string">.</span><span class="string">.</span>  <span class="literal">-</span><span class="comment">w_i</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">X</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="string">.</span><span class="string">.</span><span class="string">.</span>
              <span class="comment">X</span>
<span class="comment">x_p</span> <span class="literal">-</span><span class="comment">w_p</span><span class="literal">-</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span><span class="comment">/</span><span class="literal">-</span><span class="comment">〇</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">a_p</span>
</code></pre><p>神经层上的每个神经元的输入时相同的，但权值是不同的。我们用 \( a^{(l)}_i \)表示第 \( l \)层第 \( i \)单元的激活值（输出值）。</p>
<p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/Network331.png" alt=""></p>
<pre><code>神经网
-<span class="ruby">〇-\    -〇-\
</span>-<span class="ruby">〇--〇-<span class="regexp">/-〇--〇-...
</span></span>-<span class="ruby">〇--〇-\-〇--〇-...
</span>-<span class="ruby">〇-<span class="regexp">/    -〇-/</span>
</span>n_1  n_2  n_3   ...   n_L
</code></pre><h5 id="2-记号">2.记号</h5><p>超参数(并不是学习出来的，而是认为根据需要设定的参数，也称元参数)：层数L，第l层的神经元个数\(n^{(l)}\)，神经元非线性函数\(f_l()\).</p>
<p>要学习的参数：连接权weight参数\(w_i^{(1)},w_i^{(2)},..w_i^{(L)}\)，两层之间的连接权。 偏bias参数\(b^{(1)},b^{(2)},..b^{(L)}\).</p>
<p>第l层神经元的状态\(z^{(l)},1\le l\le L\)；第l层神经元的激活activation：\(a^{(l)}\)</p>
<h5 id="3-前馈计算">3.前馈计算</h5><p>(1)基本公式</p>
<p>\(z^{(l+1)}=w^l a^l+b^l  \)</p>
<p>\(a^l=f_l(z^l) \) </p>
<p>\(h_{w,b}(x)=a^{(l+1)}=f(z^{(l+1)})\)</p>
<p>(2)前馈计算(\(W=(w^1,…,w^l),b=(b^1,..,b^l)\))</p>
<p>\(l=1, a^l=x\) </p>
<p>计算步骤：\(a^1-&gt;z^2-&gt;a^2-&gt;…-&gt;z^L-&gt;a^L\) </p>
<p><img src="https://raw.githubusercontent.com/gwang-cv/gwang-cv.github.io/master/img/Network3322.png" alt=""></p>
<p>“目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是\(n<em>l\)层的神经网络，第1层是输入层，第 \(n_l\)层是输出层，中间的每个层 \(l\)与层 \(l+1\)紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 \(L_2\)层的所有激活值，然后是第 \(L_3\)层的激活值，以此类推，直到第 \(L</em>{n_l}\)层。这是一个前馈神经网络的例子，因为这种联接图没有闭环或回路。”——<a href="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="external">UFLDL</a></p>
<h5 id="4-应用于ML">4.应用于ML</h5><p>数据D：\((x_i,y_i),1\le i\le N\)</p>
<p>模型M：\(y=h(x|w,b)\)</p>
<p>准则C: \(\sum_i|y_i-a^l(x|w,b)|^2+\lambda|w|_F^2) \)=min</p>
<p>其中，第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过度拟合: \(||w||<em>F^2=\sum_i\sum_jw</em>{ij}^2\)</p>
<p>权重衰减参数 \( \lambda \)用于控制公式中两项的相对重要性。</p>
<p>使用梯度下降法进行求解这个优化问题。</p>
<p>将上式写为：\(\sum_i J(x_i,y_i;w,b)+\lambda|w|_F^2\)=min</p>
<p>目标函数关于待求参数的导数：</p>
<p>其中：$$\frac{\partial|w|_F^2}{\partial w}=2w$$</p>
<p>然后重点求:$$\frac{\partial \sum J(.)}{\partial w};~~\frac{\partial \sum J(.)}{\partial b}$$</p>
<p>迭代公式：<br>$$w^{t+1}=w^t-\alpha \frac{\partial \sum J(.)}{\partial w}$$<br>$$b^{t+1}=b^t-\alpha \frac{\partial \sum J(.)}{\partial b}$$<br>其中\(\alpha\)是学习速率。</p>
<h4 id="二-BP算法">二.BP算法</h4><p>“我们的目标是针对参数 \( W \)和 \( b \)来求其函数 \( J(W,b) \)的最小值。为了求解神经网络，我们需要将每一个参数 \( W^{(l)}<em>{ij} \)和 \( b^{(l)}_i \)初始化为一个很小的、接近零的随机值（比如说，使用正态分布 \( {Normal}(0,\epsilon^2) \)生成的随机值，其中 \( \epsilon \)设置为 \( 0.01 \) ），之后对目标函数使用诸如批量梯度下降法的最优化算法。因为 \( J(W, b) \)是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。最后，需要再次强调的是，要将参数进行随机初始化，而不是全部置为  0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，对于所有 \( i\)，\( W^{(1)}</em>{ij}\)都会取相同的值，那么对于任何输入 \( x \)都会有：\( a^{(2)}_1 = a^{(2)}_2 = a^{(2)}_3 = \ldots \)）。随机初始化的目的是使对称失效。”——<a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external">UFLDL</a></p>
<h5 id="1-多元函数的偏导数">1.多元函数的偏导数</h5><p>(1)<br>$$X=[x_1,…,x_p]^T\in R^p,~ y=f(X)=f(x_1,…,x_p)$$<br>$$\nabla_x f(x)=\nabla_x y=\frac{\partial y}{\partial x}=[\frac{\partial f(x_1)}{\partial x},…,\frac{\partial f(x_p)}{\partial x}]^T\in R^p$$<br>(2)<br>$$x\in R^p, y=[..]^T\in R^q$$<br>$$y=[f_1(x),..,f_q(x)]^T$$<br>$$\nabla_xy=\nabla_x f(x)=\frac{\partial y}{\partial x}=[\frac{\partial f(x)}{\partial x_i}]^T\in R^{p\times q}$$<br>(3) 导数法则：链式法则</p>
<h5 id="2-BP算法">2.BP算法</h5><p>BP思路：给定一个样例 \( (x,y)\)，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \( h_{W,b}(x) \)的输出值。之后，针对第 \( l \)层的每一个节点 \( i\)，我们计算出其“残差” \( \delta^{(l)}_i\)，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 \( \delta^{(n_l)}_i \)（第 \( n_l \)层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点（第 \( l+1 \)层节点）残差的加权平均值计算 \( \delta^{(l)}_i\)，这些节点以 \( a^{(l)}_i \)作为输入。</p>
<p>BP算法步骤：</p>
<p>(i)进行前馈传导计算，利用前向传导公式，得到 \( L<em>2, L_3, \ldots \) 直到输出层 \( L</em>{n_l} \)的激活值。</p>
<p>(ii)对于第 \( n_l \)层（输出层）的每个输出单元 \( i\)，我们根据以下公式计算残差：</p>
<p>$$\delta_i^{(n_l)}= \frac{\partial J}{\partial z_i^{n_l}}=-(y_i-a_i^{(n_l)})\cdot f’(z_i^{(n_l)})$$</p>
<p>(iii)令\(\delta^l=\frac{\partial J}{\partial z^l}\)，则对 \(  l = n_l-1, n_l-2, n_l-3, \ldots, 2 \)的各个层，第 \(  l \)层的第 \(  i \)个节点的残差计算方法如下：（矩阵向量形式）</p>
<p>$$\begin{equation}\begin{split}\delta^{l}&amp;=\frac{\partial J}{\partial z^{l}}=\frac{\partial a^l}{\partial z^l}\frac{\partial z^{l+1}}{\partial a^l}\frac{\partial J}{\partial z^{l+1}}\\<br>&amp;=diag(f’(z^l))\cdot((w^l)^T\cdot\delta^{l+1})\\<br>&amp;=(f’_l(z^l))\odot((w^l)^T\cdot\delta^{l+1})\\<br>&amp;=((w^l)^T\cdot\delta^{l+1})\odot(f’_l(z^l))<br>\end{split}\end{equation}$$<br>以上逐次从后向前求导的过程即为“反向传导（BP）”的本意所在.</p>
<p>(iv)计算所需的偏导数：<br>$$\frac{\partial J}{\partial w^l}=\delta^{l+1}\cdot(a^l)^T$$<br>$$\frac{\partial J}{\partial b^l}=\delta^{l+1}$$</p>
<p>其中，假设 \( f(z) \)是sigmoid函数，并且我们已经在前向传导运算中得到了 \( a^{(l)}_i\)。那么，使用我们早先推导出的 \( f’(z)\)表达式，就可以计算得到 \( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)\)。</p>
<h5 id="3-梯度下降法求解过程">3.梯度下降法求解过程</h5><p>(1) 对所有 \(  l\)，令 \(  \Delta W^{(l)} := 0 \),  \(  \Delta b^{(l)} := 0 \)（设置为全零矩阵或全零向量）</p>
<p>(2) For     \(  i = 1 \) to \(  m\)，使用反向传播算法计算: </p>
<p>\(\nabla_{W^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \nabla_{b^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \Delta W^{(l)} := \Delta W^{(l)} + \nabla_{W^{(l)}} J(W,b;x,y)  \)</p>
<p>\( \Delta b^{(l)} := \Delta b^{(l)} + \nabla_{b^{(l)}} J(W,b;x,y) \)</p>
<p>(3) 更新参数：<br>$$ \begin{align}<br>W^{(l)} &amp;= W^{(l)} - \alpha \left[ \left(\frac{1}{m} \Delta W^{(l)} \right) + \lambda W^{(l)}\right] \\<br>b^{(l)} &amp;= b^{(l)} - \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]<br>\end{align}$$</p>
<p>重复梯度下降法的迭代步骤来减小代价函数 \( J(W,b)\) ，以训练我们的神经网络。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Clarifai" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/Clarifai/" class="article-date">
  	<time datetime="2016-10-21T13:40:07.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/Clarifai/">Clarifai图像自动化标签</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Clarifai 公司：www.clarifai.com</p>
<p>Clarifai创始人Matt Zeiler是New York University (NYU) Rob Fergus教授门下的学生。从上个世纪开始，NYU就一直是neural computation的重镇。现在Deep net的前身ConvNet，就是出自 NYU 的 Yann LeCun教授组.</p>
<p>ImageNet Large Scale Visual Recognition Competition 2013 (ILSVRC2013)</p>
<p>其中Matt Zeiler (<a href="http://Clarifai.com" target="_blank" rel="external">http://Clarifai.com</a>) 的算法排名第一，在不用额外训练数据的情况下，跑到了error rate 0.1174这样的成绩。</p>
<p>这个成绩是这样解读的：任选一张图片，扔给算法，算法返回5个结果。如果5个结果中，有一个猜对了物体类别，就算正确。换言之，如果允许猜5次，Clarifai已经有接近90%的准确率了。这里的物体类别包括了英语中两万多个名词，几乎涵盖了各大类别。</p>
<p><a href="https://developer.clarifai.com/guide/" target="_blank" rel="external">Clarifai API</a></p>
<p>参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/19821292" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/19821292</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Ubuntu16.04+Titan X+CUDA8.0+cudnn5" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/" class="article-date">
  	<time datetime="2016-10-21T07:49:55.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/">Ubuntu16.04+Titan X+CUDA8.0+cudnn5.1+Caffe</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>1.安装Ubuntu16.04 LTS x64</strong></p>
<p>利用工具rufus制作USB系统盘(官方下载64位版本: <a href="http://www.ubuntu.com/download/alternative-downloads" target="_blank" rel="external">ubuntu-16.04-desktop-amd64.iso</a>)，因为已有Win7系统，此处选择“Install Ubuntu alongside Windows Boot Manager”，分区采用默认选择，语言选择English，安装完毕。</p>
<p><em>注：此时显示器VGA接口接到主板集成显卡接口上。</em></p>
<p><strong>2.更新源</strong></p>
<pre><code>cd /etc/apt/
sudo cp sources<span class="class">.list</span> sources<span class="class">.list</span><span class="class">.bak</span>
sudo gedit sources.list
</code></pre><p>在sources.list文件头部添加如下源：</p>
<pre><code>deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse</span>
deb-src <span class="string">http:</span><span class="comment">//mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse</span>
</code></pre><p>然后更新源和安装的包:</p>
<pre><code>sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>
sudo apt-<span class="built_in">get</span> upgrade
</code></pre><p><strong>3.安装NVIDIA显卡驱动</strong></p>
<p>采用ppa安装方式，没选择最新的nvidia-370，我选择了nvidia-367。</p>
<pre><code>sudo<span class="instruction"> add-apt-repository </span>ppa:graphics-drivers/ppa
sudo apt-get update
sudo apt-get install nvidia-367
sudo apt-get install mesa-common-dev
sudo apt-get install freeglut3-dev
sudo reboot
</code></pre><p><em>将显示器VGA接口换到NVIDIA显卡上。</em></p>
<p><strong>4.修改分辨率</strong></p>
<p>启动到界面之后发现分辨率只有1366x768，显示器适合1920x1080，采用xrandr并修改xorg.conf来解决。</p>
<pre><code>sudo gedit /etc/X11/xorg<span class="class">.conf</span>
修改如下：
HorizSync <span class="number">31.0</span> - <span class="number">84.0</span>
VertRefresh <span class="number">56.0</span>-<span class="number">77.0</span>
</code></pre><p>注销系统再次登录后，选择适合的桌面分辨率即可。</p>
<p><strong>5.安装CUDA8.0</strong></p>
<p>到官网下载<a href="https://developer.nvidia.com/cuda-release-candidate-download" target="_blank" rel="external">cuda_8.0.44_linux.run</a>，复制到根目录下。</p>
<pre><code>sudo sh cuda_8.0.44_linux.<span class="command">run</span> <span class="comment">--tmpdir=/tmp/</span>
</code></pre><p>遇到问题：incomplete installation，然后执行</p>
<pre><code>sudo apt-<span class="built_in">get</span> install freeglut3-<span class="built_in">dev</span> build-essential libx11-<span class="built_in">dev</span> libxmu-<span class="built_in">dev</span> libxi-<span class="built_in">dev</span> libgl1-mesa-glx libglu1-mesa libglu1-mesa-<span class="built_in">dev</span>
sudo sh cuda_8.0.44_linux.run -silent -driver
</code></pre><p>注：此时安装过程中提示是否要安装NVIDIA驱动时选择no。其他选择yes或默认即可。</p>
<pre><code>Install NVIDIA Accelerated Graphics Driver <span class="keyword">for</span> Linux-x86_64 <span class="number">361.62</span>? <span class="params">(y)</span>es/<span class="params">(n)</span>o/<span class="params">(q)</span>uit: n
</code></pre><p>安装完毕后声明环境变量：</p>
<pre><code><span class="title">sudo</span> gedit ~/.bashrc
</code></pre><p>在.bashrc尾部添加如下内容：</p>
<pre><code>export <span class="constant">PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/bin</span><span class="variable">${</span><span class="constant">PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">PATH</span>}}
export <span class="constant">LD_LIBRARY_PATH</span>=<span class="regexp">/usr/local</span><span class="regexp">/cuda-8.0/lib</span>64<span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span><span class="symbol">:+</span><span class="symbol">:</span><span class="variable">${</span><span class="constant">LD_LIBRARY_PATH</span>}}
</code></pre><p>测试下安装是否成功：</p>
<p>测试1：</p>
<pre><code>cd NVIDI<span class="built_in">A_CUDA</span>-<span class="number">8.0</span>_Samples/
nvidia-smi
</code></pre><p>输出：</p>
<pre><code>Tue Oct 18 15:20:34 2016       
+-----------------------------------------------------------------------------+
|<span class="string"> NVIDIA-SMI 367.44                 Driver Version: 367.44                    </span>|
|<span class="string">-------------------------------+----------------------+----------------------+
</span>|<span class="string"> GPU  Name        Persistence-M</span>|<span class="string"> Bus-Id        Disp.A </span>|<span class="string"> Volatile Uncorr. ECC </span>|
|<span class="string"> Fan  Temp  Perf  Pwr:Usage/Cap</span>|<span class="string">         Memory-Usage </span>|<span class="string"> GPU-Util  Compute M. </span>|
|<span class="string">===============================+======================+======================</span>|
|<span class="string">   0  GeForce GTX TIT...  Off  </span>|<span class="string"> 0000:01:00.0      On </span>|<span class="string">                  N/A </span>|
|<span class="string"> 22%   48C    P5    27W / 250W </span>|<span class="string">    169MiB / 12205MiB </span>|<span class="string">      1%      Default </span>|
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
|<span class="string"> Processes:                                                       GPU Memory </span>|
|<span class="string">  GPU       PID  Type  Process name                               Usage      </span>|
|<span class="string">=============================================================================</span>|
|<span class="string">    0      2421    G   /usr/lib/xorg/Xorg                             105MiB </span>|
|<span class="string">    0     10062    G   compiz                                          63MiB </span>|
+-----------------------------------------------------------------------------+
</code></pre><p>测试2：</p>
<pre><code>cd <span class="number">1</span>_Utilities/deviceQuery
make
<span class="attribute">...</span><span class="attribute">...</span><span class="built_in">..
</span><span class="built_in">.</span>/deviceQuery 
</code></pre><p>输出：</p>
<pre><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "GeForce GTX TITAN X"
  CUDA Driver Version / Runtime Version          8.0 / 8.0
  CUDA Capability Major/Minor version number:    5.2
  Total amount of global memory:                 12205 MBytes (12798197760 bytes)
  (24) Multiprocessors, (128) CUDA Cores/MP:     3072 CUDA Cores
  GPU Max Clock rate:                            1076 MHz (1.08 GHz)
  Memory Clock rate:                             3505 Mhz
  Memory Bus Width:                              384-bit
  L2 <span class="operator"><span class="keyword">Cache</span> <span class="keyword">Size</span>:                                 <span class="number">3145728</span> bytes
  Maximum Texture Dimension <span class="keyword">Size</span> (x,y,z)         <span class="number">1</span>D=(<span class="number">65536</span>), <span class="number">2</span>D=(<span class="number">65536</span>, <span class="number">65536</span>), <span class="number">3</span>D=(<span class="number">4096</span>, <span class="number">4096</span>, <span class="number">4096</span>)
  Maximum Layered <span class="number">1</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">1</span>D=(<span class="number">16384</span>), <span class="number">2048</span> layers
  Maximum Layered <span class="number">2</span>D Texture <span class="keyword">Size</span>, (num) layers  <span class="number">2</span>D=(<span class="number">16384</span>, <span class="number">16384</span>), <span class="number">2048</span> layers
  Total amount <span class="keyword">of</span> constant memory:               <span class="number">65536</span> bytes
  Total amount <span class="keyword">of</span> shared memory per block:       <span class="number">49152</span> bytes
  Total <span class="built_in">number</span> <span class="keyword">of</span> registers available per block: <span class="number">65536</span>
  Warp <span class="keyword">size</span>:                                     <span class="number">32</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per multiprocessor:  <span class="number">2048</span>
  Maximum <span class="built_in">number</span> <span class="keyword">of</span> threads per block:           <span class="number">1024</span>
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a thread block (x,y,z): (<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">64</span>)
  <span class="keyword">Max</span> dimension <span class="keyword">size</span> <span class="keyword">of</span> a grid <span class="keyword">size</span>    (x,y,z): (<span class="number">2147483647</span>, <span class="number">65535</span>, <span class="number">65535</span>)
  Maximum memory pitch:                          <span class="number">2147483647</span> bytes
  Texture alignment:                             <span class="number">512</span> bytes
  <span class="keyword">Concurrent</span> copy <span class="keyword">and</span> kernel execution:          Yes <span class="keyword">with</span> <span class="number">2</span> copy <span class="keyword">engine</span>(s)
  Run <span class="keyword">time</span> <span class="keyword">limit</span> <span class="keyword">on</span> kernels:                     Yes
  Integrated GPU sharing Host Memory:            <span class="keyword">No</span>
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class="keyword">for</span> Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI <span class="keyword">Domain</span> ID / Bus ID / location ID:   <span class="number">0</span> / <span class="number">1</span> / <span class="number">0</span>
  Compute <span class="keyword">Mode</span>:
     &lt; <span class="keyword">Default</span> (multiple host threads can <span class="keyword">use</span> ::cudaSetDevice() <span class="keyword">with</span> device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver <span class="keyword">Version</span> = <span class="number">8.0</span>, CUDA Runtime <span class="keyword">Version</span> = <span class="number">8.0</span>, NumDevs = <span class="number">1</span>, Device0 = GeForce GTX TITAN X
Result = PASS</span>
</code></pre><p>测试3：</p>
<pre><code>cd <span class="built_in">..</span><span class="subst">/</span><span class="built_in">..</span>/<span class="number">5</span>_Simulations/nbody<span class="subst">/</span>
make
<span class="attribute">...</span><span class="attribute">...</span><span class="attribute">...</span>
<span class="built_in">.</span>/nbody <span class="attribute">-benchmark</span> <span class="attribute">-numbodies</span><span class="subst">=</span><span class="number">256000</span> <span class="attribute">-device</span><span class="subst">=</span><span class="number">0</span>
</code></pre><p>输出：</p>
<pre><code>mark -numbodies=<span class="number">256000</span> -device=<span class="number">0</span>
Run <span class="string">"nbody -benchmark [-numbodies=&lt;numBodies&gt;]"</span> <span class="keyword">to</span> measure performance.
-fullscreen       (<span class="command">run</span> n-body simulation <span class="keyword">in</span> fullscreen mode)
-fp64             (use double precision floating point values <span class="keyword">for</span> simulation)
-hostmem          (stores simulation data <span class="keyword">in</span> host memory)
-benchmark        (<span class="command">run</span> benchmark <span class="keyword">to</span> measure performance) 
-numbodies=&lt;N&gt;    (<span class="type">number</span> <span class="keyword">of</span> bodies (&gt;= <span class="number">1</span>) <span class="keyword">to</span> <span class="command">run</span> <span class="keyword">in</span> simulation) 
-device=&lt;d&gt;       (<span class="keyword">where</span> d=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2.</span>... <span class="keyword">for</span> <span class="keyword">the</span> CUDA device <span class="keyword">to</span> use)
-numdevices=&lt;i&gt;   (<span class="keyword">where</span> i=(<span class="type">number</span> <span class="keyword">of</span> CUDA devices &gt; <span class="number">0</span>) <span class="keyword">to</span> use <span class="keyword">for</span> simulation)
-compare          (compares simulation results <span class="property">running</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> default GPU <span class="keyword">and</span> once <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-cpu              (<span class="command">run</span> n-body simulation <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">the</span> CPU)
-tipsy=&lt;<span class="type">file</span>.bin&gt; (load a tipsy model <span class="type">file</span> <span class="keyword">for</span> simulation)

NOTE: The CUDA Samples are <span class="keyword">not</span> meant <span class="keyword">for</span> performance measurements. Results may vary when GPU Boost <span class="keyword">is</span> enabled.

&gt; Windowed mode
&gt; Simulation data stored <span class="keyword">in</span> video memory
&gt; Single precision floating point simulation
&gt; <span class="number">1</span> Devices used <span class="keyword">for</span> simulation
gpuDeviceInit() CUDA Device [<span class="number">0</span>]: <span class="string">"GeForce GTX TITAN X
&gt; Compute 5.2 CUDA device: [GeForce GTX TITAN X]
number of bodies = 256000
256000 bodies, total time for 10 iterations: 3104.433 ms
= 211.105 billion interactions per second
= 4222.091 single-precision GFLOP/s at 20 flops per interaction</span>
</code></pre><p><strong>6.安装OpenCV 3.1.0</strong></p>
<p>从官网下载zip源代码，解压到根目录下。<br>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> -y remove ffmpeg x264 libx264-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libopencv-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install build-essential checkinstall cmake pkg-config yasm
sudo apt-<span class="built_in">get</span> -y install libtiff4-<span class="built_in">dev</span> libjpeg-<span class="built_in">dev</span> libjasper-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libavcodec-<span class="built_in">dev</span> libavformat-<span class="built_in">dev</span> libswscale-<span class="built_in">dev</span> libdc1394-<span class="number">22</span>-<span class="built_in">dev</span> libxine-<span class="built_in">dev</span> libgstreamer0.10-<span class="built_in">dev</span> libgstreamer-plugins-base0.10-<span class="built_in">dev</span> libv4l-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install python-<span class="built_in">dev</span> python-numpy
sudo apt-<span class="built_in">get</span> -y install libtbb-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libqt4-<span class="built_in">dev</span> libgtk2.0-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install libfaac-<span class="built_in">dev</span> libmp3lame-<span class="built_in">dev</span> libopencore-amrnb-<span class="built_in">dev</span> libopencore-amrwb-<span class="built_in">dev</span> libtheora-<span class="built_in">dev</span> libvorbis-<span class="built_in">dev</span> libxvidcore-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> -y install x264 v4l-utils ffmpeg
sudo apt-<span class="built_in">get</span> -y install libgtk2.0-<span class="built_in">dev</span>

cd opencv-<span class="number">3.1</span>.0
mkdir build   
cd build/
cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON ..
make -j4
sudo make install
</code></pre><p>遇到的错误：Errors</p>
<pre><code><span class="keyword">error</span>: ‘NppiGraphcutState’ has <span class="keyword">not</span> been declared
<span class="keyword">error</span>: ‘NppiGraphcutState’ <span class="keyword">does</span> <span class="keyword">not</span> <span class="property">name</span> a type
...
</code></pre><p>解决方法：(由于CUDA版本高于8.0，所以需要做如下修改。在源文件中找到“graphcuts.cpp”)</p>
<p>将：</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)</span>
</code></pre><p>改为:</p>
<pre><code><span class="preprocessor">#<span class="keyword">include</span> "precomp.hpp"</span>
<span class="preprocessor">#<span class="keyword">if</span> !defined (HAVE_CUDA) || defined (CUDA_DISABLER)  || (CUDART_VERSION &gt;= 8000)</span>
</code></pre><p>because graphcuts is not supported directly with CUDA8 anymore.</p>
<p>安装成功后配置环境：</p>
<pre><code>sudo <span class="keyword">sh</span> -c 'echo <span class="string">"/usr/local/lib"</span> &gt; /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/opencv.<span class="keyword">conf</span>'
sudo ldconfig
</code></pre><p>测试OpenCV安装是否成功：</p>
<pre><code><span class="built_in">mkdir</span> DisplayImage  
<span class="built_in">cd</span> DisplayImage 
gedit DisplayImage.cpp 
</code></pre><p>添加代码：</p>
<pre><code><span class="built_in">#</span><span class="preprocessor"><span class="keyword">include</span> &lt;stdio.h&gt;</span>  
<span class="built_in">#</span><span class="preprocessor"><span class="keyword">include</span> &lt;opencv2/opencv.hpp&gt;</span>  
using namespace cv;  

int main<span class="params">(int argc, char** argv)</span>  
{  
     <span class="keyword">if</span><span class="params">(argc!= <span class="number">2</span>)</span>  
     {  
               printf<span class="params">(<span class="string">"usage:DisplayImage.out &lt;Image_Path&gt;\n"</span>)</span>;  
               return -<span class="number">1</span>;  
     }  

     Mat image;  
     image= imread<span class="params">(argv[<span class="number">1</span>], <span class="number">1</span>)</span>;  

&lt;span style=<span class="string">"white-space:pre"</span>&gt;    &lt;/span&gt;<span class="keyword">if</span><span class="params">(!image.data)</span>  
&lt;span style=<span class="string">"white-space:pre"</span>&gt;    &lt;/span&gt;{  
               printf<span class="params">(<span class="string">"Noimage data\n"</span>)</span>;  
               return -<span class="number">1</span>;  
     }  

     namedWindow<span class="params">(<span class="string">"DisplayImage"</span>,CV_WINDOW_AUTOSIZE)</span>;  
     imshow<span class="params">(<span class="string">"DisplayImage"</span>,image)</span>;  

     waitKey<span class="params">(<span class="number">0</span>)</span>;  
     return <span class="number">0</span>;  
}  
</code></pre><p>创建CMake文件：</p>
<pre><code>gedit CMakeLists<span class="class">.txt</span>  
</code></pre><p>添加内容：</p>
<pre><code><span class="function"><span class="title">cmake_minimum_required</span><span class="params">(VERSION <span class="number">2.8</span>)</span></span>  
<span class="function"><span class="title">project</span><span class="params">(DisplayImage)</span></span>  
<span class="function"><span class="title">find_package</span><span class="params">(OpenCV REQUIRED)</span></span>  
<span class="function"><span class="title">add_executable</span><span class="params">(DisplayImage DisplayImage.cpp)</span></span>  
<span class="function"><span class="title">target_link_libraries</span><span class="params">(DisplayImage ${OpenCV_LIBS})</span></span> 
</code></pre><p>编译：</p>
<pre><code>cmake .  
<span class="built_in">make</span> 
</code></pre><p>执行：</p>
<pre><code>./DisplayImage lena<span class="class">.jpg</span>  
</code></pre><p><strong>7.安装cudnn 5.1</strong></p>
<p>从官网下载<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cudnn-8.0-linux-x64-v5.1.tgz</a> for CUDA 8.0. 解压到当前目录：</p>
<pre><code><span class="tag">tar</span> <span class="tag">-zxvf</span> <span class="tag">cudnn-8</span><span class="class">.0-linux-x64-v5</span><span class="class">.1</span><span class="class">.tgz</span>
</code></pre><p>解压后的文件如下：</p>
<pre><code>cuda/include/cudnn<span class="class">.h</span>
cuda/lib64/libcudnn<span class="class">.so</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5</span>
cuda/lib64/libcudnn<span class="class">.so</span>.<span class="number">5.1</span>.<span class="number">5</span>
cuda/lib64/libcudnn_static.a
</code></pre><p>然后执行：</p>
<pre><code>sudo cp cuda<span class="regexp">/include/</span>cudnn.h <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/
sudo cp cuda<span class="regexp">/lib64/</span>libcudnn* <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>include/cudnn.h
sudo chmod a+r <span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span>lib64/libcudnn*
</code></pre><p><strong>8.安装MATLAB 2014a</strong></p>
<p>需要注意的是Ubuntu16.04 LTS的gcc版本为5.4，而Matlab2014a支持的是gcc4.7。</p>
<p>用Crack文件中的install替换matlab2014安装目录下/java/jar/下的install文件，然后执行install程序。</p>
<p>注意：选择“不联网安装”；当出现密钥时，随意输入20个数字12345-67890-12345-67890即可；需要激活时选择不要联网激活，用Crack目录下的“license_405329_R2014a.lic”文件激活。</p>
<p>安装完成之后，将Crack/linux目录下的libmwservices.so文件拷贝到/usr/local/MATLAB/R2014a/bin/glnxa64。</p>
<pre><code>$ sudo cp libmwservices.so <span class="regexp">/usr/</span>local<span class="regexp">/MATLAB/</span>R2014a<span class="regexp">/bin/g</span>lnxa64
</code></pre><p>打开Matlab并激活：</p>
<pre><code><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/MATLAB/R2014a/bin
sudo ./matlab <span class="comment"># sudo不可缺少，否则选择激活文件后报错</span>
</code></pre><p><strong>9.Python</strong></p>
<p>选用Ubuntu16.04默认的安装和配置，python版本2.7.12.</p>
<p><strong>10.BLAS安装与配置</strong></p>
<p>BLAS（基础线性代数集合）是一个应用程序接口的标准。caffe官网上推荐了三种实现：ATLAS, MKL, OpenBLAS。其中ATLAS可以直接通过命令行安装。MKL是微软开发的商业工具包，面向科研和学生免费开放。申请学生版的<a href="https://software.intel.com/en-us/qualify-for-free-software/student" target="_blank" rel="external">Parallel Studio XE Cluster Edition</a>，下载parallel_studio_xe_2017.tgz。注意接收邮件中的序列号。</p>
<pre><code>tar zxvf parallel_studio_xe_2017.tgz   <span class="comment">#解压下载文件</span>
 chmod <span class="number">777</span> parallel_studio_xe_2017 -R   <span class="comment">#获取文件权限</span>
<span class="built_in">cd</span> parallel_studio_xe_2017/
 sudo ./install_GUI.sh
</code></pre><p>安装完成之后，进行相关文件的链接：</p>
<pre><code>sudo gedit /etc/ld.<span class="keyword">so</span>.<span class="keyword">conf</span>.<span class="keyword">d</span>/intel_mkl.<span class="keyword">conf</span>
</code></pre><p>添加库文件:</p>
<pre><code><span class="regexp">/opt/i</span>ntel<span class="regexp">/lib/i</span>ntel64
<span class="regexp">/opt/i</span>ntel<span class="regexp">/mkl/</span>lib<span class="regexp">/intel64</span>
</code></pre><p>编译链接使lib文件生效：</p>
<pre><code><span class="title">sudo</span> ldconfig    
</code></pre><p>如果选择安装ATLAS，在终端输入<code>sudo apt-get install libatlas-base-dev</code>即可。</p>
<p><strong>11.Caffe的安装与配置</strong></p>
<p>Caffe是由BVLC开发的一个深度学习框架，主要由贾扬清在UC Berkeley攻读PhD期间完成。参考官网上的<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">教程</a>以及Github上针对Ubuntu15.04和16.04的<a href="https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide" target="_blank" rel="external">教程</a>。从官方下载caffe源包<a href="https://github.com/BVLC/caffe" target="_blank" rel="external">caffe-master</a>。</p>
<p>安装库文件：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libboost-<span class="built_in">all</span>-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span> protobuf-compiler
</code></pre><p>安装依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install -y build-essential cmake git pkg-config
sudo apt-<span class="built_in">get</span> install -y libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libopencv-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span> protobuf-compiler   libatlas-base-<span class="built_in">dev</span> libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> install --no-install-recommends libboost-<span class="built_in">all</span>-<span class="built_in">dev</span>
</code></pre><p>Python接口依赖：</p>
<pre><code>sudo apt-<span class="built_in">get</span> install  the <span class="keyword">python</span>-dev
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-pip
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-devsudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python</span>-numpy <span class="keyword">python</span>-scipy # (Python <span class="number">2.7</span> development <span class="keyword">files</span>)
sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-devsudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> <span class="keyword">python3</span>-numpy <span class="keyword">python3</span>-scipy # (Python <span class="number">3.5</span> development <span class="keyword">files</span>)
</code></pre><p>在python文件夹内，使用root执行依赖项的检查与安装：</p>
<pre><code>sudo <span class="keyword">su</span>
<span class="keyword">cd</span> caffe-master/python
<span class="keyword">for</span> req <span class="keyword">in</span> $(<span class="keyword">cat</span> requirements.txt); <span class="keyword">do</span> pip install <span class="label">$req</span>; done
</code></pre><p>Makefile.config：</p>
<pre><code>cd ~/caffe-master
cp Makefile<span class="class">.config</span><span class="class">.example</span> Makefile.config
</code></pre><p>配置如下：</p>
<pre><code><span class="preprocessor">## Refer to http://caffe.berkeleyvision.org/installation.html</span>
<span class="preprocessor"># Contributions simplifying and improving our build system are welcome!</span>

<span class="preprocessor"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>
<span class="constant"> USE_CUDNN </span>:= <span class="number">1</span>

<span class="preprocessor"># CPU-only switch (uncomment to build without GPU support).</span>
<span class="preprocessor"># CPU_ONLY := 1</span>

<span class="preprocessor"># uncomment to disable IO dependencies and corresponding data layers</span>
 <span class="constant"> USE_OPENCV </span>:= <span class="number">1</span>
<span class="preprocessor"># USE_LEVELDB := 0</span>
<span class="preprocessor"># USE_LMDB := 0</span>

<span class="preprocessor"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span>
<span class="preprocessor">#    You should not set this flag if you will be reading LMDBs with any</span>
<span class="preprocessor">#    possibility of simultaneous read and write</span>
<span class="preprocessor"># ALLOW_LMDB_NOLOCK := 1</span>

<span class="preprocessor"># Uncomment if you're using OpenCV 3</span>
<span class="constant"> OPENCV_VERSION </span>:= <span class="number">3</span>

<span class="preprocessor"># To customize your choice of compiler, uncomment and set the following.</span>
<span class="preprocessor"># N.B. the default for Linux is g++ and the default for OSX is clang++</span>
<span class="preprocessor"># CUSTOM_CXX := g++</span>

<span class="preprocessor"># CUDA directory contains bin/ and lib/ directories that we need.</span>
CUDA_DIR := /usr/local/cuda
<span class="preprocessor"># On Ubuntu 14.04, if cuda tools are installed via</span>
<span class="preprocessor"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>
<span class="preprocessor"># CUDA_DIR := /usr</span>

<span class="preprocessor"># CUDA architecture setting: going with all of them.</span>
<span class="preprocessor"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span>
CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \
        -gencode arch=compute_20,code=sm_21 \
        -gencode arch=compute_30,code=sm_30 \
        -gencode arch=compute_35,code=sm_35 \
        -gencode arch=compute_50,code=sm_50 \
        -gencode arch=compute_50,code=compute_50

<span class="preprocessor"># BLAS choice:</span>
<span class="preprocessor"># atlas for ATLAS (default)</span>
<span class="preprocessor"># mkl for MKL</span>
<span class="preprocessor"># open for OpenBlas</span>
BLAS := mkl
<span class="preprocessor"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span>
<span class="preprocessor"># Leave commented to accept the defaults for your choice of BLAS</span>
<span class="preprocessor"># (which should work)!</span>
<span class="preprocessor"># BLAS_INCLUDE := /path/to/your/blas</span>
<span class="preprocessor"># BLAS_LIB := /path/to/your/blas</span>

<span class="preprocessor"># Homebrew puts openblas in a directory that is not on the standard search path</span>
<span class="preprocessor"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span>
<span class="preprocessor"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span>

<span class="preprocessor"># This is required only if you will compile the matlab interface.</span>
<span class="preprocessor"># MATLAB directory should contain the mex binary in /bin.</span>
 <span class="constant"> MATLAB_DIR </span>:= /usr/local/MATLAB/R2014a
<span class="preprocessor"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span>

<span class="preprocessor"># NOTE: this is required only if you will compile the python interface.</span>
<span class="preprocessor"># We need to be able to find Python.h and numpy/arrayobject.h.</span>
PYTHON_INCLUDE := /usr/include/python2.7 \
        /usr/local/lib/python2.7/dist-packages/numpy/core/include
<span class="preprocessor"># Anaconda Python distribution is quite popular. Include path:</span>
<span class="preprocessor"># Verify anaconda location, sometimes it's in root.</span>
<span class="preprocessor"># ANACONDA_HOME := $(HOME)/anaconda</span>
<span class="preprocessor"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>
        # $(ANACONDA_HOME)/include/python2.7 \
        # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

<span class="preprocessor"># Uncomment to use Python 3 (default is Python 2)</span>
<span class="preprocessor"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>
<span class="preprocessor"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>
<span class="preprocessor">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span>

<span class="preprocessor"># We need to be able to find libpythonX.X.so or .dylib.</span>
PYTHON_LIB := /usr/lib
<span class="preprocessor"># PYTHON_LIB := $(ANACONDA_HOME)/lib</span>

<span class="preprocessor"># Homebrew installs numpy in a non standard path (keg only)</span>
<span class="preprocessor"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span>
<span class="preprocessor"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span>

<span class="preprocessor"># Uncomment to support layers written in Python (will link against Python libs)</span>
 <span class="constant"> WITH_PYTHON_LAYER </span>:= <span class="number">1</span>

<span class="preprocessor"># Whatever else you find you need goes here.</span>
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include  /usr/include/hdf5/serial
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/share/OpenCV/<span class="number">3</span>rdparty/lib/

<span class="preprocessor"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span>
<span class="preprocessor"># INCLUDE_DIRS += $(shell brew --prefix)/include</span>
<span class="preprocessor"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span>

<span class="preprocessor"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span>
<span class="preprocessor"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span>
<span class="preprocessor"># USE_PKG_CONFIG := 1</span>

<span class="preprocessor"># N.B. both build and distribute dirs are cleared on `make clean`</span>
BUILD_DIR := build
DISTRIBUTE_DIR := distribute

<span class="preprocessor"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span>
<span class="preprocessor"># DEBUG := 1</span>

<span class="preprocessor"># The ID of the GPU that 'make runtest' will use to run unit tests.</span>
TEST_GPUID := <span class="number">0</span>

<span class="preprocessor"># enable pretty build (comment to see full commands)</span>
Q ?= @
</code></pre><p>在Makefile中配置：</p>
<pre><code><span class="label">LIBRARIES</span> += glog gflags protobuf <span class="keyword">boost_system </span><span class="keyword">boost_filesystem </span>m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs
</code></pre><p>hdf5的配置：官方说这对于Ubuntu 16.04是必须的。libhdf5的版本号需要根据实际来修改下。</p>
<pre><code>find . -<span class="keyword">type</span> f -exec sed -i -<span class="keyword">e</span> 's^<span class="string">"hdf5.h"</span>^<span class="string">"hdf5/serial/hdf5.h"</span>^<span class="keyword">g</span>' -<span class="keyword">e</span> 's^<span class="string">"hdf5_hl.h"</span>^<span class="string">"hdf5/serial/hdf5_hl.h"</span>^<span class="keyword">g</span>' '{}' \;
<span class="keyword">cd</span> /usr/lib/x86_64-linux-gnu
sudo ln -s libhdf5_serial.<span class="keyword">so</span>.10.1.0 libhdf5.<span class="keyword">so</span>
sudo ln -s libhdf5_serial_hl.<span class="keyword">so</span>.10.0.2 libhdf5_hl.<span class="keyword">so</span>
</code></pre><p>编译：</p>
<pre><code><span class="keyword">cd</span> ~/caffe-master
<span class="keyword">make</span> <span class="keyword">all</span> -j4
<span class="keyword">make</span> test -j4
<span class="keyword">make</span> runtest -j4
<span class="keyword">make</span> pycaffe -j4
<span class="keyword">make</span> matcaffe -j4
</code></pre><p>编译接口matcaffe时，有如下警告：</p>
<pre><code>Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
Warning: You are <span class="keyword">using</span> gcc <span class="built_in">version</span> <span class="string">'5.4.0-6ubuntu1~16.04.2)'</span>. The <span class="built_in">version</span> <span class="operator">of</span> gcc is <span class="operator">not</span> supported. The <span class="built_in">version</span> currently supported <span class="operator">with</span> MEX is <span class="string">'4.7.x'</span>. For <span class="operator">a</span> list <span class="operator">of</span> currently supported compilers see: <span class="keyword">http</span>://www.mathworks.com/support/compilers/current_release.
MEX completed successfully.
</code></pre><p>若OpenCV安装不正确则会在caffe编译过程中遇到如下错误：</p>
<pre><code><span class="regexp">/usr/</span>bin/<span class="string">ld:</span> cannot find -lopencv_imgcodecs
<span class="string">collect2:</span> <span class="string">error:</span> ld returned <span class="number">1</span> exit status
<span class="string">Makefile:</span><span class="number">566</span>: recipe <span class="keyword">for</span> target <span class="string">'.build_release/lib/libcaffe.so.1.0.0-rc3'</span> failed
<span class="string">make:</span> *** [.build_release<span class="regexp">/lib/</span>libcaffe.so.1.0.0-rc3] Error <span class="number">1</span>
</code></pre><p>MNIST测试：</p>
<pre><code>sh data/mnist/get_mnist<span class="class">.sh</span>  #数据预处理
sh examples/mnist/create_mnist<span class="class">.sh</span> #重建lmdb文件。Caffe支持多种数据格式: <span class="function"><span class="title">Image</span><span class="params">(.jpg, .png等)</span></span>,leveldb,lmdb,HDF5. 生成mnist-train-lmdb 和 mnist-train-lmdb文件夹，这里包含了lmdb格式的数据集
sh examples/mnist/train_lenet<span class="class">.sh</span> #训练mnist
</code></pre><p>输出：</p>
<pre><code>I<span class="number">1019 21:48</span>:<span class="number">30.078994</span> 20063 caffe.cpp:217] Using GPUs 0
I<span class="number">1019 21:48</span>:<span class="number">30.092034</span> 20063 caffe.cpp:222] GPU 0: GeForce GTX TITAN X
...
....
.....
I<span class="number">1019 21:48</span>:<span class="number">49.415398</span> 20063 solver.cpp:317] Iteration 10000, loss = <span class="number">0.00242468</span>
I<span class="number">1019 21:48</span>:<span class="number">49.415410</span> 20063 solver.cpp:337] Iteration 10000, Testing net (#0)
I<span class="number">1019 21:48</span>:<span class="number">49.479605</span> 20063 solver.cpp:404] Test net output #0: accuracy = 0.9914
I<span class="number">1019 21:48</span>:<span class="number">49.479625</span> 20063 solver.cpp:404] Test net output #1: loss = <span class="number">0.0284448</span> (* 1 = <span class="number">0.0284448</span> loss)
I<span class="number">1019 21:48</span>:<span class="number">49.479629</span> 20063 solver.cpp:322] Optimization Done.
I<span class="number">1019 21:48</span>:<span class="number">49.479632</span> 20063 caffe.cpp:254] Optimization Done.
</code></pre><p><strong>参考：</strong></p>
<p><a href="http://www.2cto.com/os/201607/528798.html" target="_blank" rel="external">ubuntu14.04+cuda8.0（GTX1080）+caffe安装</a></p>
<p><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu-16-04-nvidia-gtx-1080-cuda-8" target="_blank" rel="external">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></p>
<p><a href="http://www.jianshu.com/p/74e9c8697372" target="_blank" rel="external">深度学习框架torch/caffe/tensor/mxnet安装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习硬件配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/21/深度学习硬件配置/" class="article-date">
  	<time datetime="2016-10-21T04:03:58.000Z" itemprop="datePublished">2016-10-21</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/21/深度学习硬件配置/">深度学习硬件配置</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.收到NVIDIA资助的显卡GeForce Titan X (12GB)</p>
<p>2.其他硬件设备：</p>
<pre><code>CPU： Intel i<span class="number">7</span>-<span class="number">6700</span>
主板：华硕B150M-PLUS (LGA<span class="number">1151</span>)
内存：<span class="number">4</span>GB x <span class="number">2</span>
硬盘：希捷<span class="number">1</span><span class="keyword">TB</span>
电源：鑫谷Segotep GP700G（金牌认证、宽幅） 额定<span class="number">600</span><span class="keyword">W</span> 
机箱：Tt小板机箱
显卡接口转换器：DVI-&gt;VGI
</code></pre><p>3.备注</p>
<pre><code>深度学习对CPU的要求并不是特别高，根据实际情况选择。
主板建议还是选择一个好的品牌，预算充足，可以考虑<span class="keyword">X</span><span class="number">99</span>平台。
显卡GTX<span class="number">1080</span>比老Titan <span class="keyword">X</span>的性价比要高。
内存建议<span class="number">32</span>G(<span class="number">16</span>x<span class="number">2</span>)，近期内存价格暴涨。
硬盘，最好配SSD，用来存放软件和数据集，提升IO效率。
电源根据显卡和整机功率需求选择(TitanX+CPU等差不多<span class="number">350</span><span class="keyword">W</span>)。
机箱可扩展，散热好即可。
</code></pre><p>4.组装<br>    主板安装CPU及散热器，将主板安装到机箱内(面板接口要仔细)，然后按照电源、硬盘，最后将电源线依次插入相应供电接口位置。</p>
<p>5.参考</p>
<p><a href="http://www.jianshu.com/p/0198ad851b16" target="_blank" rel="external">个人深度学习环境搭建：主机配置与组装</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MacOS安装torch7" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/07/MacOS安装torch7/" class="article-date">
  	<time datetime="2016-04-07T08:07:25.000Z" itemprop="datePublished">2016-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/07/MacOS安装torch7/">MacOS安装torch7</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>手动安装：</p>
<pre><code>$ # <span class="keyword">in</span> a terminal, run the commands
$ git clone <span class="string">https:</span><span class="comment">//github.com/torch/distro.git ~/torch --recursive</span>
$ cd ~/torch; bash install-deps;
$ ./install.sh

－－－－－－－－－－－－－－－－－－－－－－－－－

$ source <span class="regexp">~/torch/</span>install<span class="regexp">/bin/</span>torch-activate

－－－－－－－－－－－－－－－－－－－－－－－－－
$ th

   ______             __   |  Torch7                                         
 <span class="regexp">/_  __/</span>__  ________<span class="regexp">/ /</span>   |  Scientific computing <span class="keyword">for</span> Lua. 
   <span class="regexp">/ /</span> <span class="regexp">/ _ \/</span> __<span class="regexp">/ __/</span> _ \  |  Type ? for help                                
 /_/  \___/_/  \__/_//_/  |  https:<span class="comment">//github.com/torch         </span>
                         |  <span class="string">http:</span><span class="comment">//torch.ch                  </span>

th&gt; torch.Tensor{<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>}
 <span class="number">5</span>
<span class="number">6</span>
 <span class="number">7</span>
[torch.DoubleTensor of size <span class="number">3</span>]
</code></pre><p>退出 th&gt;</p>
<pre><code>os.<span class="function"><span class="title">exit</span><span class="params">()</span></span>
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-MacOS安装caffe" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/07/MacOS安装caffe/" class="article-date">
  	<time datetime="2016-04-06T17:03:37.000Z" itemprop="datePublished">2016-04-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/07/MacOS安装caffe/">MacOS安装caffe</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.需要安装CUDA，不管有没有N卡<br>2.安装OpenBLAS，并在caffe设置文件中设置为：</p>
<pre><code><span class="preprocessor"># BLAS choice:</span>
<span class="preprocessor"># atlas for ATLAS (default)</span>
<span class="preprocessor"># mkl for MKL</span>
<span class="preprocessor"># open for OpenBlas</span>
BLAS := open
<span class="preprocessor"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span>
<span class="preprocessor"># Leave commented to accept the defaults for your choice of BLAS</span>
<span class="preprocessor"># (which should work)!</span>
BLAS_INCLUDE := /opt/OpenBLAS/include
BLAS_LIB := /opt/OpenBLAS/lib
</code></pre><p>3.安装依赖库，可用<code>brew list</code>检查安装是否完全<br>4.添加路径：</p>
<pre><code>export DYLD_FALLBACK_LIBRARY_PATH=<span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span><span class="string">lib:</span><span class="regexp">/usr/</span>local/lib  

export DYLD_FALLBACK_LIBRARY_PATH=<span class="regexp">/usr/</span>local<span class="regexp">/cuda/</span><span class="string">lib:</span>$HOME<span class="regexp">/anaconda/</span><span class="string">lib:</span><span class="regexp">/usr/</span>local<span class="regexp">/lib:/</span>usr<span class="regexp">/lib:/</span>opt<span class="regexp">/OpenBLAS/</span><span class="string">lib:</span><span class="regexp">/opt/</span>OpenBLAS
</code></pre><p>5.下载caffe源文件，编译：<code>make all</code>时报错：</p>
<pre><code>    PROTOC src/caffe/proto/caffe.proto  
    make: protoc: No such <span class="built_in">file</span> <span class="operator">or</span> <span class="built_in">directory</span> 

解决： 

    sudo chown yourname /usr/<span class="built_in">local</span>  

    brew link yourlibpackage
</code></pre><p>6.Build: 继续编译：<code>make test</code>， <code>make runtest</code></p>
<pre><code>添加路径：

    <span class="built_in">export</span> DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib

    <span class="comment">#python</span>
    <span class="keyword">for</span> req <span class="keyword">in</span> $(cat python/requirements.txt); <span class="keyword">do</span> pip install <span class="variable">$req</span>; <span class="keyword">done</span>
    make pycaffe
    <span class="built_in">export</span> PYTHONPATH=~/technologies/caffe/python/:<span class="variable">$PYTHONPATH</span>
    <span class="built_in">cd</span> ..
</code></pre><p>make runtest的结果：</p>
<pre><code>[----------] <span class="number">10</span> tests from PowerLayerTest/<span class="number">0</span>, where TypeParam = N5caffe9CPUDeviceIfEE
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPower</span>
[       OK ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPower</span> (<span class="number">2</span> ms)
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerZeroGradient</span>
[       OK ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerZeroGradient</span> (<span class="number">1</span> ms)
[ RUN      ] PowerLayerTest/<span class="number">0</span><span class="class">.TestPowerTwoGradient</span>
...
[----------] <span class="number">10</span> tests from PowerLayerTest/<span class="number">0</span> (<span class="number">16</span> ms total)
</code></pre><p>Bugs:</p>
<pre><code>    <span class="keyword">library</span> <span class="keyword">not</span> found <span class="keyword">for</span> -lboost_python

解决： 

    brew install boost-python
</code></pre><hr>
<h3 id="测试">测试</h3><p>MNIST</p>
<p>下载mnist数据，如下代码，报错，需安装<code>brew install wget</code></p>
<pre><code>./<span class="typedef"><span class="keyword">data</span>/mnist/get_mnist.sh </span>
</code></pre><p>转数据：</p>
<pre><code>./examples/mnist/create_mnist.sh
<span class="variable">Creating</span> lmdb...
<span class="variable">Done</span>.

./examples/mnist/train_lenet.sh
</code></pre><p>问题：</p>
<pre><code>Cannot use GPU in CPU-only Caffe:<span class="instruction"> check </span>mode.
</code></pre><p>解决： 修改lenet_solver.prototxt 最后一行的GPU改为CPU，继续执行<code>./examples/mnist/train_lenet.sh</code></p>
<pre><code>I<span class="number">0407 00:51</span>:<span class="number">59.023721</span> <span class="number">2035871744</span> caffe.cpp:178] Use CPU.
I<span class="number">0407 00:51</span>:<span class="number">59.024705</span> <span class="number">2035871744</span> solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_test.prototxt"
I<span class="number">0407 00:51</span>:<span class="number">59.024927</span> <span class="number">2035871744</span> solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I<span class="number">0407 00:51</span>:<span class="number">59.026877</span> <span class="number">2035871744</span> net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I<span class="number">0407 00:51</span>:<span class="number">59.026897</span> <span class="number">2035871744</span> net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy

...结果...

I<span class="number">0407 00:57</span>:<span class="number">52.664399</span> <span class="number">2035871744</span> solver.cpp:317] Iteration 10000, loss = <span class="number">0.0032766</span>
I<span class="number">0407 00:57</span>:<span class="number">52.664429</span> <span class="number">2035871744</span> solver.cpp:337] Iteration 10000, Testing net (#0)
I<span class="number">0407 00:57</span>:<span class="number">54.761032</span> <span class="number">2035871744</span> solver.cpp:404]     Test net output #0: accuracy = 0.9901
I<span class="number">0407 00:57</span>:<span class="number">54.761072</span> <span class="number">2035871744</span> solver.cpp:404]     Test net output #1: loss = <span class="number">0.0290336</span> (* 1 = <span class="number">0.0290336</span> loss)
I<span class="number">0407 00:57</span>:<span class="number">54.761081</span> <span class="number">2035871744</span> solver.cpp:322] Optimization Done.
I<span class="number">0407 00:57</span>:<span class="number">54.761087</span> <span class="number">2035871744</span> caffe.cpp:222] Optimization Done.
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python爬取优酷视频" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/04/Python爬取优酷视频/" class="article-date">
  	<time datetime="2016-04-04T14:12:31.000Z" itemprop="datePublished">2016-04-04</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/04/Python爬取优酷视频/">Python爬取优酷视频</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="get_youku_videos_using_“flvcd-com”">get youku videos using “flvcd.com”</h3><p>借用flvcd.com实现对优酷视频的爬取。</p>
<p><strong>参考</strong>：<br>    <a href="http://www.flvcd.com/" target="_blank" rel="external">www.flvcd.com</a></p>
<p><strong>格式</strong>：</p>
<pre><code><span class="keyword">http</span>://www.flvcd.com/parse.php?kw=...&amp;<span class="built_in">format</span>=...

<span class="built_in">format</span>=<span class="string">'normal'</span>; <span class="string">'high'</span>; <span class="string">'super'</span>
</code></pre><hr>
<pre><code><span class="comment"># -*- coding:utf-8 -*-</span>
<span class="keyword">from</span> lxml <span class="keyword">import</span> etree
<span class="keyword">import</span> urllib2
<span class="keyword">import</span> urllib
<span class="keyword">import</span> os
<span class="keyword">import</span> requests

<span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span>
    html = requests.get(url).content
    selector = etree.HTML(html)
    <span class="keyword">return</span> selector


<span class="function"><span class="keyword">def</span> <span class="title">getContent</span><span class="params">(htm, xpathStr)</span>:</span>
    selector = htm
    content = selector.xpath(xpathStr)
    <span class="keyword">return</span> content

<span class="function"><span class="keyword">def</span> <span class="title">getFlv</span><span class="params">(cons, title, folder)</span>:</span>
    fn = <span class="string">'%s'</span> % title
    pa = os.path.dirname(__file__) + <span class="string">'/'</span> + <span class="string">'youku/'</span> +  folder
    <span class="comment"># check and create folder</span>
    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(pa):
        os.mkdir(pa)
    fl = pa + <span class="string">'/%s.flv'</span> % fn
    r = requests.get(cons)
    <span class="keyword">with</span> open(fl, <span class="string">"wb"</span>) <span class="keyword">as</span> code:
        code.write(r.content)

<span class="comment"># = = = = = = #</span>
videourl = <span class="string">'http://v.youku.com/v_show/id_XMTUyMjE2MTcwOA==.html'</span>
format = <span class="string">'normal'</span>  <span class="comment"># 'high'  'normal'  'super'</span>

url = <span class="string">'http://www.flvcd.com/parse.php?kw='</span> + urllib.quote(videourl) + <span class="string">'&amp;format='</span> + format
<span class="keyword">print</span> url
req = urllib2.Request(url)
req.add_header(<span class="string">'Referer'</span>, <span class="string">'http://www.flvcd.com/'</span>)
req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0'</span>)
res = urllib2.urlopen(req)

html = res.read()
<span class="comment"># print html</span>
selector = etree.HTML(html)

<span class="comment"># get flv title</span>
xp_title=<span class="string">'//*[@id="subtitle"]'</span>
htm0=getHtml(videourl)
cons=getContent(htm0,xp_title)
title=cons[<span class="number">0</span>].text
<span class="keyword">print</span> title

<span class="comment"># get flv href</span>
xp = <span class="string">'//*[@class="mn STYLE4"]//@href'</span>
content = selector.xpath(xp)
<span class="keyword">print</span> <span class="string">'%s'</span> % len(content)

x=<span class="number">0</span>
<span class="keyword">for</span> con <span class="keyword">in</span> content:
    <span class="keyword">if</span> <span class="string">'http://k.youku.com'</span> <span class="keyword">in</span> con:
        <span class="keyword">print</span> con
        getFlv(con,  <span class="string">'%s'</span> % x, title)
        <span class="comment"># urllib.urlretrieve(con, getPath('%s' % x, title))# , callbackfunc)</span>
        x+=<span class="number">1</span>
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/">Python</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Debug/">Debug</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python爬取arxiv的paper" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/04/01/Python爬取arxiv的paper/" class="article-date">
  	<time datetime="2016-04-01T07:21:47.000Z" itemprop="datePublished">2016-04-01</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/01/Python爬取arxiv的paper/">Python爬取arxiv的paper</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>每天都要去arxiv上关注最新一天的论文（computer vision）更新，而且每次下载的论文的名称都是arxiv的代号，需要花时间去整理，于是用python写一个非常简单的爬虫，自己使用，足够节省时间了。</p>
<pre><code><span class="keyword">import</span> requests
<span class="keyword">from</span> lxml <span class="keyword">import</span> etree
<span class="keyword">import</span> os
<span class="keyword">import</span> time
<span class="keyword">import</span> re
<span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool

<span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span>
    html = requests.get(url).content
    selector = etree.HTML(html)
    <span class="keyword">return</span> selector

<span class="function"><span class="keyword">def</span> <span class="title">getContent</span><span class="params">(htm, xpathStr)</span>:</span>
    selector = htm
    content = selector.xpath(xpathStr)  
    <span class="keyword">return</span> content

<span class="function"><span class="keyword">def</span> <span class="title">getDownPdf</span><span class="params">(cons, title, folder)</span>:</span>
    fn = <span class="string">'%s'</span> % title
    pa = os.path.dirname(__file__) + <span class="string">'/'</span> + <span class="string">'arxiv'</span> + <span class="string">'/%s'</span> % folder
    <span class="comment"># check and create folder</span>
    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(pa):
        os.mkdir(pa)
    fl = pa + <span class="string">'/%s.pdf'</span> % fn
    r = requests.get(cons)
    <span class="keyword">with</span> open(fl, <span class="string">"wb"</span>) <span class="keyword">as</span> code:
        code.write(r.content)

<span class="comment">#### main ###</span>
url0 = <span class="string">'http://arxiv.org/list/cs.CV/recent'</span>
<span class="keyword">print</span> url0
<span class="comment"># xpath of each page</span>
xp1 = <span class="string">'//dl[1]//*[@class="list-identifier"]//a[2]//@href'</span>  <span class="comment"># pdf href list</span>
xp2 = <span class="string">'//dl[1]//*[@class="list-title"]/text()'</span>  <span class="comment"># Title</span>
xp_date = <span class="string">'//*[@id="dlpage"]/h3[1]/text()'</span>  <span class="comment"># date-&gt;folder</span>

htm0 = getHtml(url0)
cons1 = getContent(htm0, xp1)  <span class="comment"># get pdfs' href</span>
cons2 = getContent(htm0, xp2)  <span class="comment"># get papers' title</span>
cons_date = getContent(htm0, xp_date) <span class="comment"># get date</span>

folder = cons_date[<span class="number">0</span>].split(<span class="string">', '</span>) <span class="comment"># get date string</span>

<span class="keyword">print</span> folder[<span class="number">1</span>] + <span class="string">': having %s'</span> % len(cons1) + <span class="string">'  files'</span>
<span class="keyword">print</span> <span class="string">'pdfs are downloading...'</span>

<span class="keyword">for</span> indx <span class="keyword">in</span> range(<span class="number">0</span>, len(cons1)):
    href = <span class="string">'http://arxiv.org'</span> + cons1[indx]
    title = cons2[<span class="number">2</span> * indx + <span class="number">1</span>]
    <span class="keyword">print</span> <span class="string">'%s.'</span> % (<span class="number">1</span> + indx) + <span class="string">' '</span> + href + <span class="string">' '</span> + title
    getDownPdf(href, title, folder[<span class="number">1</span>])
</code></pre>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/">Python</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Debug/">Debug</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习开发包" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/03/31/深度学习开发包/" class="article-date">
  	<time datetime="2016-03-31T13:27:56.000Z" itemprop="datePublished">2016-03-31</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/31/深度学习开发包/">Deep Learning Libraries by Language</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Python"><strong>Python</strong></h4><ol>
<li><p><a href="http://deeplearning.net/software/theano" target="_blank" rel="external">Theano</a>  is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built.<br> a. <a href="http://keras.io/" target="_blank" rel="external">Keras</a> is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU.<br> b. <a href="http://deeplearning.net/software/pylearn2/" target="_blank" rel="external">Pylearn2</a> is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano.<br> c. <a href="https://github.com/Lasagne/Lasagne" target="_blank" rel="external">Lasagne</a> is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism , focus and restraint principles.<br> d. <a href="https://github.com/mila-udem/blocks" target="_blank" rel="external">Blocks</a> a framework that helps you build neural network models on top of Theano.</p>
</li>
<li><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">Caffe</a> is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google’s DeepDream is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.</p>
</li>
<li><p><a href="https://github.com/dnouri/nolearn" target="_blank" rel="external">nolearn</a> contains a number of wrappers and abstractions around existing neural network libraries, most notably Lasagne, along with a few machine learning utility modules.</p>
</li>
<li><p><a href="http://radimrehurek.com/gensim/" target="_blank" rel="external">Gensim</a> is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms.</p>
</li>
<li><p><a href="http://chainer.org/" target="_blank" rel="external">Chainer</a> bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the flexible framework for Deep Learning.</p>
</li>
<li><p><a href="https://github.com/nitishsrivastava/deepnet" target="_blank" rel="external">deepnet</a> is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets.</p>
</li>
<li><p><a href="https://github.com/hannes-brt/hebel" target="_blank" rel="external">Hebel</a> is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping.</p>
</li>
<li><p><a href="https://github.com/dmlc/cxxnet" target="_blank" rel="external">CXXNET</a> is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction.</p>
</li>
<li><p><a href="https://github.com/andersbll/deeppy" target="_blank" rel="external">DeepPy</a> is a Pythonic deep learning framework built on top of NumPy.</p>
</li>
<li><p><a href="https://github.com/vishwa-raman/DeepLearning" target="_blank" rel="external">DeepLearning</a> is deep learning library, developed with C++ and python.</p>
</li>
<li><p><a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> is Nervana’s Python based Deep Learning framework.</p>
</li>
</ol>
<h4 id="Matlab"><strong>Matlab</strong></h4><ol>
<li><p><a href="https://github.com/sdemyanov/ConvNet" target="_blank" rel="external">ConvNet</a> Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs.</p>
</li>
<li><p><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" target="_blank" rel="external">DeepLearnToolBox</a> is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets.</p>
</li>
<li><p><a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">cuda-convnet</a> is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm.</p>
</li>
<li><p><a href="http://www.vlfeat.org/matconvnet/" target="_blank" rel="external">MatConvNet</a>  is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs</p>
</li>
</ol>
<hr>
<p><strong>More</strong>… <a href="http://www.teglor.com/b/deep-learning-libraries-language-cm569/" target="_blank" rel="external">http://www.teglor.com/b/deep-learning-libraries-language-cm569/</a></p>
<hr>
<p><strong>Author</strong>: <a href="http://www.teglor.com/b/deep-learning-libraries-language-cm569/" target="_blank" rel="external">teglor</a> </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Research/">Research</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Gang Wang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>