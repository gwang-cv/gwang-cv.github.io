{"meta":{"title":"gwang's lab","subtitle":"CV researcher","description":null,"author":"gwang's lab","url":"http://gwang-cv.github.io"},"pages":[{"title":"","date":"2018-05-13T15:12:34.662Z","updated":"2016-01-28T16:17:42.000Z","comments":true,"path":"categories/index.html","permalink":"http://gwang-cv.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2018-05-13T15:12:34.669Z","updated":"2016-01-28T16:17:42.000Z","comments":true,"path":"tags/index.html","permalink":"http://gwang-cv.github.io/tags/index.html","excerpt":"","text":""},{"title":"About Me","date":"2015-11-15T07:52:00.000Z","updated":"2016-01-23T13:07:53.000Z","comments":true,"path":"me/index.html","permalink":"http://gwang-cv.github.io/me/index.html","excerpt":"","text":"Gang Wang Ph.D. Candidate, Department of Computer Science and Technology, College of Electronic and Information Engineering (CEIE),Tongji University,Shanghai, P.R. China E-mail: gwang.cv [at] gmail.com [ ResearchGate | 中文] Biography Ph.D. in computer science, Tongji University, expected July 2016 M.Sc. in computer science, Shandong Normal University, June 2013 B.Sc. in computer science, Shandong Normal University, June 2009 My research interests mainly consist of computer vision and pattern recognition: Feature Extraction &amp; Matching, Point Set Registration, Image Stitching, Biomedical Image Registration. Publications 2016 High-speed Feature Extraction with Sparsity Preserving Laplacian Discriminant Analysis Yingchun Ren, Zhicheng Wang, Yufei Chen, Weidong Zhao, Lei Peng, and Gang Wang. Journal of Tongji University, 2016, In Press. Non-rigid Point Set Registration Based on Neighbor Structure and Gaussian Mixture Models Lei Peng, Guangyao Li, Mang Xiao, Gang Wang, and Li Xie. Chinese Journal of Electronics &amp; Information Technology, 2016, 38(1), 47-52. Sparsity Preserving Discriminant Projections with Applications to Face Recognition Yingchun Ren, Zhicheng Wang, Yufei Chen, Gang Wang, and Weidong Zhao. Mathematical Problems in Engineering, 2016, In Press. 2015 Fuzzy Correspondences and Kernel Density Estimation for Contaminated Point Set Registration. Gang Wang, Zhicheng Wang, Yufei Chen, Weidong Zhao, and Xianhui Liu. IEEE International Conference on Systems, Man, and Cybernetics, Oct 9-12, 2015, Hong Kong. Oral. A Robust Non-rigid Point Set Registration Method Based on Asymmetric Gaussian Representation. Gang Wang, Zhicheng Wang, Yufei Chen, and Weidong Zhao. Computer Vision and Image Understanding (CVIU), 2015, Vol. 141, pp. 67-80. Robust Point Matching Method for Multimodal Retinal Image Registration. Gang Wang, Zhicheng Wang, Yufei Chen, and Weidong Zhao. Biomedical Signal Processing and Control (BSPC), 2015, Vol. 19, pp. 68–76. Image Saliency Analysis Based on Grey Relational Computation and Prior Combination. Qiangqiang Zhou, Zhicheng Wang, Weidong Zhao, Yufei Chen, and Gang Wang. Chinese Journal of System Simulation, 2015, 27(7), 1511-1519. 2014 Robust Point Matching using Mixture of Asymmetric Gaussians for Nonrigid Transformation. Gang Wang, Zhicheng Wang, Weidong Zhao, and Qiangqiang Zhou. The 12th Asian Conference on Computer Vision (ACCV 2014), Nov 1-5, 2014, Singapore."}],"posts":[{"title":"项目页的盾牌小图标","slug":"项目页的盾牌小图标","date":"2018-10-04T07:11:07.661Z","updated":"2018-10-04T07:11:54.682Z","comments":true,"path":"2018/10/04/项目页的盾牌小图标/","link":"","permalink":"http://gwang-cv.github.io/2018/10/04/项目页的盾牌小图标/","excerpt":"","text":"在很多项目页中看到的小图标，可以通过工具在线制作，更方便显示当前项目所采用的软件及版本以及其他相关信息： shields.io Pixel-perfect Retina-ready Fast Consistent Hackable No tracking可在线通过自定义来制作盾牌小图标，示例如图：","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Image Deformation","slug":"Image Deformation","date":"2018-07-01T04:32:26.017Z","updated":"2018-07-01T10:58:27.676Z","comments":true,"path":"2018/07/01/Image Deformation/","link":"","permalink":"http://gwang-cv.github.io/2018/07/01/Image Deformation/","excerpt":"","text":"1.目的 输入图像，输出其grid网格以及变换后的图像及网格 2.配置 环境： Win10 x64 VS2015 (vc14) OpenCV2.4.13 #设置PATH环境变量C:/opencv/bin/x86/vc14/bin 3.VS2015配置OpenCV 新建一个win32控制台空白项目； 配置：项目-&gt;属性-&gt;VC++目录-&gt;包含目录 C:/opencv/build/include/opencv2 C:/opencv/build/include/opencv C:/opencv/build/include 配置：项目-&gt;属性-&gt;VC++目录-&gt;库目录 C:/opencv/build/x86/vc14/lib 配置：项目-&gt;属性-&gt;链接器-&gt;输入-&gt;附加依赖项 #for debug opencv_ml2413d.lib opencv_calib3d2413d.lib opencv_contrib2413d.lib opencv_core2413d.lib opencv_features2d2413d.lib opencv_flann2413d.lib opencv_gpu2413d.lib opencv_highgui2413d.lib opencv_imgproc2413d.lib opencv_legacy2413d.lib opencv_objdetect2413d.lib opencv_ts2413d.lib opencv_video2413d.lib opencv_nonfree2413d.lib opencv_ocl2413d.lib opencv_photo2413d.lib opencv_stitching2413d.lib opencv_superres2413d.lib opencv_videostab2413d.lib 4.编译运行 $ $Figure 1. test$ $$ $Figure 2. def","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Retina Image Segmentation with UNet","slug":"Retina Image Segmentation with UNet","date":"2018-06-29T13:05:51.373Z","updated":"2018-07-01T11:09:46.788Z","comments":true,"path":"2018/06/29/Retina Image Segmentation with UNet/","link":"","permalink":"http://gwang-cv.github.io/2018/06/29/Retina Image Segmentation with UNet/","excerpt":"","text":"1.说明 基于UNet的眼底图像血管分割方法：Retina-UNet，项目地址：https://github.com/orobix/retina-unet 2.配置运行环境及依赖： Ubuntu-16 + CUDA-9.0 + cudnn-7.0 + GPUDriver-390.67 + OpenCV-3.2 Python 2.7 TensorFlow 1.8.0 TensorFlow-GPU 1.8.0 #不安装，则直接用CPU训练 Keras 2.2.0 #原release使用的1.1.0版本keras，若使用keras2.x，则需要修改部分代码 PIL (pillow 3.1.2) h5py 2.8.0 ConfigParser 3.5.0 Numpy 1.14.5 scikit-learn 0.19.1 pydot 1.2.4 $ pip install xxx --user #安装上面这些依赖项 GraphViz $ sudo apt-get install graphviz libgraphviz-dev 3.数据集 训练用的是视网膜血管分割经典的数据集DRIVE：http://www.isi.uu.nl/Research/Databases/DRIVE/，其中包含手工分割的ground-truth和mask图像。 下载解压后放于项目根目录下： DRIVE │ └───test | ├───1st_manual | └───2nd_manual | └───images | └───mask │ └───training ├───1st_manual └───images └───mask 4.训练 首先整理数据： python prepare_datasets_DRIVE.py 执行训练： 配置文件可修改configuration.txt pytohn run_train.py 12345678910111213141516171819202122232425262728293031323334353637383940411. Create directory for the results (if not already existing)Dir already existingcopy the configuration file in the results folder2. Run the training on GPU (no nohup)Using TensorFlow backend.train images/masks shape:(20, 1, 565, 565)train images range (min-max): 0.0 - 1.0train masks are within 0-1patches per full image: 9500train PATCHES images/masks shape:(190000, 1, 48, 48)train PATCHES images range (min-max): 0.0 - 1.02018-06-30 00:43:12.163406: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-06-30 00:43:12.584228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-06-30 00:43:12.585652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076pciBusID: 0000:01:00.0totalMemory: 11.92GiB freeMemory: 11.57GiB2018-06-30 00:43:12.585718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 02018-06-30 00:43:20.161626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:2018-06-30 00:43:20.161703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929] 0 2018-06-30 00:43:20.161729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0: N 2018-06-30 00:43:20.174512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11206 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)Check: final output of the network:(None, 2304, 2)Train on 171000 samples, validate on 19000 samplesEpoch 1/150 - 204s - loss: 0.2764 - acc: 0.9045 - val_loss: 0.2325 - val_acc: 0.9240Epoch 00001: val_loss improved from inf to 0.23253, saving model to ./train/train_best_weights.h5Epoch 2/150 - 199s - loss: 0.1679 - acc: 0.9380 - val_loss: 0.1698 - val_acc: 0.9420Epoch 00002: val_loss improved from 0.23253 to 0.16979, saving model to ./train/train_best_weights.h5Epoch 3/150 5.测试 6.Debug： 1) Keras2版本不兼容的问题： from keras.utils.visualize_util import plot ImportError: No module named 'keras.utils.visualize_util' From visualize_util to vis_utils;The plot function was also renamed to plot_model;The nb_epoch argument in fit has been renamed epochs;Update your Model call to the Keras 2 API: Model(outputs=Tensor(&quot;ac..., inputs=Tensor(&quot;in...) model = Model(input=inputs, output=conv7);更新代码或者选用合适的Keras版本。 2) ImportError: No module named tensorflow使用pip install tensorflow安装失败且出现importing error，解决方法是在命令后加上--user。 3) ImportError: Failed to import pydot. Please install pydot. For example with pip install pydot. 4) pydot failed to call GraphViz. Please install GraphViz https://www.graphviz.org/ and ensure that its executables are in the $PATH. 在~/.bashrc中添加了PATH后错误仍然出现，不知道问题在哪。。。暂时注释掉retinaNN_training.py中的#plot_model(model, to_file=&#39;./&#39;+name_experiment+&#39;/&#39;+name_experiment + &#39;_model.png&#39;) #check how the model looks like。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Ubuntu下查看OpenCV版本号","slug":"Ubuntu下查看OpenCV版本号","date":"2018-06-28T12:33:09.605Z","updated":"2018-06-28T12:33:09.605Z","comments":true,"path":"2018/06/28/Ubuntu下查看OpenCV版本号/","link":"","permalink":"http://gwang-cv.github.io/2018/06/28/Ubuntu下查看OpenCV版本号/","excerpt":"","text":"查看Ubuntu下OpenCV版本号： pkg-config --modversion opencv 输出： 3.2.0","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Ubuntu下查看CUDA版本、cudnn版本号","slug":"Ubuntu下查看CUDA版本、cudnn版本号","date":"2018-06-28T11:53:07.741Z","updated":"2018-06-28T11:57:16.533Z","comments":true,"path":"2018/06/28/Ubuntu下查看CUDA版本、cudnn版本号/","link":"","permalink":"http://gwang-cv.github.io/2018/06/28/Ubuntu下查看CUDA版本、cudnn版本号/","excerpt":"","text":"1.查看CUDA版本 cat /usr/local/cuda/version.txt 例如输出： CUDA Version 8.0.61 or: nvcc --version 输出： Copyright (c) 2005-2017 NVIDIA Corporation Built on Fri_Nov__3_21:07:56_CDT_2017 Cuda compilation tools, release 9.1, V9.1.85 2.查看cudnn版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 例如输出： #define CUDNN_MAJOR 6 #define CUDNN_MINOR 0 #define CUDNN_PATCHLEVEL 21 -- #define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) #include \"driver_types.h\"","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Theano+OpenCV+LIFT","slug":"Theano+OpenCV+LIFT","date":"2018-06-28T09:56:15.223Z","updated":"2018-06-28T11:22:12.455Z","comments":true,"path":"2018/06/28/Theano+OpenCV+LIFT/","link":"","permalink":"http://gwang-cv.github.io/2018/06/28/Theano+OpenCV+LIFT/","excerpt":"","text":"0.说明 LIFT: Learned Invariant Feature Points作者开源代码：https://github.com/cvlab-epfl/LIFT PS: 注意下载作者release的1.0版，修复了一些bug:.locks directory is now included in the repository by default。 1.安装最新OpenCV 3.x 略 2.安装Theano并配置 pip install theano==0.9.0 配置：(开源代码中三个主要部分需要GPU的支持) sudo vim ~/.theanorc [global] device = gpu0 floatX = float32 [nvcc] fastmath = True [cuda] root=/usr/local/cuda-8.0 3.安装其他依赖项 安装flufl.lock:1) 下载安装包https://pypi.python.org/pypi/flufl.lock/2.4.1 tar -zxf flufl.lock-2.4.1.tar.gz sudo python setup.py install 2) 或 pip install flufl.lock==2.4.1 安装Lasagne:下载安装包：https://github.com/Lasagne/Lasagne【(0.2.dev1)】 tar -zxf Lasagne.tar.gz sudo python setup.py install 其他项： pip install numpy parse h5py scipy 4.测试demo # Build the shared library by cd c-code/build cmake -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 .. make #To run the test program simply ./run.sh 5.其他问题 cmake报错： Could NOT find CUDA: Found unsuitable version \"9.1\", but required is exact version “8.0” (found /usr/local/cuda-9.1) #需指定cuda版本 cmake -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 .. 已解决的一些issues： https://github.com/cvlab-epfl/LIFT/issues?q=is%3Aissue+is%3Aclosed","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"MacOs High Sierra using aria2","slug":"MacOs High Sierra using aria2","date":"2018-05-20T07:27:30.453Z","updated":"2018-05-20T07:48:43.124Z","comments":true,"path":"2018/05/20/MacOs High Sierra using aria2/","link":"","permalink":"http://gwang-cv.github.io/2018/05/20/MacOs High Sierra using aria2/","excerpt":"","text":"说明 配置 aria2，结合 Chrome，代替baidu网盘App下载文件。 安装aria2 12345678910brew install aria2==&gt; Downloading https://homebrew.bintray.com/bottles/aria2-1.34.0.high_sierra.bottle.tar.gz######################################################################## 100.0%==&gt; Pouring aria2-1.34.0.high_sierra.bottle.tar.gz==&gt; CaveatsBash completion has been installed to: /usr/local/etc/bash_completion.d==&gt; Summary🍺 /usr/local/Cellar/aria2/1.34.0: 22 files, 4.0MB 配置aria2 RPC模式配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152cd ~mkdir .aria2cd .aria2touch aria2.confvim aria2.conf#用户名#rpc-user=user#密码#rpc-passwd=passwd#上面的认证方式不建议使用,建议使用下面的token方式#设置加密的密钥#rpc-secret=token#允许rpcenable-rpc=true#允许所有来源, web界面跨域权限需要rpc-allow-origin-all=true#允许外部访问，false的话只监听本地端口rpc-listen-all=true#RPC端口, 仅当默认端口被占用时修改#rpc-listen-port=6800#最大同时下载数(任务数), 路由建议值: 3max-concurrent-downloads=5#断点续传continue=true#同服务器连接数max-connection-per-server=5#最小文件分片大小, 下载线程数上限取决于能分出多少片, 对于小文件重要min-split-size=10M#单文件最大线程数, 路由建议值: 5split=10#下载速度限制max-overall-download-limit=0#单文件速度限制max-download-limit=0#上传速度限制max-overall-upload-limit=0#单文件速度限制max-upload-limit=0#断开速度过慢的连接#lowest-speed-limit=0#验证用，需要1.16.1之后的release版本#referer=*#文件保存路径, 默认为当前启动位置dir=/Users/xxx/Downloads#文件缓存, 使用内置的文件缓存, 如果你不相信Linux内核文件缓存和磁盘内置缓存时使用, 需要1.16及以上版本#disk-cache=0#另一种Linux文件缓存方式, 使用前确保您使用的内核支持此选项, 需要1.15及以上版本(?)#enable-mmap=true#文件预分配, 能有效降低文件碎片, 提高磁盘性能. 缺点是预分配时间较长#所需时间 none &lt; falloc ? trunc « prealloc, falloc和trunc需要文件系统和内核支持file-allocation=prealloc 默认下载路径的「/Users/xxx/Downloads」可以改。xxx为Mac用户名，输入:w保存。 启动RPC模式 在终端输入aria2c --conf-path=&quot;/Users/xx/.aria2/aria2.conf&quot; -D，aria2 启动。 配置chrome 下载 Chrome 插件BaiduExporter.crx 将其拖拽至Chrome扩展程序处安装，在百度网盘页面会新增一个功能按钮来使用aria下载。 配置GUI 下载aria2gui，解压后将其拷贝到应用程序中，打开会看到当前下载的文件列表。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"MacOs High Sierra安装labelImg","slug":"MacOSHighSierra安装labelImg","date":"2018-05-20T03:33:39.537Z","updated":"2018-05-20T07:48:52.957Z","comments":true,"path":"2018/05/20/MacOSHighSierra安装labelImg/","link":"","permalink":"http://gwang-cv.github.io/2018/05/20/MacOSHighSierra安装labelImg/","excerpt":"","text":"labelImg官方地址:https://github.com/tzutalin/labelImg/blob/master/README.rst macOSPython 2 + Qt4 12345brew install qt qt4brew install libxml2make qt4py2python labelImg.pypython labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE] Python 3 + Qt5 (Works on macOS High Sierra) 12345brew install qt # will install qt-5.x.xbrew install libxml2make qt5py3python labelImg.pypython labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE] 实际在使用python2的时候，无法用brew install qt qt4，且在python3下安装libxml2后报错： 1ImportError: No module named lxml 所以在macos high sierra下安装python3，然后 1234brew install qtpip3 install lxmlmake qt5py3python3 labelImg.py","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"py-Faster-rcnn for CPU_only MacOS","slug":"py-Faster-rcnn for MacOS","date":"2018-01-03T09:13:36.000Z","updated":"2018-01-03T10:03:16.000Z","comments":true,"path":"2018/01/03/py-Faster-rcnn for MacOS/","link":"","permalink":"http://gwang-cv.github.io/2018/01/03/py-Faster-rcnn for MacOS/","excerpt":"","text":"1. 准备 需要安装OpenCV3.x版本，以及OpenBLAS，参考：Mac安装OpenCV, MacOS安装caffe 确认安装brew Cython easydict protobuf 下载py-faster-rcnn源码： git clone –recursive https://github.com/rbgirshick/py-faster-rcnn.git 2. 编译caffe 修改makefile.config # USE_CUDNN := 1 CPU_ONLY := 1 USE_OPENCV := 1 OPENCV_VERSION := 3 CUDA_DIR := /usr/local/cuda BLAS := open BLAS_INCLUDE := /usr/local/Cellar/openblas/0.2.20/include BLAS_LIB := /usr/local/Cellar/openblas/0.2.20/lib PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/local/lib/python2.7/site-packages/numpy/core/include PYTHON_LIB := /usr/lib WITH_PYTHON_LAYER := 1 INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/local/Cellar/hdf5 /usr/local/Cellar/opencv/3.2.0/include LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/local/Cellar/opencv/3.2.0/lib BUILD_DIR := build DISTRIBUTE_DIR := distribute TEST_GPUID := 0 执行编译： make -j32 &amp;&amp; make pycaffe 3. 编译faster-rcnn 修改文件，替换或删除关于gpu的代码行： 修改./lib/setup.py ... #CUDA = locate_cuda() ... ... #self.set_executable('compiler_so', CUDA['nvcc']) ... ... #Extension('nms.gpu_nms', #[‘nms/nms_kernel.cu', 'nms/gpu_nms.pyx'], #library_dirs=[CUDA['lib64']], #libraries=['cudart'], #language='c++', #runtime_library_dirs=[CUDA['lib64']], ## this syntax is specific to this build system ## we're only going to use certain compiler args with nvcc and not with ## gcc the implementation of this trick is in customize_compiler() below #extra_compile_args={'gcc': [\"-Wno-unused-function\"], #’nvcc': ['-arch=sm_35', #’—ptxas-options=-v', #’-c’, #’—compiler-options', #”’-fPIC'\"]}, #include_dirs = [numpy_include, CUDA['include']] #) 执行编译： cd ./lib make 4. 运行demo 运行demo前需要修改几个文件，替换或删除关于gpu的代码行： (1) 修改~/py-faster-rcnn/lib/fast_rcnn/config.py # Use GPU implementation of non-maximum suppression # __C.USE_GPU_NMS = True __C.USE_GPU_NMS = False (2) 修改~/py-faster-rcnn/tools/test_net.py和 ~/py-faster-rcnn/tools/train_net.py #caffe.set_mode_gpu() caffe.set_mode_cpu() (3) 将~/py-faster-rcnn/lib/setup.py中，含有nms.gpu_nms的部分去掉 (4) 修改./lib/fast-rcnn/nms_wrapper.py，修改如下： from fast_rcnn.config import cfg #from nms.gpu_nms import gpu_nms from nms.cpu_nms import cpu_nms def nms(dets, thresh, force_cpu=False): \"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\" if dets.shape[0] == 0: return [] if cfg.USE_GPU_NMS and not force_cpu: print \"nms_wrapper GPU ...\" #return gpu_nms(dets, thresh, device_id=cfg.GPU_ID) else: return cpu_nms(dets, thresh) 接下来运行demo即可。 5. 问题 (1) import caffe错误，或者报错fatal error: numpy/arrayobject.h没有那个文件或目录 修改Makefile.config找到PYTHON_INCLUDE： PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include 要加一个local，变成： PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/local/lib/python2.7/dist-packages/numpy/core/include (2) 报错AttributeError: &#39;DataFrame&#39; object has no attribute &#39;sort_values&#39; # sort_values is new in version 0.17.0, so check your version of pandas. In the previous versions you should use sort pip install -U pandas # 查看pandas版本 &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; pd.show_versions() &gt;&gt;&gt; pd.__version__ (3) 找不到目录，在_init_paths.py中定义的路径修改： # 将 this_dir = osp.dirname(__file__) 改为： this_dir = osp.dirname(osp.abspath(__file__)) 参考： Faster R-CNN CPU环境搭建 Mac下跑仅CPU模式下的py-faster-rcnn","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/categories/DeepLearning/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"训练faster-rcnn报错","slug":"faster-rcnn errors","date":"2017-12-04T02:32:17.000Z","updated":"2017-12-05T05:52:28.000Z","comments":true,"path":"2017/12/04/faster-rcnn errors/","link":"","permalink":"http://gwang-cv.github.io/2017/12/04/faster-rcnn errors/","excerpt":"","text":"训练结束后测试时出现类似 File \"/home/xxx/py-faster-rcnn/tools/../lib/datasets/voc_eval.py\", line 126, in voc_eval R = [obj for obj in recs[imagename] if obj['name'] == classname] KeyError: '000002' 解决方法: 删除data/VOCdekit2007下的annotations_cache文件夹 基于ResNet101训练py-faster-rcnn遇到错误： /faster-rcnn-py/tools/../lib/fast_rcnn/bbox_transform.py:50: RuntimeWarning: overflow encountered in exp pred_h = np.exp(dh) * heights[:, np.newaxis] faster-rcnn-py/tools/../lib/rpn/proposal_layer.py:176: RuntimeWarning: invalid value encountered in greater_equal keep = np.where((ws &gt;= min_size) &amp; (hs &gt;= min_size))[0] 类似解决方法：减小lr from 0.001 to 0.0001 A possible solution could be to decrease the base learning rate in the solver.prototxt As it is recommended here http://caffe.berkeleyvision.org/tutorial/solver.html Just change the base_lr: 0.001 to 0.0001 Note also that the above settings are merely guidelines, and they’re definitely not guaranteed to be optimal (or even work at all!) in every situation. If learning diverges (e.g., you start to see very large or NaN or inf loss values or outputs), try dropping the base_lr (e.g., base_lr: 0.001) and re-training, repeating this until you find a base_lr value that works. I did try to change the base_lr value and now the NAN value disappeared.","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Faster-rcnn detect small or tiny objects","slug":"Faster-rcnn detect small or tiny objects","date":"2017-12-01T11:26:03.000Z","updated":"2017-12-03T04:04:18.000Z","comments":true,"path":"2017/12/01/Faster-rcnn detect small or tiny objects/","link":"","permalink":"http://gwang-cv.github.io/2017/12/01/Faster-rcnn detect small or tiny objects/","excerpt":"","text":"https://github.com/rbgirshick/py-faster-rcnn/issues/161 R-CNN for Small Object Detection: https://www.merl.com/publications/docs/TR2016-144.pdf Perceptual Generative Adversarial Networks for Small Object Detection https://arxiv.org/pdf/1706.05274v1.pdf https://github.com/rbgirshick/py-faster-rcnn/issues/86 https://github.com/rbgirshick/py-faster-rcnn/issues/433 Hi, I had the same problem and those are my conclusion at this point : To me, the best answer was to cut the images in smaller patches, at least for the training phase. According to hardware requirement, you need : 3GB GPU memory for ZF net8GB GPU memory for VGG-16 netThat’s taking into account the 600x1000 original scaling, so to make it simple you need 8GB for 600 000 pixels assuming that you use VGG.I have 12GB on my GPU so if this is linear, i can go up to (600 000x12)/8 = 900 000 pixels maximum.I couldn’t resize my images because my objects are small and I couldn’t afford losing resolution.I chose to cut my 3000x4000 images in 750x1000 patches, which is the simplest division to go under 900 000 pixels. SCALES: [750] MAX_SIZE: 1000 However, the good thing is that you only need to cut the images for the training phase. Then you can apply the trained network on full images thanks the the separate test parameters : TEST: SCALES: [3000] MAX_SIZE: 4000 At least that’s what I did and now I have a network working on 3000x4000 images to detect 100x100 objects, in full c++ thanks to the c++ version. Hi guys,I already changed the code in lib/rpn/generate_anchors.py and nub_output like this:ratios and num_output like this However, I got the following error. ============== layer { name: 'rpn_cls_prob_reshape' type: 'Reshape' bottom: 'rpn_cls_prob' top: 'rpn_cls_prob_reshape' reshape_param { shape { dim: 0 dim: 18 dim: -1 dim: 0 } } } You should change this 18 to 24 JayMarx Hello! Do you solve the problem? I changed aspect ratios and followed catsdogone’s method, it’s works, but when I changed scales just like you, it didn’t work.Do you have any idea how to fix it?These are my changes:As you see, I just changed “dim: 18” to “dim: 140” and I don’t know whether it’s right or not!The error goes like this: -&gt;&gt;&gt;liuhyCV @JayMarx I have meet the same error with you. I have found the solutions as follows:at function “ def generate_anchors(base_size=16, ratios=[0.3, 0.5, 1, 1.5, 2], scales=2**np.arange(1, 6)): “,but at anchor_target_layer.py: def setup(self,bottom,top) layer_params = yaml.load(self.param_str_) anchor_scales =layer_params.get('scales', (8, 16, 32)) self._anchors = generate_anchors(scales=np.array(anchor_scales)) we should change this code by: layer_params = yaml.load(self.param_str_) anchor_scales =layer_params.get('scales', (8, 16, 32)) self._anchors = generate_anchors() at last the generate_anchors() can use the scales that we defintion @harjatinsingh So far I havent being able to successfully make it work for smaller images as I wanted. However, it seems changing the values of the ratios in generate_anchors.py does make the algorithm to recognize smaller objects, but the bounding box looses precision. For instance, what I have done is changing the code below from this: def generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)): to this: def generate_anchors(base_size=16, ratios=[0.3, 0.75, 1], scales=2**np.arange(3, 6)): Also, it seems that changing the values of anchors does work as noted in #161 but I couldnt make it work for me. So my question (in both issues) is still pending.","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/categories/DeepLearning/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"MacOSX+XXNet3.7.5+IPV6","slug":"MacOSX+XXNet3.7.5+IPV6","date":"2017-10-26T15:51:01.000Z","updated":"2017-10-26T15:51:01.000Z","comments":true,"path":"2017/10/26/MacOSX+XXNet3.7.5+IPV6/","link":"","permalink":"http://gwang-cv.github.io/2017/10/26/MacOSX+XXNet3.7.5+IPV6/","excerpt":"","text":"1.安装XX-Net https://github.com/XX-net/XX-Net/releases 2.MacOS设置IPV6 安装merido http://www.deepdarc.com/miredo-osx/ 安装TunTap(pkg安装包 不安装的话 Miredo会一直链接不成功,设置界面显示红灯) https://sourceforge.net/projects/tuntaposx/?source=typ_redirect 在系统设置界面，打开merido，其中选择client，然后填入teredo.remlab.net，然后apply即可看到上面的黄灯变为绿色，并且在下方出现IPV6地址。 3.XX-Net运行检查所有IP XX-Net启用IPV6 能得到上千个有效IP数，连接成功！","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Windows下安装labelImg","slug":"Windows下安装labelImg","date":"2017-09-20T17:53:21.000Z","updated":"2017-09-21T01:19:41.000Z","comments":true,"path":"2017/09/21/Windows下安装labelImg/","link":"","permalink":"http://gwang-cv.github.io/2017/09/21/Windows下安装labelImg/","excerpt":"","text":"1.安装必备的Visual Studio 2015，以提供nmake工具 2.在开始菜单中找到Visual Studio Tools &gt; VS2015 x64本机工具命令提示符 3.到pyqt4官方下载sip的源文件 4.编译安装sip cd C:\\sip-4.19.3 python configure.py nmake nmake install 5.安装pyqt4，在Windows下不要采用编译安装方法，会掉到坑里。我们使用非官方的安装包，下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/，下载对应的PyQt4‑4.11.4‑cp27‑cp27m‑win_amd64.whl【注意python的版本号，此处是基于python2.7（cp27）】 pip install PyQt4‑4.11.4‑cp27‑cp27m‑win_amd64.whl 安装完成之后，将C:\\Python27\\Lib\\site-packages\\PyQt4添加到系统路径中 6.安装labelImg，这里编译安装 pyrcc4 -o resources.py resources.qrc python labelImg.py 7.lxml的安装方法参考pyqt4，从中http://www.lfd.uci.edu/~gohlke/pythonlibs/下载对应python版本号的lxml非官方安装包，然后安装即可 pip install lxml-3.8.0-cp27-cp27m-win_amd64.whl 完成！！ https://github.com/tzutalin/labelImg http://www.cnblogs.com/qinjiting/p/5822572.html","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Caffe训练出错","slug":"Caffe训练出错","date":"2017-09-14T13:42:31.000Z","updated":"2017-09-14T13:46:34.000Z","comments":true,"path":"2017/09/14/Caffe训练出错/","link":"","permalink":"http://gwang-cv.github.io/2017/09/14/Caffe训练出错/","excerpt":"","text":"Caffe训练出错 F0204 17:40:38.201807 10866 db_lmdb.hpp:15] Check failed: mdb_status == 0 (2 vs. 0) No such file or directory You have not set your paths to the LMDB directories correctly.LMDB目录错误,修改prototxt文件的LMDB文件路径 写相对路径不行，写成绝对路径就通过了。。。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"为终端设置Shadowsocks代理","slug":"为终端设置Shadowsocks代理","date":"2017-09-12T09:55:10.000Z","updated":"2017-09-14T13:46:20.000Z","comments":true,"path":"2017/09/12/为终端设置Shadowsocks代理/","link":"","permalink":"http://gwang-cv.github.io/2017/09/12/为终端设置Shadowsocks代理/","excerpt":"","text":"PS：为解决python tweepy连接twitter api超时的问题。 tweepy.api(auth,proxy=\"localhost:1080\") 做开发都会经常接触终端，有些时候我们在终端会做一些网络操作，比如下载gradle包等，由于一些你懂我也懂的原因，某些网络操作不是那么理想，这时候我们就需要设置代理来自由地访问网络。Shadowsocks是我们常用的代理工具，它使用socks5协议，而终端很多工具目前只支持http和https等协议，对socks5协议支持不够好，所以我们为终端设置shadowsocks的思路就是将socks协议转换成http协议，然后为终端设置即可。仔细想想也算是适配器模式的一种现实应用吧。想要进行转换，需要借助工具，这里我们采用比较知名的polipo来实现。polipo是一个轻量级的缓存web代理程序。 准备工作 首先需要配置好一个可用的shadowsocks，客户端下载地址：https://github.com/shadowsocks/shadowsocks/releases 安装和配置ss，请自行搜索解决。 安装Fedora安装 sudo yum install polipo Mac下使用Homebrew安装 brew install polipo Ubuntu安装 sudo apt-get install polipo 修改配置(Linux) 如下打开配置文件 sudo vim /etc/polipo/config 设置ParentProxy为Shadowsocks，通常情况下本机shadowsocks的地址如下 # Uncomment this if you want to use a parent SOCKS proxy: socksParentProxy = \"localhost:1080\" socksProxyType = socks5 设置日志输出文件 logFile=/var/log/polipo logLevel=4 修改配置(Mac) 设置每次登陆启动polipo ln -sfv /usr/local/opt/polipo/*.plist ~/Library/LaunchAgents 修改文件/usr/local/opt/polipo/homebrew.mxcl.polipo.plist设置parentProxy &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt; &lt;plist version=\"1.0\"&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;homebrew.mxcl.polipo&lt;/string&gt; &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;KeepAlive&lt;/key&gt; &lt;true/&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;/usr/local/opt/polipo/bin/polipo&lt;/string&gt; &lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt; &lt;/array&gt; &lt;!-- Set `ulimit -n 20480`. The default OS X limit is 256, that's not enough for Polipo (displays 'too many files open' errors). It seems like you have no reason to lower this limit (and unlikely will want to raise it). --&gt; &lt;key&gt;SoftResourceLimits&lt;/key&gt; &lt;dict&gt; &lt;key&gt;NumberOfFiles&lt;/key&gt; &lt;integer&gt;20480&lt;/integer&gt; &lt;/dict&gt; &lt;/dict&gt; &lt;/plist&gt; 修改的地方是增加了&lt;string&gt;socksParentProxy=localhost:1080&lt;/string&gt; 启动（Linux） 先关闭正在运行的polipo，然后再次启动 sudo service polipo stop sudo service polipo start 启动(Mac) launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.polipo.plist launchctl load ~/Library/LaunchAgents/homebrew.mxcl.polipo.plis 注意：请确保Shadowsocks正常工作。 验证及使用 安装完成就需要进行验证是否work。这里展示一个最简单的验证方法，打开终端，如下执行 07:56:24-androidyue/var/log$ curl ip.gs 当前 IP：125.39.112.15 来自：中国天津天津 联通 08:09:23-androidyue/var/log$ http_proxy=http://localhost:8123 curl ip.gs 当前 IP：210.140.193.128 来自：日本日本 如上所示，为某个命令设置代理，前面加上http_proxy=http://localhost:8123 后接命令即可。注：8123是polipo的默认端口，如有需要，可以修改成其他有效端口。 设置别名 bash中有一个很好的东西，就是别名alias. Linux用户修改~/.bashrc，Mac用户修改~/.bash_profile文件，增加如下设置 alias hp=\"http_proxy=http://localhost:8123\" 然后Linux用户执行source ~/.bashrc，Mac用户执行source ~/.bash_profile 测试使用 20:39:39-androidyue~$ curl ip.gs 当前 IP：125.39.112.14 来自：中国天津天津 联通 20:39:44-androidyue~$ hp curl ip.gs 当前 IP：210.140.193.128 来自：日本日本 20:39:48-androidyue~$ 当前会话全局设置 如果嫌每次为每一个命令设置代理比较麻烦，可以为当前会话设置全局的代理。即使用export http_proxy=http://localhost:8123即可。 如果想撤销当前会话的http_proxy代理，使用 unset http_proxy 即可。 示例效果如下 21:29:49-androidyue~$ curl ip.gs 当前 IP：125.39.112.14 来自：中国天津天津 联通 21:29:52-androidyue~$ export http_proxy=http://localhost:8123 21:30:07-androidyue~$ curl ip.gs 当前 IP：210.140.193.128 来自：日本日本 21:30:12-androidyue~$ unset http_proxy 21:30:37-androidyue~$ curl ip.gs 当前 IP：125.39.112.14 来自：中国天津天津 联通 如果想要更长久的设置代理，可以将export http_proxy=http://localhost:8123加入.bashrc或者.bash_profile文件 设置Git代理复杂一些的设置Git代理 git clone https://android.googlesource.com/tools/repo --config http.proxy=localhost:8123 Cloning into 'repo'... remote: Counting objects: 135, done remote: Finding sources: 100% (135/135) remote: Total 3483 (delta 1956), reused 3483 (delta 1956) Receiving objects: 100% (3483/3483), 2.63 MiB | 492 KiB/s, done. Resolving deltas: 100% (1956/1956), done. 其实这样还是比较复杂，因为需要记忆的东西比较多，下面是一个更简单的实现首先，在.bashrc或者.bash_profile文件加入这一句。 gp=\" --config http.proxy=localhost:8123\" 然后执行source操作，更新当前bash配置。 更简单的使用git的方法 git clone https://android.googlesource.com/tools/repo $gp Cloning into 'repo'... remote: Counting objects: 135, done remote: Finding sources: 100% (135/135) remote: Total 3483 (delta 1956), reused 3483 (delta 1956) Receiving objects: 100% (3483/3483), 2.63 MiB | 483 KiB/s, done. Resolving deltas: 100% (1956/1956), done. End","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"解决opencv无法读取视频数据(Ubuntu)","slug":"解决opencv无法读取视频数据(Ubuntu)","date":"2017-09-10T09:25:51.000Z","updated":"2017-09-10T09:37:00.000Z","comments":true,"path":"2017/09/10/解决opencv无法读取视频数据(Ubuntu)/","link":"","permalink":"http://gwang-cv.github.io/2017/09/10/解决opencv无法读取视频数据(Ubuntu)/","excerpt":"","text":"错误描述： OpenCV Error: Unspecified error (The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script) in cvNamedWindow, file /io/opencv/modules/highgui/src/window.cpp, line 565 Traceback (most recent call last): File \"video_tools/demo.py\", line 103, in &lt;module&gt; cv2.namedWindow('ret',0) cv2.error: /io/opencv/modules/highgui/src/window.cpp:565: error: (-2) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function cvNamedWindow 分析： 在Ubuntu16.04下安装OpenCV3.2之前，确实已经提前安装了各种依赖包，特别是libgtk2.0-dev and pkg-config已经确认安装，通过在python下使用cv2.getBuildInformation()查看： GUI: QT: NO GTK+: NO GThread : YES (ver 2.12.3) GtkGlExt: NO OpenGL support: NO VTK support: NO Media I/O: ZLib: zlib (ver 1.2.8) JPEG: libjpeg (ver 90) WEBP: build (ver 0.3.1) PNG: build (ver 1.6.24) TIFF: build (ver 42 - 4.0.2) JPEG 2000: build (ver 1.900.1) OpenEXR: build (ver 1.7.1) GDAL: NO GDCM: NO Video I/O: DC1394 1.x: NO DC1394 2.x: NO FFMPEG: NO avcodec: NO avformat: NO avutil: NO swscale: NO avresample: NO GStreamer: NO OpenNI: NO OpenNI PrimeSensor Modules: NO OpenNI2: NO PvAPI: NO GigEVisionSDK: NO Aravis SDK: NO UniCap: NO UniCap ucil: NO V4L/V4L2: NO/NO XIMEA: NO Xine: NO gPhoto2: NO Parallel framework: pthreads Other third-party libraries: Use IPP: NO Use IPP Async: NO Use VA: NO Use Intel VA-API/OpenCL: NO Use Lapack: NO Use Eigen: NO Use Cuda: NO Use OpenCL: YES Use OpenVX: NO Use custom HAL: NO OpenCL: &lt;Dynamic loading of OpenCL library&gt; Include path: /io/opencv/3rdparty/include/opencl/1.2 Use AMDFFT: NO Use AMDBLAS: NO Python 2: Interpreter: python (ver 2.7.13) Libraries: numpy: /opt/python/cp27-cp27mu/lib/python2.7/site-packages/numpy/core/include (ver 1.11.1) packages path: /opt/python/cp27-cp27mu/lib/python2.7/site-packages Python 3: Interpreter: NO Python (for build): python Java: ant: NO JNI: NO Java wrappers: NO Java tests: NO Matlab: Matlab not found or implicitly disabled Tests and samples: Tests: NO Performance tests: NO C/C++ Examples: NO Install path: /usr/local cvconfig.h is in: /io/opencv/build ----------------------------------------------------------------- 安装官方给出的建议，重新编译，特别注意一些开关要设置下： sudo apt-get -y install libopencv-dev build-essential checkinstall cmake pkg-config yasm libjpeg-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev libdc1394-22-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libv4l-dev python-dev python-numpy libtbb-dev libqt4-dev libgtk2.0-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils libtiff5-dev libjpeg62-dev ffmpeg libatlas-base-dev gfortran cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D FORCE_VTK=ON \\ -D BUILD_NEW_PYTHON_SUPPORT=ON \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D WITH_TBB=ON \\ -D WITH_V4L=ON \\ -D WITH_QT=ON \\ -D WITH_OPENGL=ON \\ -D WITH_CUBLAS=ON \\ -D CUDA_NVCC_FLAGS=\"-D_FORCE_INLINES\" \\ -D WITH_GDAL=ON \\ -D WITH_XINE=ON \\ -D BUILD_EXAMPLES=ON \\ -D WITH_FFMPEG=ON .. make -j32 make install 但是我发现，在重新编译后安装后的cv2.so，引入到python后得到的信息如下： GUI: QT 4.x: YES (ver 4.8.7 EDITION = OpenSource) QT OpenGL support: YES (/usr/lib/x86_64-linux-gnu/libQtOpenGL.so) OpenGL support: YES (/usr/lib/x86_64-linux-gnu/libGLU.so /usr/lib/x86_64-linux-gnu/libGL.so) VTK support: NO Media I/O: ZLib: /usr/lib/x86_64-linux-gnu/libz.so (ver 1.2.8) JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (ver ) WEBP: build (ver 0.3.1) PNG: /usr/lib/x86_64-linux-gnu/libpng.so (ver 1.2.54) TIFF: /usr/lib/x86_64-linux-gnu/libtiff.so (ver 42 - 4.0.6) JPEG 2000: /usr/lib/x86_64-linux-gnu/libjasper.so (ver 1.900.1) OpenEXR: /usr/lib/x86_64-linux-gnu/libImath.so /usr/lib/x86_64-linux-gnu/libIlmImf.so /usr/lib/x86_64-linux-gnu/libIex.so /usr/lib/x86_64-linux-gnu/libHalf.so /usr/lib/x86_64-linux-gnu/libIlmThread.so (ver 2.2.0) GDAL: NO GDCM: NO Video I/O: DC1394 1.x: NO DC1394 2.x: YES (ver 2.2.4) FFMPEG: YES avcodec: YES (ver 56.60.100) avformat: YES (ver 56.40.101) avutil: YES (ver 54.31.100) swscale: YES (ver 3.1.101) avresample: NO GStreamer: base: YES (ver 0.10.36) video: YES (ver 0.10.36) app: YES (ver 0.10.36) riff: YES (ver 0.10.36) pbutils: YES (ver 0.10.36) OpenNI: NO OpenNI PrimeSensor Modules: NO OpenNI2: NO PvAPI: NO GigEVisionSDK: NO Aravis SDK: NO UniCap: NO UniCap ucil: NO V4L/V4L2: NO/YES XIMEA: NO Xine: NO gPhoto2: NO Parallel framework: TBB (ver 4.4 interface 9002) Other third-party libraries: Use IPP: 9.0.1 [9.0.1] at: /home/gwang/opencv-3.2.0/build/3rdparty/ippicv/ippicv_lnx Use IPP Async: NO Use VA: NO Use Intel VA-API/OpenCL: NO Use Lapack: YES (/opt/intel/mkl/lib/intel64/libmkl_core.so /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so /opt/intel/mkl/lib/intel64/libmkl_sequential.so /opt/intel/mkl/lib/intel64/libmkl_core.so /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so /opt/intel/mkl/lib/intel64/libmkl_sequential.so /opt/intel/mkl/lib/intel64/libmkl_core.so /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so /opt/intel/mkl/lib/intel64/libmkl_sequential.so -lpthread -lm -ldl) Use Eigen: NO Use Cuda: YES (ver 8.0) Use OpenCL: YES Use OpenVX: NO Use custom HAL: NO NVIDIA CUDA Use CUFFT: YES Use CUBLAS: YES USE NVCUVID: NO NVIDIA GPU arch: 20 30 35 37 50 52 60 61 NVIDIA PTX archs: Use fast math: NO OpenCL: &lt;Dynamic loading of OpenCL library&gt; Include path: /home/gwang/opencv-3.2.0/3rdparty/include/opencl/1.2 Use AMDFFT: NO Use AMDBLAS: NO Python 2: Interpreter: /usr/bin/python2.7 (ver 2.7.12) Libraries: /usr/lib/x86_64-linux-gnu/libpython2.7.so (ver 2.7.12) numpy: /usr/local/lib/python2.7/dist-packages/numpy/core/include (ver 1.11.0) packages path: lib/python2.7/dist-packages Python 3: Interpreter: /usr/bin/python3 (ver 3.5.2) Python (for build): /usr/bin/python2.7 Java: ant: NO JNI: /usr/lib/jvm/default-java/include /usr/lib/jvm/default-java/include/linux /usr/lib/jvm/default-java/include Java wrappers: NO Java tests: NO Matlab: mex: /usr/local/MATLAB/R2014a/bin/mex Compiler/generator: Not working (bindings will not be generated) Documentation: Doxygen: NO Tests and samples: Tests: YES Performance tests: YES C/C++ Examples: YES Install path: /usr/local cvconfig.h is in: /home/gwang/opencv-3.2.0/build ----------------------------------------------------------------- 同样在终端内python控制台得到的information和.py文件内得到information居然不一样，仔细分析发现这两个不是一个东西，想想是不是通过其他方式安装了opencv，通过尝试如下命令发现了这个bug： sudo pip uninstall opencv-python The directory '/home/xxx/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag. Uninstalling opencv-python-3.2.0.7: /home/xxx/.local/LICENSE-3RD-PARTY.txt /home/xxx/.local/LICENSE.txt /home/xxx/.local/lib/python2.7/site-packages/cv2/__init__.py /home/xxx/.local/lib/python2.7/site-packages/cv2/__init__.pyc /home/xxx/.local/lib/python2.7/site-packages/cv2/cv2.so /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/DESCRIPTION.rst /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/INSTALLER /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/METADATA /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/RECORD /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/WHEEL /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/metadata.json /home/xxx/.local/lib/python2.7/site-packages/opencv_python-3.2.0.7.dist-info/top_level.txt Proceed (y/n)? y Successfully uninstalled opencv-python-3.2.0.7 The directory '/home/xx/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag. 卸载后，到终端python命令行内进行测试： &gt;&gt;&gt;python &gt;&gt;&gt;import cv2 &gt;&gt;&gt;Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; ImportError: No module named cv2 没有找到cv2，那么要把编译安装后的cv2.so重新链接下： cd /usr/local/lib/python2.7/dist-packages sudo ln -s /home/xxx/opencv3.2/build/lib/cv2.so cv2.so 至此，终于搞定这个问题！！","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"编译ab压力测试并发(Ubuntu)","slug":"编译ab压力测试并发(Ubuntu)","date":"2017-09-08T07:50:31.000Z","updated":"2017-09-08T08:06:42.000Z","comments":true,"path":"2017/09/08/编译ab压力测试并发(Ubuntu)/","link":"","permalink":"http://gwang-cv.github.io/2017/09/08/编译ab压力测试并发(Ubuntu)/","excerpt":"","text":"Abstract 用深度学习框架caffe做的project，需要对其进行并发数请求数的压力测试，找到了apachebench(ab)简单的压力测试工具，但在Ubuntu下通过apt-get安装的ab，在处理请求数高于200,300左右的时候就会报错终止apr_socket_recv: Connection reset by peer，查阅资料显示需要对ab进行重新修改并编译后方可破除这个问题（无论是如何安装的，都要从头开始重新修改编译安装）。 1.下载所需要的源码 apr和apr-util下载：http://apr.apache.org/download.cgi ab-standalone源码下载:https://code.google.com/p/apachebench-standalone/downloads/list 2.编译安装 分别解压三个压缩包： ab-standalone-0.1.tar.bz2 apr-1.x.x.tar.gz apr-util-1.x.x.tar.gz a.编译apr $ cd apr $ ./configure $ make $ sudo make install b.编译apr-util $ cd apr-util $ ./configure --with-apr=/usr/local/apr $ make $ sudo make install c.编译ab 修改ab-standalone/ab.c文件 $ cd ab/ab-standalone $ vim ab.c 到1380行修改成如下: /* catch legitimate fatal apr_socket_recv errors */ else if (status != APR_SUCCESS) { err_recv++; if (recverrok) { bad++; close_connection(c); if (verbosity &gt;= 1) { char buf[120]; fprintf(stderr,\"%s: %s (%d)\\n\", \"apr_socket_recv\", apr_strerror(status, buf, sizeof buf), status); } return; } else { //apr_err(\"apr_socket_recv\", status); // begin bad++; close_connection(c); return; // end } } 然后在编译之前将apr-util/include/下的apr_base64.h和apu.h拷贝到ab-standalone文件夹内，否则找不到头文件 开始编译ab $ make apr-skeleton $ sudo make ab $ sudo cp ab apr-skeleton /usr/local/bin/ 3.注意 连接库文件问题： 终端中安装： libapr1，否则会出错找不到apr-1：sudo apt-get install libapr1 或者从编译后的usr/local/apr/lib下面将几个文件拷贝到/usr/lib/x86_64-linux-gnu下做链接： sudo cp /usr/local/apr/lib/libaprutil-1.a /usr/lib/x86_64-linux-gnu/ sudo cp /usr/local/apr/lib/liaprutil-1.so /usr/lib/x86_64-linux-gnu/ sudo cp /usr/local/apr/lib/libaprutil-1.so.0.6.0 /usr/lib/x86_64-linux-gnu/ sudo cp /usr/local/apr/lib/libaprutil-1.la /usr/lib/x86_64-linux-gnu/ cd /usr/lib/x86_64-linux-gnu/ sudo ln -s libaprutil-1.so.0.6.0 libaprutil-1.so.0 sudo ldconfig 同理，不选择apt-get安装，而如此设置libapr1.so.0即可。 4.测试 查看版本： $ ab -V This is ApacheBench, Version 2.3 &lt;$Revision$&gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ 测试并发数： ab -n 500 -c 500 127.0.0.1 # -n请求数，-c并发数，-n&gt;=-c This is ApacheBench, Version 2.3 &lt;$Revision$&gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 127.0.0.1 (be patient) Completed 100 requests Completed 200 requests Completed 300 requests Completed 400 requests Completed 500 requests Finished 500 requests Server Software: TornadoServer/4.5.2 Server Hostname: 127.0.0.1 Server Port: 1000 Document Path: / Document Length: 221727 bytes Concurrency Level: 500 Time taken for tests: 215.661 seconds Complete requests: 500 Failed requests: 1023 (Connect: 0, Receive: 341, Length: 341, Exceptions: 341) Write errors: 0 Total transferred: 35272401 bytes HTML transferred: 35254593 bytes Requests per second: 2.32 [#/sec] (mean) Time per request: 215661.110 [ms] (mean) Time per request: 431.322 [ms] (mean, across all concurrent requests) Transfer rate: 159.72 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 355 3586.5 0 63141 Processing: 1557 119576 33974.7 127335 182963 Waiting: 0 33466 58718.9 0 182963 Total: 1566 119931 34207.2 127335 210234 Percentage of the requests served within a certain time (ms) 50% 127335 66% 127336 75% 127337 80% 127337 90% 151213 95% 175948 98% 176152 99% 177742 100% 210234 (longest request) 参考 http://shenwang.blog.ustc.edu.cn/%E5%AE%89%E8%A3%85abapachebench-standalone/http://blog.csdn.net/kimsung/article/details/9422991https://code.google.com/p/apachebench-standalone/wiki/HowToBuild","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Caffe训练分类模型","slug":"Caffe训练分类模型","date":"2017-08-31T06:27:54.000Z","updated":"2017-08-31T07:42:11.000Z","comments":true,"path":"2017/08/31/Caffe训练分类模型/","link":"","permalink":"http://gwang-cv.github.io/2017/08/31/Caffe训练分类模型/","excerpt":"","text":"1.安装Caffe 按照官方https://github.com/BVLC/caffe说明进行编译，若使用了较老的cudnn版本，则要在老版本的caffe与新版本的caffe进行合并，具体是将cudnn开头的相关文件替换掉旧版本caffe里的。 2.整理自己的数据 图像文件可以放置在硬盘的任何位置，将训练的图像及label生成一个train.txt文件，然后验证图像和label生成一个val.txt，一起放到caffe/data/mydata文件夹下。 path/to/image/001.jpg 0 path/to/image/002.jpg 1 path/to/image/003.jpg 2 在caffe/examples/imagenet中，将创建lmdb文件和mean文件的的几个sh文件拷贝到自己的examples/mytask中，然后修改这两个文件中的几个地址和名称： 修改create_imagenet.sh //当前路径 EXAMPLE=/home/xxx/caffe/examples/mytask //存放train.txt和val.txt的路径，就是data/mydata DATA=/home/xxx/caffe/data/mydata //不变 TOOLS=/home/xxx/caffe/build/tools //存放train和val图片文件夹的主目录（这里与txt文件中的目录合起来才是图片目录） TRAIN_DATA_ROOT=/home/xxx/project-data/ VAL_DATA_ROOT=/home/xxx/project-data/ GLOG_logtostderr=1 $TOOLS/convert_imageset \\ --resize_height=$RESIZE_HEIGHT \\ --resize_width=$RESIZE_WIDTH \\ --shuffle \\ $TRAIN_DATA_ROOT \\ $DATA/train.txt \\ $EXAMPLE/train_lmdb ##修改名称，与prototxt中的对应 echo \"Creating val lmdb...\" GLOG_logtostderr=1 $TOOLS/convert_imageset \\ --resize_height=$RESIZE_HEIGHT \\ --resize_width=$RESIZE_WIDTH \\ --shuffle \\ $VAL_DATA_ROOT \\ $DATA/val.txt \\ $EXAMPLE/val_lmdb ##修改名称 同理，修改make_imagenet_mean.sh。 分别执行这两个命令，创建文件lmdb数据库和均值mean文件。 3.设置模型 在caffe/models下有很多预制的模型可供使用，模型文件一共可包括至少三个文件：deploy.prototxt；solver.prototxt；train_val.prototxt，有时需要finetune已训练好的模型，还需要一些比如在imagenet上预训练的caffemodel文件。 sovler里面包含超参数和一些信息： net: \"models/bvlc_googlenet/train_val.prototxt\" test_iter: 1000 test_interval: 4000 test_initialization: false display: 40 average_loss: 40 base_lr: 0.01 lr_policy: \"step\" stepsize: 320000 gamma: 0.96 max_iter: 10000000 momentum: 0.9 weight_decay: 0.0002 snapshot: 40000 snapshot_prefix: \"models/bvlc_googlenet/bvlc_googlenet\" solver_mode: GPU train_val里面的主要修改的地方是batch_size，它与sovler里面的test_iter相乘的结果对应训练图像的总数；修改train和val的sorce文件地址：train_lmdb,val_lmdb. data_param { source: \"examples/imagenet/ilsvrc12_train_lmdb\" batch_size: 32 backend: LMDB } PS: 若使用lmdb的话，type为Data，使用图像的话就是ImageData layer { name: \"data\" type: \"Data\" top: \"data\" top: \"label\" include { phase: TRAIN } PS: 在finetune残差网络resnet时，网络结构文件中BatchNorm层的参数要注意： 1.在训练时所有BN层要设置use_global_stats: false（也可以不写，caffe默认是false） 2.在测试时所有BN层要设置use_global_stats: true #1.训练如果不设为false，会导致模型不收敛 #2.测试如果不设置为true，会导致准确率极低 4.开始训练 #!/bin/bash #!/usr/bin/env sh PRETRAINED_MODEL=./models/resnet50/ResNet-50-model.caffemodel t=$(date +%Y-%m-%d_%H:%M:%S) LOG=./log/train_res50_$t.log GLOG_logtostderr=1 ./build/tools/caffe train \\ --solver=./models/resnet50/solver.prototxt \\ --weights=$PRETRAINED_MODEL \\ --gpu=0,1 2&gt;&amp;1 | tee $LOG 训练时若出现内存不足停止，要减小train_val.prototxt中batchsize的大小。若出现waiting for data，可能是硬盘读写速度慢，那就waiting吧。。。 5.测试 #!/bin/bash #!/usr/bin/env sh GLOG_logtostderr=1 ./build/tools/caffe test \\ --model=./models/resnet50/ResNet-50-train-val.prototxt \\ --weights=./models/resnet50/output_iter_100000.caffemodel \\ --gpu=1 \\ --iterations=2000 6.分类demo 在caffe/python下的classify.py文件可以用来对图像进行分类，但是没有输出可见的分类结果，需要改写些内容。开始之前需要把mean均值文件转换为.npy格式，代码如下： import caffe import numpy as np import sys if len(sys.argv) != 3: print \"Usage: python convert_protomean.py proto.mean out.npy\" sys.exit() blob = caffe.proto.caffe_pb2.BlobProto() data = open( sys.argv[1] , 'rb' ).read() blob.ParseFromString(data) arr = np.array( caffe.io.blobproto_to_array(blob) ) out = arr[0] np.save( sys.argv[2] , out ) 自己改写的classify-demo.py import numpy as np import sys import os import caffe import io #from sys import argv #input filefolder you want to test #tools,filefolder=argv caffe.set_device(0) caffe.set_mode_gpu() MODEL_FILE =\"../models/googlenet/deploy.prototxt\" PRETRAINED = \"../models/googlenet/output/bvlc_googlenet_iter_200000.caffemodel\" MEAN = \"mean.npy\" net = caffe.Classifier(MODEL_FILE, PRETRAINED, mean = np.load(MEAN).mean(1).mean(1), channel_swap=(2,1,0), raw_scale=255, image_dims=(256, 256)) filewriter = open(\"../data/mydata/result.txt\",\"w+\") labels_file = '../data/mydata/synset_words.txt' labels = np.loadtxt(labels_file, str, delimiter='\\t') #load groundtruth txt to dictionary groundtruth_file = '../data/mydata/groundtruth/abc.txt' dict_data = {} with open(groundtruth_file, 'r') as df: for kv in [d.strip().split(' ') for d in df]: dict_data[kv[0]] = kv[1] for root,dirs,files in os.walk(\"/home/xxx/project_data/mydata/abc/\"): # all the images are in test folder for file in files: IMAGE_FILE = os.path.join(root,file).decode('gbk').encode('utf8') #print IMAGE_FILE input_image = caffe.io.load_image(IMAGE_FILE) prediction = net.predict([input_image]) string = os.path.basename(IMAGE_FILE)+\" \"+str(prediction[0].argmax())+\"\\n\" filewriter.write(string) #print os.path.basename(IMAGE_FILE), prediction[0].argmax() # load ImageNet labels label_true=dict_data[os.path.basename(IMAGE_FILE)] if label_true==str(prediction[0].argmax()): print os.path.basename(IMAGE_FILE)+\" \"+labels[prediction[0].argmax()]+' '+'v' else: print os.path.basename(IMAGE_FILE)+\" \"+labels[prediction[0].argmax()]+' '+'x' filewriter.close() 7.plot训练测试曲线 caffe/tools/extra下包含几个有用的工具，其中包括将日志文件.log绘制出训练阶段的loss变化和测试阶段accuracy的变化情况，具体： ./parse_log.sh xxxx.log #解析日志，划分为train test两个文件 ./plot_training_log.py.example 0~7 save.png xxxx.log #绘制图 Notes: 1. Supporting multiple logs. 2. Log file name must end with the lower-cased \".log\". Supported chart types: 0: Test accuracy vs. Iters 1: Test accuracy vs. Seconds 2: Test loss vs. Iters 3: Test loss vs. Seconds 4: Train learning rate vs. Iters 5: Train learning rate vs. Seconds 6: Train loss vs. Iters 7: Train loss vs. Seconds","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"python+opencv图像二值化","slug":"python+opencv图像二值化","date":"2017-08-25T08:20:01.000Z","updated":"2017-08-25T08:27:55.000Z","comments":true,"path":"2017/08/25/python+opencv图像二值化/","link":"","permalink":"http://gwang-cv.github.io/2017/08/25/python+opencv图像二值化/","excerpt":"","text":"Goal In this tutorial, you will learn Simple thresholding, Adaptive thresholding, Otsu’s thresholding etc.You will learn these functions : cv2.threshold, cv2.adaptiveThreshold etc. Simple Thresholding Here, the matter is straight forward. If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black). The function used is cv2.threshold. First argument is the source image, which should be a grayscale image. Second argument is the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be given if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function. Different types are: cv2.THRESH_BINARY cv2.THRESH_BINARY_INV cv2.THRESH_TRUNC cv2.THRESH_TOZERO cv2.THRESH_TOZERO_INV Documentation clearly explain what each type is meant for. Please check out the documentation. Two outputs are obtained. First one is a retval which will be explained later. Second output is our thresholded image. code: #!/usr/bin/python # -*- coding: UTF-8 -*- import cv2 import numpy as np from PIL import Image from matplotlib import pyplot as plt import matplotlib import os import sys reload(sys) sys.setdefaultencoding('utf-8') fln='76.jpg' img = cv2.imread(fln,0) ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV) ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC) ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO) ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV) titles = ['Original_Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV'] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] for i in xrange(6): cv2.imwrite('0_'+titles[i]+'.jpg',images[i]) plt.subplot(2,3,i+1),plt.imshow(images[i],'gray') plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.savefig('res_'+fln) plt.show() Adaptive Thresholding In the previous section, we used a global value as threshold value. But it may not be good in all the conditions where image has different lighting conditions in different areas. In that case, we go for adaptive thresholding. In this, the algorithm calculate the threshold for a small regions of the image. So we get different thresholds for different regions of the same image and it gives us better results for images with varying illumination. It has three ‘special’ input params and only one output argument. Adaptive Method - It decides how thresholding value is calculated. cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area. cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window. Block Size - It decides the size of neighbourhood area. C - It is just a constant which is subtracted from the mean or weighted mean calculated. Below piece of code compares global thresholding and adaptive thresholding for an image with varying illumination: import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread('sudoku.png',0) img = cv2.medianBlur(img,5) ret,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) th2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\ cv2.THRESH_BINARY,11,2) th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\ cv2.THRESH_BINARY,11,2) titles = ['Original Image', 'Global Thresholding (v = 127)', 'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding'] images = [img, th1, th2, th3] for i in xrange(4): plt.subplot(2,2,i+1),plt.imshow(images[i],'gray') plt.title(titles[i]) plt.xticks([]),plt.yticks([]) plt.show() Otsu’s Binarization In the first section, I told you there is a second parameter retVal. Its use comes when we go for Otsu’s Binarization. So what is it? In global thresholding, we used an arbitrary value for threshold value, right? So, how can we know a value we selected is good or not? Answer is, trial and error method. But consider a bimodal image (In simple words, bimodal image is an image whose histogram has two peaks). For that image, we can approximately take a value in the middle of those peaks as threshold value, right ? That is what Otsu binarization does. So in simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.) For this, our cv2.threshold() function is used, but pass an extra flag, cv2.THRESH_OTSU. For threshold value, simply pass zero. Then the algorithm finds the optimal threshold value and returns you as the second output, retVal. If Otsu thresholding is not used, retVal is same as the threshold value you used. Check out below example. Input image is a noisy image. In first case, I applied global thresholding for a value of 127. In second case, I applied Otsu’s thresholding directly. In third case, I filtered image with a 5x5 gaussian kernel to remove the noise, then applied Otsu thresholding. See how noise filtering improves the result. import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread('noisy2.png',0) # global thresholding ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) # Otsu's thresholding ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # Otsu's thresholding after Gaussian filtering blur = cv2.GaussianBlur(img,(5,5),0) ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # plot all the images and their histograms images = [img, 0, th1, img, 0, th2, blur, 0, th3] titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)', 'Original Noisy Image','Histogram',\"Otsu's Thresholding\", 'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"] for i in xrange(3): plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray') plt.title(titles[i*3]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256) plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray') plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([]) plt.show() Link: http://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Tesseract4.0+jTessBoxEditor训练(ubuntu下)","slug":"Tesseract4.0+jTessBoxEditor训练(ubuntu下)","date":"2017-08-25T07:48:57.000Z","updated":"2017-08-25T08:07:35.000Z","comments":true,"path":"2017/08/25/Tesseract4.0+jTessBoxEditor训练(ubuntu下)/","link":"","permalink":"http://gwang-cv.github.io/2017/08/25/Tesseract4.0+jTessBoxEditor训练(ubuntu下)/","excerpt":"","text":"下载jTessBoxEditor https://sourceforge.net/projects/vietocr/files/ unzip jTessBoxEditor-2.0-Beta.zip cd jTessBoxEditor java -Xms128m -Xmx1024m -jar jTessBoxEditor.jar #打开可视化界面 —- 训练 —- 预处理 把所有待训练的图像转换为tiff格式（可使用ubuntu下的imageMagick的convert命令或mogrify命令批量处理） mogrify -path ./TIFF/ -format tiff *.jpg 然后采用jTessBoxEditor中的tools/Merge Tiff功能，将待训练的tiff图像合并为一个tiff文件，在可视化界面中先选中所有tiff文件，然后点击OK，然后在再次弹出的窗口中输入要合并的tiff文件的名称，例如chi.wang.exp0.tif，chi表示中文，wang表示字体。 生成.box 文件 sudo tesseract chi.wang.exp0.tif chi.wang.exp0 batch.nochop makebox or sudo tesseract chi.wang.exp0.tif chi.wang.exp0 -l chi_smi -psm 7 digits batch.nochop makebox # 数字 会生成一个名为chi.wang.exp0.box文件 使用jTessBoxEditor校正字符 在可视化界面中，在Box Editor面板中点击open按钮打开合并的tiff文件，然后每一页每一页的去矫正矩形框的位置以及正确的识别字符，亦可以同时修改chi.wang.exp0.box文件来添加一下empty pages的识别框！ 生成自制语言库 sudo tesseract chi.wang.exp0.tif chi.wang.exp0 nobatch box.train #生成字符特征文件 sudo unicharset_extractor chi.wang.exp0.box #产生字符集 vim font_properties wang 0 0 0 0 0 #向新生成的文件font_properties中录入此内容，普通字体，不加粗，不倾斜 sudo shapeclustering -F font_properties -U unicharset chi.wang.exp0.tr #生成shapetable sudo mftraining -F font_properties -U unicharset chi.wang.exp0.tr #聚集字符特征 sudo cntraining chi.wang.exp0.tr #生成字符形状正常变化特征文件normproto 重命名如下文件： inttemp ├── normproto ├── pffmtable ├── shapetable └── unicharset 为 wang.inttemp ├── wang.normproto ├── wang.pffmtable ├── wang.shapetable └── wang.unicharset 生成语言库 sudo combine_tessdata wang. 此时得到了训练好的wang.traineddata sudo cp wang.traineddata /usr/share/tesseract-ocr/tessdata/wang.traineddata #复制到安装目录 测试 tesseract 1.jpg 1 -l wang 上述命令可能会出错，得到如下输出： FAIL! APPLY_BOXES: boxfile line 22/园 ((1108,4637),(1150,4686)): FAILURE! Couldn't find a matching blob FAIL! APPLY_BOXES: boxfile line 25/园 ((1258,4641),(1300,4689)): FAILURE! Couldn't find a matching blob APPLY_BOXES: Boxes read from boxfile: 40 Boxes failed resegmentation: 2 Found 38 good blobs. Generated training data for 4 words Warning in pixReadMemTiff: tiff page 1 not found 意味着园字的块没找到，tr文件中就缺失了园字的特征。如果出现FATALITIES错误，必须先修正box文件修订错误才能继续。FATALITY通常意味着没有找到box文件中的任何字体。要么是坐标错误，要么是字符图像出了问题。如果字符没有可供使用的样本，它就不能被识别，那么得到的inttemp文件和unicharset文件就不匹配，Tesseract会退出。 如果tif/box文件对是通过jTessBoxEditor可视化工具得到的，那么生成文件时选定Anti-Aliasing可消除上述错误。 参考link： http://qianjiye.de/2015/08/tesseract-ocr http://lib.csdn.net/article/deeplearning/50598 http://www.jianshu.com/p/31afd7fc5813","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"ubuntu16.04+python2.7中文编码","slug":"ubuntu16.04+python2.7中文编码","date":"2017-08-25T07:11:31.000Z","updated":"2017-08-25T07:11:31.000Z","comments":true,"path":"2017/08/25/ubuntu16.04+python2.7中文编码/","link":"","permalink":"http://gwang-cv.github.io/2017/08/25/ubuntu16.04+python2.7中文编码/","excerpt":"","text":"文件头添加： #!/usr/bin/python # -*- coding: UTF-8 -*- 代码中添加： import sys reload(sys) sys.setdefaultencoding('utf-8') 执行代码即可： if isinstance(ss, unicode): #unicode(\"中文字符\").encode(\"utf-8\") ss = unicode(ss).encode(\"utf-8\") print ss matplotlib绘图中中文乱码情况解决 查看ubuntu中文字体 $ fc-list :lang=zh-cn 安装中文字体 sudo apt-get install -y --force-yes --no-install-recommends fonts-wqy-microhei 再次查看fc-list :lang=zh-cn 其中输出： /usr/share/fonts/truetype/wqy/wqy-microhei.ttc: WenQuanYi Micro Hei,文泉驛微米黑,文泉驿微米黑:style=Regular 复制ttc文件地址 /usr/share/fonts/truetype/wqy/wqy-microhei.ttc 在python代码中： zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc') plt.title(unicode(ss).encode(\"utf-8\"), color='white', fontproperties= zhfont) 即可解决！！","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"ubuntu16.04+Tesseract4.0","slug":"ubuntu16.04+Tesseract4.0","date":"2017-08-25T07:06:09.000Z","updated":"2017-09-05T07:38:17.000Z","comments":true,"path":"2017/08/25/ubuntu16.04+Tesseract4.0/","link":"","permalink":"http://gwang-cv.github.io/2017/08/25/ubuntu16.04+Tesseract4.0/","excerpt":"","text":"The build instructions for Linux also apply to other UNIX like operating systems. 安装依赖Dependencies A compiler for C and C++: GCC or ClangGNU Autotools: autoconf, automake, libtoolautoconf-archivepkg-configLeptonicalibpng, libjpeg, libtiff Ubuntu If they are not already installed, you need the following libraries (Ubuntu 16.04/14.04): sudo apt-get install g++ # or clang++ (presumably) sudo apt-get install autoconf automake libtool sudo apt-get install autoconf-archive sudo apt-get install pkg-config sudo apt-get install libpng12-dev sudo apt-get install libjpeg8-dev sudo apt-get install libtiff5-dev sudo apt-get install zlib1g-dev if you plan to install the training tools, you also need the following libraries: sudo apt-get install libicu-dev sudo apt-get install libpango1.0-dev sudo apt-get install libcairo2-dev Leptonica You also need to install Leptonica. Ensure that the development headers for Leptonica are installed before compiling Tesseract. Tesseract versions and the minimum version of Leptonica required: Tesseract Leptonica Ubuntu4.00 1.74.2 Must build from source3.05 1.74.0 Must build from source3.04 1.71 Ubuntu 16.043.03 1.70 Ubuntu 14.043.02 1.69 Ubuntu 12.043.01 1.67 One option is to install the distro’s Leptonica package: sudo apt-get install libleptonica-dev but if you are using an oldish version of Linux, the Leptonica version may be too old, so you will need to build from source. The sources are at https://github.com/DanBloomberg/leptonica . The instructions for building are given in Leptonica README. Leptonica 1.74.4 git clone --recursive https://github.com/DanBloomberg/leptonica cd leptonica ./autobuild ./configure ./make-for-auto sudo make sudo make install Tesseract 4.0 git clone --depth 1 https://github.com/tesseract-ocr/tesseract.git tesseract cd tesseract ./autogen.sh ./configure --enable-debug LDFLAGS=\"-L/usr/local/lib\" CFLAGS=\"-I/usr/local/include\" make sudo make install sudo ldconfig build training tools if you like: make training sudo make training-install test Tesseract $ tesseract imagename outputbase [-l lang] [--psm pagesegmode] [configfiles...] $ tesseract 1.jpg 1.txt -l chi_sim Error: Error opening data file /usr/local/share/eng.traineddata Please make sure the TESSDATA_PREFIX environment variable is set to your \"tessdata\" directory. Failed loading language 'eng' Tesseract couldn't load any languages! Could not initialize tesseract. 办法：下载语言包 https://github.com/tesseract-ocr/tessdata/blob/master/eng.traineddata https://github.com/tesseract-ocr/tessdata #These language data files only work with Tesseract 4.0 sudo mv eng.traineddata /usr/share/tesseract-ocr/tessdata export TESSDATA_PREFIX=/usr/share/tesseract-ocr/tessdata or sudo mv /usr/local/share/tessdata /usr/local/share/tessdata.bak sudo ln -s /usr/share/tesseract-ocr/tessdata /usr/local/share/ 安装python接口 sudo pip install pytesseract 比如识别中文及数字: tessdata_dir_config='-psm 7 digits' ss = pytesseract.image_to_string(image, lang='chi_sim', config=tessdata_dir_config) 修改配置文件 当使用命令参数 digits来识别数字时，有考虑识别字母和数字，即可在系统tesseract所在位置修改配置文件：usr/share/tesseract-orc/tessdata/configs/digits tessedit_chaar_whitelist 0123456789-. #default tessedit_chaar_whitelist ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-. PS：图片太大时识别不好，缩放到指甲盖大小反而识别会好些。。。 link: https://www.youtube.com/watch?v=vOdnt2h1U8U https://lengerrong.blogspot.com/2017/03/how-to-build-latest-tesseract-leptonica.html https://github.com/tesseract-ocr/tesseract/wiki/Compiling-%E2%80%93-GitInstallation https://github.com/tesseract-ocr/tesseract/wiki/Compiling#linux https://lucacerone.net/2017/install-tesseract-3-0-5-in-ubuntu-16-04/","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"安装Hexo的问题","slug":"安装Hexo的问题","date":"2017-08-25T06:48:15.000Z","updated":"2017-08-25T07:54:24.000Z","comments":true,"path":"2017/08/25/安装Hexo的问题/","link":"","permalink":"http://gwang-cv.github.io/2017/08/25/安装Hexo的问题/","excerpt":"","text":"安装Hexo时，执行npm install -g hexo-cli出现错误： npm ERR! tar.unpack untar error /Users/Macx/.npm/hexo-cli/0.1.8/package.tgz npm ERR! Darwin 14.4.0 npm ERR! argv \"/usr/local/bin/node\" \"/usr/local/bin/npm\" \"install\" \"hexo-cli\" \"-g\" npm ERR! node v4.2.1 npm ERR! npm v2.14.7 npm ERR! path /usr/local/lib/node_modules/hexo-cli npm ERR! code EACCES npm ERR! errno -13 npm ERR! syscall mkdir npm ERR! Error: EACCES: permission denied, mkdir '/usr/local/lib/node_modules/hexo-cli' npm ERR! at Error (native) npm ERR! { [Error: EACCES: permission denied, mkdir '/usr/local/lib/node_modules/hexo-cli'] npm ERR! errno: -13, npm ERR! code: 'EACCES', npm ERR! syscall: 'mkdir', npm ERR! path: '/usr/local/lib/node_modules/hexo-cli', npm ERR! fstream_type: 'Directory', npm ERR! fstream_path: '/usr/local/lib/node_modules/hexo-cli', npm ERR! fstream_class: 'DirWriter', npm ERR! fstream_stack: npm ERR! [ '/usr/local/lib/node_modules/npm/node_modules/fstream/lib/dir-writer.js:35:25', npm ERR! '/usr/local/lib/node_modules/npm/node_modules/mkdirp/index.js:47:53', npm ERR! 'FSReqWrap.oncomplete (fs.js:82:15)' ] } npm ERR! npm ERR! Please try running this command again as root/Administrator. npm ERR! Please include the following file with any support request: npm ERR! /Users/Macx/Desktop/GitHub/npm-debug.log 分析下错误，异常可能因为权限问题，所以我们执行一些安装命令是需要申请root执行权限。执行以下代码解决: sudo npm install --unsafe-perm --verbose -g hexo 成功解决！ pm WARN optional Skipping failed optional dependency /hexo/chokidar/fsevents: npm WARN notsup Not compatible with your operating system or architecture: fsevents@1.1.2 npm verb exit [ 0, true ] npm info ok $ hexo --version hexo-cli: 1.0.3 os: Linux 4.4.0-92-generic linux x64 http_parser: 2.5.0 node: 4.2.6 v8: 4.5.103.35 uv: 1.8.0 zlib: 1.2.8 ares: 1.10.1-DEV icu: 55.1 modules: 46 openssl: 1.0.2g-fips 生成github ssh公钥 cd github ssh-keygen -t rsa -C \"your_email@youremail.com\" 将github/id_rsa.pub的公钥添加到github 测试是否生效: ssh -T git@github.com 若出现Permission denied (publickey). 执行一句ssh-add github/id-rsa 即可。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Ubuntu下载工具","slug":"Ubuntu下载工具","date":"2017-08-11T02:12:02.000Z","updated":"2017-08-11T02:17:24.000Z","comments":true,"path":"2017/08/11/Ubuntu下载工具/","link":"","permalink":"http://gwang-cv.github.io/2017/08/11/Ubuntu下载工具/","excerpt":"","text":"Ubuntu下也有类似的下载工具，那就是aira2。aira2是一个命令行下载工具，可以配合其他图形界面的下载软件使用。即使用uget+aria2。uget本身是一个小巧实用的多线程下载工具，加上aria2作为插件，下载速度有明显提高。 安装 使用ppa方式进行安装较新的版本。 1.uget的安装： sudo add-apt-repository ppa:plushuang-tw/uget-stable sudo apt-get update sudo apt-get install uget 2.aria2的安装： sudo add-apt-repository ppa:t-tujikawa/ppa sudo apt-get update sudo apt-get install aria2 安装完aria2后，可以在终端中运行aria2c -v，查看版本和支持的特性。需要1.10以上的版本才能支持资源搜索。 最后打开uget可视化界面，依次在菜单Edit -&gt; Settings -&gt; uGet - Settings-&gt; Plug-in -&gt; Plug-in matching order: aria2 -&gt; Arguments: --enable-rpc=true -D -disable-ipv6 --check-certificate=false. 然后在主界面左侧的All Category右键选择Properties，在第二个选项页下设置下载文件夹以及最大连接数Max Connections: 16. 下载对比： Chrome浏览器下载百度盘内的资料，网速为80k/s左右； 使用uGet下载网速可达1.2M/s。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/tags/Debug/"}]},{"title":"Ubuntu下查看显卡型号及NVIDIA驱动版本","slug":"Ubuntu下查看显卡型号及NVIDIA驱动版本","date":"2017-08-07T07:32:22.000Z","updated":"2017-08-07T07:32:22.000Z","comments":true,"path":"2017/08/07/Ubuntu下查看显卡型号及NVIDIA驱动版本/","link":"","permalink":"http://gwang-cv.github.io/2017/08/07/Ubuntu下查看显卡型号及NVIDIA驱动版本/","excerpt":"","text":"查看GPU型号 lspci | grep -i nvidia 查看NVIDIA驱动版本 sudo dpkg --list | grep nvidia-*","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"TextBoxes文本检测","slug":"TextBoxes文本检测","date":"2017-08-02T08:03:20.000Z","updated":"2017-08-02T08:13:10.000Z","comments":true,"path":"2017/08/02/TextBoxes文本检测/","link":"","permalink":"http://gwang-cv.github.io/2017/08/02/TextBoxes文本检测/","excerpt":"","text":"安装环境：Ubuntu16.04, Nvidia Titan x, cuda 8.0, cudnn 5.0, protobuf 2.6, python 2.7, OpenCV 3.2 下载TextBoxes：https://github.com/MhLiao/TextBoxes # 修改Makefile.config文件：use cuda，use opencv, mkl git clone https://github.com/MhLiao/TextBoxes.git cd TextBoxes make -j64 make py 测试demo 下载训练好的模型Models trained on ICDAR 2013，拷贝到文件夹examples/TextBoxes/下。 # 测试demo.jpg的文本区域检测 python examples/TextBoxes/demo.py 报错1：No module named shapely安装shapely： sudo apt-get install python-shapely 报错2：Could not find lib geos_c or load any of its variants安装shapely的依赖库GEOS： sudo apt-get install libgeos-dev","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"py-R-FCN-MultiGPU安装","slug":"py-R-FCN-MultiGPU安装","date":"2017-07-29T09:00:26.000Z","updated":"2017-08-07T04:09:12.000Z","comments":true,"path":"2017/07/29/py-R-FCN-MultiGPU安装/","link":"","permalink":"http://gwang-cv.github.io/2017/07/29/py-R-FCN-MultiGPU安装/","excerpt":"","text":"下载R-FCN master git clone --recursive https://github.com/bharatsingh430/py-R-FCN-multiGPU/ coco branch git clone --recursive https://github.com/bharatsingh430/py-R-FCN-multiGPU/ 编译Cython模块 cd py-R-FCN-MultiGPU-coco/lib make 安装依赖 cython, python-opencv, easydict 修改Makefile.config # In your Makefile.config, make sure to have this line uncommented WITH_PYTHON_LAYER := 1 # Unrelatedly, it's also recommended that you use CUDNN USE_CUDNN := 1 USE_NCCL := 1 安装NCCL #Nvidia's NCCL library which is used for multi-GPU training #nccl.hpp:5:18: fatal error: nccl.h: No such file or directory # 在多个 GPU 上运行 Caffe 需要使用 NVIDIA NCCL git clone https://github.com/NVIDIA/nccl.git cd nccl sudo make install -j32 # NCCL 库和文件头将安装在 /usr/local/lib 和 /usr/local/include 中 sudo ldconfig # 该命令不执行会出现错误： error while loading shared libraries: libnccl.so.1: cannot open shared object file: No such file or directory Build Caffe and pycaffe cd py-R-FCN-MultiGPU-coco/caffe make -j8 &amp;&amp; make pycaffe 若编译过程报错：fatal error: caffe/proto/caffe.pb.h: No such file or directoryhttps://github.com/NVIDIA/DIGITS/issues/105 可通过如下方式将编译通过（首先需要进入 caffe 根目录）： protoc src/caffe/proto/caffe.proto --cpp_out=. sudo mkdir include/caffe/proto sudo mv src/caffe/proto/caffe.pb.h include/caffe/proto 尝试 下载训练好的模型https://1drv.ms/u/s!AoN7vygOjLIQqUWHpY67oaC7mopf 解压并拷贝到目录： py-R-FCN-MultiGPU-coco/data/rfcn_models/resnet50_rfcn_final.caffemodel py-R-FCN-MultiGPU-coco/data/rfcn_models/resnet101_rfcn_final.caffemodel 运行demo ./tools/demo_rfcn.py demo报错： [libprotobuf ERROR google/protobuf/message_lite.cc:123] Can't parse message of type \"caffe.NetParameter\" because it is missing required fields: layer[494].psroi_pooling_param.output_dim, layer[494].psroi_pooling_param.group_size 可能的解决方法： zhanghaoinf commented on Apr 12 • edited Hi, @liu09114 , I checked before when I used the detection model published by msra (models store in onedrive), the problem will occur. If you initialize and train your own model with resnet-101 (classification model pre-trained on ImageNet-1000), the problem will disappear. By the way, you need the coco branch to train your own model. I think the sentence \"If you want to use/train this model, please use the coco branch of this repository. \" in the readme is important. :) 训练自己的模型1.数据集 数据集拷贝到$RFCN_ROOT/data下，此处只有VOC2007的数据： VOCdevkit2007 #link VOCdevkit0712 #link VOCdevkit/VOC2007/ VOCdevkit/VOC0712/ #作者是用VOC2007和VOC2012训练的，所以文件夹名字带0712 2.准备预训练模型 deep-residual-networks: https://github.com/KaimingHe/deep-residual-networks OneDrive download: https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&amp;id=4006CBB8476FF777%2117887&amp;cid=4006CBB8476FF777 然后将caffemodel放在./data/imagenet_models文件夹下。 3.修改网络 打开./models/pascal_voc/ResNet-50/rfcn_end2end (以end2end为例) PS：下面的cls_num指的是你数据集的类别数+1（背景）。比如我有15类，+1类背景，cls_num=16. 修改class-aware/train_ohem.prototxt layer { name: 'input-data' type: 'Python' top: 'data' top: 'im_info' top: 'gt_boxes' python_param { module: 'roi_data_layer.layer' layer: 'RoIDataLayer' param_str: \"'num_classes': 16\" #cls_num } } + layer { name: 'roi-data' type: 'Python' bottom: 'rpn_rois' bottom: 'gt_boxes' top: 'rois' top: 'labels' top: 'bbox_targets' top: 'bbox_inside_weights' top: 'bbox_outside_weights' python_param { module: 'rpn.proposal_target_layer' layer: 'ProposalTargetLayer' param_str: \"'num_classes': 16\" #cls_num } } + layer { bottom: \"conv_new_1\" top: \"rfcn_cls\" name: \"rfcn_cls\" type: \"Convolution\" convolution_param { num_output: 784 #cls_num*(score_maps_size^2) kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"conv_new_1\" top: \"rfcn_bbox\" name: \"rfcn_bbox\" type: \"Convolution\" convolution_param { num_output: 3136 #4*cls_num*(score_maps_size^2) kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"rfcn_cls\" bottom: \"rois\" top: \"psroipooled_cls_rois\" name: \"psroipooled_cls_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 16 #cls_num group_size: 7 } } + layer { bottom: \"rfcn_bbox\" bottom: \"rois\" top: \"psroipooled_loc_rois\" name: \"psroipooled_loc_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 64 #4*cls_num group_size: 7 } } 修改class-aware/test.prototxt layer { bottom: \"conv_new_1\" top: \"rfcn_cls\" name: \"rfcn_cls\" type: \"Convolution\" convolution_param { num_output: 784 #cls_num*(score_maps_size^2) kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"conv_new_1\" top: \"rfcn_bbox\" name: \"rfcn_bbox\" type: \"Convolution\" convolution_param { num_output: 3136 #4*cls_num*(score_maps_size^2) kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"rfcn_cls\" bottom: \"rois\" top: \"psroipooled_cls_rois\" name: \"psroipooled_cls_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 16 #cls_num group_size: 7 } } + layer { bottom: \"rfcn_bbox\" bottom: \"rois\" top: \"psroipooled_loc_rois\" name: \"psroipooled_loc_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 64 #4*cls_num group_size: 7 } } + layer { name: \"cls_prob_reshape\" type: \"Reshape\" bottom: \"cls_prob_pre\" top: \"cls_prob\" reshape_param { shape { dim: -1 dim: 16 #cls_num } } } + layer { name: \"bbox_pred_reshape\" type: \"Reshape\" bottom: \"bbox_pred_pre\" top: \"bbox_pred\" reshape_param { shape { dim: -1 dim: 64 #4*cls_num } } } 修改train_agnostic.prototxt layer { name: 'input-data' type: 'Python' top: 'data' top: 'im_info' top: 'gt_boxes' python_param { module: 'roi_data_layer.layer' layer: 'RoIDataLayer' param_str: \"'num_classes': 16\" #cls_num } } + layer { bottom: \"conv_new_1\" top: \"rfcn_cls\" name: \"rfcn_cls\" type: \"Convolution\" convolution_param { num_output: 784 #cls_num*(score_maps_size^2) ### kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"rfcn_cls\" bottom: \"rois\" top: \"psroipooled_cls_rois\" name: \"psroipooled_cls_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 16 #cls_num ### group_size: 7 } } 修改train_agnostic_ohem.prototxt layer { name: 'input-data' type: 'Python' top: 'data' top: 'im_info' top: 'gt_boxes' python_param { module: 'roi_data_layer.layer' layer: 'RoIDataLayer' param_str: \"'num_classes': 16\" #cls_num ### } } + layer { bottom: \"conv_new_1\" top: \"rfcn_cls\" name: \"rfcn_cls\" type: \"Convolution\" convolution_param { num_output: 784 #cls_num*(score_maps_size^2) ### kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"rfcn_cls\" bottom: \"rois\" top: \"psroipooled_cls_rois\" name: \"psroipooled_cls_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 16 #cls_num ### group_size: 7 } } 修改test_agnostic.prototxt layer { bottom: \"conv_new_1\" top: \"rfcn_cls\" name: \"rfcn_cls\" type: \"Convolution\" convolution_param { num_output: 784 #cls_num*(score_maps_size^2) ### kernel_size: 1 pad: 0 weight_filler { type: \"gaussian\" std: 0.01 } bias_filler { type: \"constant\" value: 0 } } param { lr_mult: 1.0 } param { lr_mult: 2.0 } } + layer { bottom: \"rfcn_cls\" bottom: \"rois\" top: \"psroipooled_cls_rois\" name: \"psroipooled_cls_rois\" type: \"PSROIPooling\" psroi_pooling_param { spatial_scale: 0.0625 output_dim: 16 #cls_num ### group_size: 7 } } + layer { name: \"cls_prob_reshape\" type: \"Reshape\" bottom: \"cls_prob_pre\" top: \"cls_prob\" reshape_param { shape { dim: -1 dim: 16 #cls_num ### } } } 4.修改代码 修改./lib/datasets/pascal_voc.py class pascal_voc(imdb): def __init__(self, image_set, year, devkit_path=None): imdb.__init__(self, 'voc_' + year + '_' + image_set) self._year = year self._image_set = image_set self._devkit_path = self._get_default_path() if devkit_path is None \\ else devkit_path self._data_path = os.path.join(self._devkit_path, 'VOC' + self._year) self._classes = ('__background__', # always index 0 '你的标签1','你的标签2',你的标签3','你的标签4' ) 修改./lib/datasets/imdb.py 在语句assert (boxes[:, 2] &gt;= boxes[:, 0]).all()前面添加： for b in range(len(boxes)): if boxes[b][2]&lt; boxes[b][0]: boxes[b][0] = 0 以避免出现AssertionError. 修改./lib/fast_rcnn/train.py和train_multi_gpu.py 在开头添加：import google.protobuf.text_format，以避免因protobuf版本问题出现的AttributeError: &#39;module&#39; object has no attribute &#39;text_format&#39;. 5.开始训练 Multi-GPU Training R-FCN cd py-R-FCN-multiGPU-coco python ./tools/train_net_multi_gpu.py --gpu 0,1 ... --solver models/pascal_voc/ResNet-50/rfcn_end2end/solver_ohem.prototxt ... --weights data/imagenet_models/ResNet-50-model.caffemodel ... --iters 110000 --cfg experiments/cfgs/rfcn_end2end_ohem.yml 6.测试训练好的模型 cd py-R-FCN-multiGPU-coco ./tools/demo_rfcn.py 此时如果报错： IndexError: index 4 is out of bounds for axis 1 with size 4 说明在demo_rfcn.py执行矩形框操作时出了问题，先找到语句cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]，将其修改为cls_boxes = boxes[:, 4:8]即可。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0","slug":"Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0","date":"2017-07-26T03:36:45.000Z","updated":"2018-10-04T08:08:58.022Z","comments":true,"path":"2017/07/26/Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0/","link":"","permalink":"http://gwang-cv.github.io/2017/07/26/Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0/","excerpt":"","text":"1.安装Ubuntu16.04 LTS x64 利用工具rufus制作USB系统盘(官方下载64位版本: ubuntu-16.04-desktop-amd64.iso). 语言选择English，安装开始：1.不选安装第三方软件；2.安装类型选择“其他选项（something else）”；3.设置分区，多硬盘挂载，如挂载到/data，/data2…；开始执行安装直到提示重新启动。 2.更新源 cd /etc/apt/ sudo cp sources.list sources.list.bak sudo gedit sources.list 在sources.list文件头部添加如下源： deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse 然后更新源和安装的包: sudo apt-get update sudo apt-get upgrade 常用软件安装： sudo apt-get install vim #编辑 sudo apt-get install htop #查看cpu和内存占用情况 sudo apt-get install python-pip 3.配置静态IP 首先查看本机的网卡名称 ifconfig 配置静态ip地址 sudo vim /etc/network/interfaces #在打开的interfaces文件中添加如下信息： auto eth0 #eth0对应你的网卡名称，在ifconfig中查看 iface eth0 inet static address 192.168.1.100 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameserver 114.114.114.114 配置DNS sudo vim /etc/resolv.conf #添加如下信息： nameserver 114.114.114.114 sudo vim /etc/resolvconf/resolv.conf.d/base #添加如下信息： nameserver 114.114.114.114 重启网卡服务 sudo /etc/init.d/networking restart #重启检验是否设置成功 sudo reboot 4.配置SSH和SFTP SSH安装命令： sudo apt-get install openssh-server ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22。 #若更改端口后请重启SSH服务： sudo /etc/init.d/ssh resart Ubuntu或Mac客户端可在命令行中执行如下语句来使用ssh： ssh username@192.168.1.100 sftp安装： sudo apt-get install openssh-sftp-server Ubuntu客户端可在文件管理器中选择“connect to server”，然后输入： sftp://192.168.1.100 即可查看到username所在的home文件夹下的内容。 5.安装NVIDIA显卡驱动 此处由于NVIDIA驱动和Ubuntu桌面冲突的问题（如循环卡在登录界面）。这里我们的VGA显示器默认接在主板的集显上，而不是接在NVIDIA显卡上，所以我们不采用ppa的显卡安装方式，而是采用独立的显卡驱动安装方式，关键之处在于不勾选OpenGL即可。 首先到NVIDIA官网下载官方驱动：http://www.nvidia.cn/Download/index.aspx?lang=cn，其中Titan XP属于GeForce 10 series系列。下载驱动：NVIDIA-Linux-x86_64-375.66.run 安装前准备： 卸载原有nvidia驱动，若采用的是apt-get安装方式 sudo apt-get purge nvidia* 或者采用--uninstall的方式卸载，按提示操作 sudo sh NVIDIA-Linux-x86_64-375.66.run --uninstall 禁用nouveau sudo vim /etc/modprobe.d/blacklist.conf 在打开的文件的最后加入nouveau黑名单，禁用第三方驱动 blacklist nouveau 然后执行 sudo update-initramfs -u 再执行如下语句，没有输出即说明已屏蔽成功 lsmod | grep nouveau 开始安装驱动 首先关闭X服务： sudo service lightdm stop 若在本机则要进入Ctrl-Alt+F1命令行界面 若在远程主机则在ssh中执行即可，前提是要关闭x服务。 开始： sudo apt-get install build-essential pkg-config xserver-xorg-dev linux-headers-`uname -r` sudo chmod a+x NVIDIA-Linux-x86_64-375.66.run sudo sh NVIDIA-Linux-x86_64-375.66.run -no-opengl-files sudo apt-get install mesa-common-dev sudo apt-get install freeglut3-dev sudo reboot 其中参数(后面两个参数不加): –no-opengl-files #只安装驱动文件，不安装OpenGL文件。这个参数最重要 –no-x-check #安装驱动时不检查X服务 –no-nouveau-check #安装驱动时不检查nouveau 若安装过程中报关于kernel-source的错误: ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 请务必执行如下语句： sudo apt-get install linux-headers-`uname -r` 若出现警告说： /sbin/ldconfig.real: /usr/lib32/nvidia-375/libEGL.so.1 is not a symbolic link 可能是由于libEGL.lib存在多个版本的冲突，解决方法： sudo mv /usr/lib/nvidia-375/libEGL.so.1 /usr/lib/nvidia-375/libEGL.so.1.org sudo mv /usr/lib32/nvidia-375/libEGL.so.1 /usr/lib32/nvidia-375/libEGL.so.1.org sudo ln -s /usr/lib/nvidia-375/libEGL.so.375.66 /usr/lib/nvidia-375/libEGL.so.1 sudo ln -s /usr/lib32/nvidia-375/libEGL.so.375.66 /usr/lib32/nvidia-375/libEGL.so.1 重启后若还是循环卡在登录界面，则要卸载到驱动，重新安装，在安装过程中务必不安装驱动提示的x-config的选项：12sudo sh NVIDIA-Linux-x86_64-375.66.run -no-opengl-files –no-x-check -no-nouveau-check #注意字符横线‘-’容易出错！ 如果出现无法进入桌面的问题，这是因为驱动修改了xorg的配置，可执行一下命令： cd /usr/share/X11/xorg.conf.d/ sudo mv nvidia-drm-outputclass.conf nvidia-drm-outputclass.conf.bak 若进入到界面后发现分辨率问题：启动到界面之后发现分辨率只有600x480，而显示器适合1920x1080，采用xrandr并修改xorg.conf来解决： sudo gedit /etc/X11/xorg.conf 修改如下： HorizSync 31.0 - 84.0 VertRefresh 56.0-77.0 即最终的xorg.conf文件部分内容为： Section \"Device\" Identifier \"Configured Video Device\" EndSection Section \"Monitor\" Identifier \"Configured Monitor\" Horizsync 30-84 Vertrefresh 56-77 EndSection Section \"Screen\" Identifier \"Default Screen\" Monitor \"Configured Monitor\" Device \"Configured Video Device\" SubSection \"Display\" Modes \"1920x1080\" \"1360x768\" \"1024x768\" \"1152x864\" EndSubSection EndSection 或者采用cvt xrand方法修改分辨率： cvt 1920 1080 # 1920x1080 59.96 Hz (CVT 2.07M9) hsync: 67.16 kHz; 173.00 MHZ # Modeline \"1920x1080_60.00\" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync xrandr --newmode \"1920x1080_60.00\" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync xrandr -q #查看VGA # Sceen 0: minimum 320 x 200 ..... # VGA-1 connected .... xrandr --addmode VGA-1 \"1920x1080_60.00\" xrandr --output VGA-1 --mode \"1920x1080_60.00\" 6.安装CUDA8.0 到官网下载cuda_8.0.61_linux.run，复制到根目录下。 sudo sh cuda_8.0.61_linux.run --tmpdir=/tmp/ 遇到问题：incomplete installation，然后执行 sudo apt-get install libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev sudo sh cuda_8.0.61_linux.run -silent -driver 注：此时安装过程中提示是否要安装NVIDIA驱动时选择no。其他选择yes或默认即可。 Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26? (y)es/(n)o/(q)uit: n 安装完毕后声明环境变量： sudo vim ~/.bashrc 在.bashrc尾部添加如下内容： export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 测试下安装是否成功： 测试： nvidia-smi 输出： xx xx xx 15:20:34 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.66 Driver Version: 375.66 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX TIT... Off | 0000:01:00.0 On | N/A | | 22% 48C P5 27W / 250W | 169MiB / 12205MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 2421 G /usr/lib/xorg/Xorg 105MiB | | 0 10062 G compiz 63MiB | +-----------------------------------------------------------------------------+ 7.安装OpenCV 3.2.0 从官网下载zip源代码，解压到根目录下。安装依赖： sudo apt-get -y remove ffmpeg x264 libx264-dev sudo apt-get -y install libopencv-dev build-essential checkinstall cmake pkg-config yasm libjpeg-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev libdc1394-22-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libv4l-dev python-dev python-numpy libtbb-dev libqt4-dev libgtk2.0-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils ffmpeg libgtk2.0-dev cd opencv-3.2.0 mkdir build cd build/ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON .. make -j32 sudo make install 安装成功后配置环境： sudo sh -c 'echo \"/usr/local/lib\" &gt; /etc/ld.so.conf.d/opencv.conf' sudo ldconfig 测试OpenCV安装是否成功： mkdir DisplayImage cd DisplayImage vim DisplayImage.cpp 添加代码： #include &lt;stdio.h&gt; #include &lt;opencv2/opencv.hpp&gt; using namespace cv; int main(int argc, char** argv) { if(argc!= 2) { printf(\"usage:DisplayImage.out &lt;Image_Path&gt;\\n\"); return -1; } Mat image; image= imread(argv[1], 1); if(!image.data) { printf(\"Noimage data\\n\"); return -1; } namedWindow(\"DisplayImage\",CV_WINDOW_AUTOSIZE); imshow(\"DisplayImage\",image); waitKey(0); return 0; } 创建CMake文件： vim CMakeLists.txt 添加内容： cmake_minimum_required(VERSION 2.8) project(DisplayImage) find_package(OpenCV REQUIRED) add_executable(DisplayImage DisplayImage.cpp) target_link_libraries(DisplayImage ${OpenCV_LIBS}) 编译： cmake . make 执行： ./DisplayImage lena.jpg 如果在make opencv-3.2过程中错误： fatal error: LAPACKE_H_PATH-NOTFOUND/lapacke.h: No such file or directory #include \"LAPACKE_H_PATH-NOTFOUND/lapacke.h\" 此时LAPACK和BLAS都已经安装了，解决方案： sudo apt-get install liblapacke-dev checkinstall 修改在build文件夹内的lapack.h文件，将如下语句 #include \"LAPACKE_H_PATH-NOTFOUND/lapacke.h\" 改为 #include \"lapacke.h\" 8.安装cudnn 5.0 从官网下载cudnn-8.0-linux-x64-v5.0.tgz for CUDA 8.0. 解压到当前目录： tar -zxvf cudnn-8.0-linux-x64-v5.0.tgz 解压后的文件如下： cuda/include/cudnn.h cuda/lib64/libcudnn.so cuda/lib64/libcudnn.so.5 cuda/lib64/libcudnn.so.5.0.5 cuda/lib64/libcudnn_static.a 然后执行： sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 9.BLAS安装与配置 BLAS（基础线性代数集合）是一个应用程序接口的标准。caffe官网上推荐了三种实现：ATLAS, MKL, OpenBLAS。其中ATLAS可以直接通过命令行安装。MKL是微软开发的商业工具包，面向科研和学生免费开放。申请学生版的Parallel Studio XE Cluster Edition，下载parallel_studio_xe_2017.tgz。注意接收邮件中的key(2HWS-34Z7S69B)。 tar zxvf parallel_studio_xe_2017.tgz #解压下载文件 chmod 777 parallel_studio_xe_2017 -R #获取文件权限 cd parallel_studio_xe_2017/ sudo ./install_GUI.sh 安装完成之后，进行相关文件的链接： sudo gedit /etc/ld.so.conf.d/intel_mkl.conf 添加库文件: /opt/intel/lib/intel64 /opt/intel/mkl/lib/intel64 编译链接使lib文件生效： sudo ldconfig 如果选择安装ATLAS，在终端输入sudo apt-get install libatlas-base-dev即可。 10.Py-Faster-RCNN配置 下载源码：包含caffe文件夹 git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 安装库文件： sudo apt-get install python-opencv sudo pip install cython easydict sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libboost-all-dev libhdf5-serial-dev libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler 安装依赖： sudo apt-get install -y build-essential cmake git pkg-config libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler libatlas-base-dev libgflags-dev libgoogle-glog-dev liblmdb-dev sudo apt-get install --no-install-recommends libboost-all-dev 安装Python接口依赖： sudo apt-get install python-tk sudo apt-get install python-dev sudo apt-get install -y python-pip sudo apt-get install -y python-dev sudo apt-get install -y python-numpy python-scipy sudo apt-get install -y python3-dev sudo apt-get install -y python3-numpy python3-scipy 在caffe的python文件夹内，使用root执行依赖项的检查与安装： sudo su cd caffe-fast-rcnn/python for req in $(cat requirements.txt); do pip install $req; done 修改Makefile文件 终端输入 cd py-faster-rcnn/caffe-fast-rcnn/ cp Makefile.config.example Makefile.config vim Makefile.config 使用python层 将 # WITH_PYTHON_LAYER := 1修改为 WITH_PYTHON_LAYER := 1 使用cudnn加速 将 # USE_CUDNN := 1修改为 USE_CUDNN := 1 保留 # CPU_ONLY := 1不变，使用GPU运行 如下两行对应内容修改为： INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/share/OpenCV/3rdparty/lib/ 在Makefile中配置： LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs hdf5的配置：官方说这对于Ubuntu 16.04是必须的；（libhdf5的版本号需要根据实际来修改） sudo find . -type f -exec sed -i -e 's^\"hdf5.h\"^\"hdf5/serial/hdf5.h\"^g' -e 's^\"hdf5_hl.h\"^\"hdf5/serial/hdf5_hl.h\"^g' '{}' \\; cd /usr/lib/x86_64-linux-gnu sudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so sudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so 编译Cython模块 cd py-faster-rcnn/lib/ make 编译caffe 由于当前版本的caffe中cudnn实现与系统所安装的cudnn的版本不一致会引起错误，rbgirshick的py-faster-rcnn其cudnn实现为旧版本的实现，所有出现了以上问题。 cudnn-7.0-linux-x64-v4.0-prod.tgz不会出现此问题 cudnn-7.5-linux-x64-v5.1.tgz会出现同样问题 cudnn-8.0-linux-x64-v5.1.tgz会出现同样问题 解决办法： 1将py-faster-rcnn/caffe-fast-rcnn/include/caffe/util/cudnn.hpp 换成最新版caffe里的相应目录下的cudnn.hpp； 2将py-faster-rcnn/caffe-fast-rcnn/include/caffe/layers/下所有cudnn开头的文件都替换为最新版caffe里相应目录下的同名文件； 3将py-faster-rcnn/caffe-fast-rcnn/src/caffe/layer下所有cudnn开头的文件都替换为最新版caffe里相应目录下的同名文件； 注：官方caffe源包caffe-master：https://github.com/BVLC/caffe 编译 cd py-faster-rcnn/caffe-fast-rcnn/ make clean #清除前一次编译结果 make -j32 编译pycaffe cd py-faster-rcnn/caffe-fast-rcnn/ make pycaffe 下载训练好的模型 终端输入 cd py-faster-rcnn/ ./data/scripts/fetch_faster_rcnn_models.sh faster-rcnn测试pascal_voc目标检测 cd py-faster-rcnn/ ./tools/demo.py 常见的报错Debug： 1.*AttributeError: &#39;module&#39; object has no attribute &#39;text_format&#39;* 需要在py-faster-rcnn/lib/fast_rcnn/train.py中添加： import google.protobuf.text_format 2.*KeyError: &#39;chair&#39; [when train only several classes]*使用py-faster-rcnn训练VOC2007数据集时遇到如下问题： File “/home/sai/py-faster-rcnn/tools/../lib/datasets/pascal_voc.py”, line 217, in _load_pascal_annotationcls = self._class_to_ind[obj.find(‘name’).text.lower().strip()]KeyError: ‘chair‘ 解决： You probably need to write some line of codes to ignore any objects with classes except the classes you are looking for when you are loading the annotation _load_pascal_annotation.Something like cls_objs = [obj for obj, clas in objs, self._classes if obj.find(‘name‘).text== clas] when you are loading the annotation in _load_pascal_annotation method, look for something like objs = diff_objs (or non_diff_objs) (after this line in pascal_voc.py) After that line insert something similar to below code cls_objs = [obj for obj in objs if obj.find('name').text in self._classes] objs = cls_objs 参考：https://github.com/rbgirshick/py-faster-rcnn/issues/316 3.Annotations files 标记文件问题 Note that: &lt;difficult&gt;0&lt;/difficult&gt; must be 0, if not, we will get error: ZeroDivisionError: integer division or modulo by zero 4.AssertionError: Selective search data not found at 训练时报错，可修改为：Change _C.TRAIN.PROPOSAL_METHOD = &#39;gt&#39; the 118 line in model/config.py file. It should be OK. 5.AttributeError: ‘module’ object has no attribute ‘text_format’ 在不采用预训练权重时，碰到错误pb2.text_format.Merge(f.read(), self.solver_param) AttributeError: &#39;module&#39; object has no attribute &#39;text_format&#39;原因是protobuf的版本问题，更换版本或者修改：在文件./lib/fast_rcnn/train.py增加一行import google.protobuf.text_format即可. 6.Ubuntu环境下python2和python3的切换 用 update-alternatives 1)建立链接： 12sudo update-alternatives --install /usr/bin/python python /usr/local/lib/python2.7 100sudo update-alternatives --install /usr/bin/python python /usr/local/lib/python3.2 150 2)sudo update-alternatives --config python按照提示选择默认python 3) 删除某个可选项：sudo update-alternatives --remove python /usr/bin/python2.7 7.网络更改 修改类别数:在train.prototxt中：input-data层的num_classes，为类别数+1 （1个背景类，下同）roi-data层的num_classes，为类别数+1cls_score层的num_output，为类别数+1bbox_pred层的num_output，为$(类别数+1)4$， 4表示一个bbox的4个坐标值在test.prototxt中修改anchor数:rpn_cls_prob_reshape层的第二个dim: $2anchor数量$（2表示bg/fg，背景和前景做二分类,下同）rpn_cls_score层的num_output: $2*anchor$数量同时，python代码中也要修改这个anchor数. 8...\\lib\\roi_data_layer\\layer.py&quot;, line 125, in setup top[idx].reshape(1, self._num_classes * 4) IndexError: Index out of range Do you provide a config file (eg. experiments/cfgs/faster_rcnn_end2end.yml)? Looks like cfg.TRAIN.HAS_RPN is false but it should be true! Please have a look at experiments/scripts/faster_rcnn_end2end.sh for details.keep __C.TRAIN.PROPOSAL_METHOD = &#39;gt&#39; __C.TEST.PROPOSAL_METHOD = &#39;gt&#39; same with the faster_rcnn_end2end.yml 9.loss_layer.cpp:25] Check failed: bottom[0]-&gt;num() == bottom[1]-&gt;num() (2 vs. 1) The data and label should have the same number. *** Check failure stack trace: *** The data blob had num = 2 so I set cfg.TRAIN.IMS_PER_BATCh to 1, and the problem is gone now.","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"Python除法","slug":"Python除法","date":"2017-07-06T03:20:16.000Z","updated":"2017-07-06T12:02:31.000Z","comments":true,"path":"2017/07/06/Python除法/","link":"","permalink":"http://gwang-cv.github.io/2017/07/06/Python除法/","excerpt":"","text":"python 2.x版本中存在两种除法运算: 即所谓的true除法和floor除法。 当使用x/y形式进行除法运算时，如果x和y都是整形，那么运算的会对结果进行截取，取运算的整数部分，比如2/3的运算结果是0； 如果x和y中有一个是浮点数，那么会进行所谓的true除法，比如2.0/3的结果是 0.66666666666666663。 另外一种除法是采用x//y的形式，那么这里采用的是所谓floor除法，即得到不大于结果的最大整数值，这个运算时与操作数无关的。比如2//3的结果是0，-2//3的结果是-1，-2.0//3的结果是-1.0。 在python 3.x中，x/y将只执行true除法，而与操作数无关；x//y则执行floor除法。 如果需要在2.x版本的python中进行这样的用法，则需要在代码最前加入from future import division的声明。 Python代码 from __future__ import division a=2/3 这时变量a的结果将是0.66666666666666663，而不是原来的0了。","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"python","slug":"python","permalink":"http://gwang-cv.github.io/tags/python/"}]},{"title":"Git配置出现Permission denied问题","slug":"Git配置出错Permission Denied","date":"2017-07-06T03:20:16.000Z","updated":"2017-07-06T03:21:47.000Z","comments":true,"path":"2017/07/06/Git配置出错Permission Denied/","link":"","permalink":"http://gwang-cv.github.io/2017/07/06/Git配置出错Permission Denied/","excerpt":"","text":"MacOS github后台配置ssh key之后本地无法git clone的问题 Permission denied (publickey). 当你在github后台添加了ssh keys之后，在本地这么测试一下： ssh -T git@github.com 如果返回是： Permission denied (publickey). 那么你可能要在本地ssh-add一下，当然在这之前你可以使用 ssh -vT git@github.com 查看一下到底是因为什么原因导致的失败。 ssh-add ~/.ssh/id_rsa (maybe: ssh-add ~/id_rsa) 然后会返回如下： Enter passphrase for /Users/xxx/.ssh/id_rsa: Identity added: /Users/xxx/.ssh/id_rsa (/Users/xxx/.ssh/youraccount_rsa) 之后再使用 ssh -T git@github.com 会返回成功： Hi youraccount! You've successfully authenticated, but GitHub does not provide shell access. 说明你目前本地的ssh已经切换到了id_rsa这个账号， 之后便可以进行git clone到本地的操作了！","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Ubuntu16.04+Titan X+CUDA8.0+cudnn5.1+Caffe","slug":"Ubuntu16.04+Titan X+CUDA8.0+cudnn5","date":"2016-12-30T03:15:04.000Z","updated":"2016-12-30T03:15:04.000Z","comments":true,"path":"2016/12/30/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/","link":"","permalink":"http://gwang-cv.github.io/2016/12/30/Ubuntu16.04+Titan X+CUDA8.0+cudnn5/","excerpt":"","text":"1.安装Ubuntu16.04 LTS x64 利用工具rufus制作USB系统盘(官方下载64位版本: ubuntu-16.04-desktop-amd64.iso)，因为已有Win7系统，此处选择“Install Ubuntu alongside Windows Boot Manager”，分区采用默认选择，语言选择English，安装完毕。 注：此时显示器VGA接口接到主板集成显卡接口上。PS: or always plug VGA to Nvidia Titan X, and then set “nomodeset” in /etc/default/grub, then install nvidia drivers in tty… 2.更新源 cd /etc/apt/ sudo cp sources.list sources.list.bak sudo gedit sources.list 在sources.list文件头部添加如下源： deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse 然后更新源和安装的包: sudo apt-get update sudo apt-get upgrade 3.安装NVIDIA显卡驱动 采用ppa安装方式，没选择最新的nvidia-370，我选择了nvidia-367。 Ctrl+Alt+F1进入tty命令控制台，停止lightdm，然后开始安装驱动。 sudo services lightdm stop sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-367 sudo apt-get install mesa-common-dev sudo apt-get install freeglut3-dev sudo reboot 将显示器VGA接口换到NVIDIA显卡上。 PS: If login loop, then Ctrl+Alt+F1, and then uninstall nvidia driver and reinstall again.. sudo apt-get purge nvidia-* sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-367 sudo apt-get install mesa-common-dev sudo apt-get install freeglut3-dev sudo reboot 4.修改分辨率 启动到界面之后发现分辨率只有1366x768，显示器适合1920x1080，采用xrandr并修改xorg.conf来解决。[或者，更容易的是采用一个HDMI的转接头来解决！] sudo gedit /etc/X11/xorg.conf 修改如下： HorizSync 31.0 - 84.0 VertRefresh 56.0-77.0 即最终的xorg.conf文件为： Section \"Device\" Identifier \"Configured Video Device\" EndSection Section \"Monitor\" Identifier \"Configured Monitor\" Horizsync 30-84 Vertrefresh 56-77 EndSection Section \"Screen\" Identifier \"Default Screen\" Monitor \"Configured Monitor\" Device \"Configured Video Device\" SubSection \"Display\" Modes \"1920x1080\" \"1360x768\" \"1024x768\" \"1152x864\" EndSubSection EndSection 注销系统再次登录后，选择适合的桌面分辨率即可。 5.安装CUDA8.0 到官网下载cuda_8.0.44_linux.run，复制到根目录下。 sudo sh cuda_8.0.44_linux.run --tmpdir=/tmp/ 遇到问题：incomplete installation，然后执行 sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev sudo sh cuda_8.0.44_linux.run -silent -driver 注：此时安装过程中提示是否要安装NVIDIA驱动时选择no。其他选择yes或默认即可。 Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.62? (y)es/(n)o/(q)uit: n 安装完毕后声明环境变量： sudo gedit ~/.bashrc 在.bashrc尾部添加如下内容： export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 测试下安装是否成功： 测试1： cd NVIDIA_CUDA-8.0_Samples/ nvidia-smi 输出： Tue Oct 18 15:20:34 2016 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 367.44 Driver Version: 367.44 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX TIT... Off | 0000:01:00.0 On | N/A | | 22% 48C P5 27W / 250W | 169MiB / 12205MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 2421 G /usr/lib/xorg/Xorg 105MiB | | 0 10062 G compiz 63MiB | +-----------------------------------------------------------------------------+ 测试2： cd 1_Utilities/deviceQuery make ........ ./deviceQuery 输出： ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"GeForce GTX TITAN X\" CUDA Driver Version / Runtime Version 8.0 / 8.0 CUDA Capability Major/Minor version number: 5.2 Total amount of global memory: 12205 MBytes (12798197760 bytes) (24) Multiprocessors, (128) CUDA Cores/MP: 3072 CUDA Cores GPU Max Clock rate: 1076 MHz (1.08 GHz) Memory Clock rate: 3505 Mhz Memory Bus Width: 384-bit L2 Cache Size: 3145728 bytes Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096) Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX TITAN X Result = PASS 测试3： cd ../../5_Simulations/nbody/ make ......... ./nbody -benchmark -numbodies=256000 -device=0 输出： mark -numbodies=256000 -device=0 Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=&lt;N&gt; (number of bodies (&gt;= 1) to run in simulation) -device=&lt;d&gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=&lt;i&gt; (where i=(number of CUDA devices &gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. &gt; Windowed mode &gt; Simulation data stored in video memory &gt; Single precision floating point simulation &gt; 1 Devices used for simulation gpuDeviceInit() CUDA Device [0]: \"GeForce GTX TITAN X &gt; Compute 5.2 CUDA device: [GeForce GTX TITAN X] number of bodies = 256000 256000 bodies, total time for 10 iterations: 3104.433 ms = 211.105 billion interactions per second = 4222.091 single-precision GFLOP/s at 20 flops per interaction 6.安装OpenCV 3.1.0 从官网下载zip源代码，解压到根目录下。安装依赖： sudo apt-get -y remove ffmpeg x264 libx264-dev sudo apt-get -y install libopencv-dev build-essential checkinstall cmake pkg-config yasm libjpeg-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev libdc1394-22-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libv4l-dev python-dev python-numpy libtbb-dev libqt4-dev libgtk2.0-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils ffmpeg libgtk2.0-dev cd opencv-3.1.0 mkdir build cd build/ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON .. make -j8 sudo make install 遇到的错误：Errors error: ‘NppiGraphcutState’ has not been declared error: ‘NppiGraphcutState’ does not name a type ... 解决方法：(由于CUDA版本高于8.0，所以需要做如下修改。在源文件中找到“graphcuts.cpp”) 将： #include \"precomp.hpp\" #if !defined (HAVE_CUDA) || defined (CUDA_DISABLER) 改为: #include \"precomp.hpp\" #if !defined (HAVE_CUDA) || defined (CUDA_DISABLER) || (CUDART_VERSION &gt;= 8000) because graphcuts is not supported directly with CUDA8 anymore. 安装成功后配置环境： sudo sh -c 'echo \"/usr/local/lib\" &gt; /etc/ld.so.conf.d/opencv.conf' sudo ldconfig 测试OpenCV安装是否成功： mkdir DisplayImage cd DisplayImage gedit DisplayImage.cpp 添加代码： #include &lt;stdio.h&gt; #include &lt;opencv2/opencv.hpp&gt; using namespace cv; int main(int argc, char** argv) { if(argc!= 2) { printf(\"usage:DisplayImage.out &lt;Image_Path&gt;\\n\"); return -1; } Mat image; image= imread(argv[1], 1); if(!image.data) { printf(\"Noimage data\\n\"); return -1; } namedWindow(\"DisplayImage\",CV_WINDOW_AUTOSIZE); imshow(\"DisplayImage\",image); waitKey(0); return 0; } 创建CMake文件： gedit CMakeLists.txt 添加内容： cmake_minimum_required(VERSION 2.8) project(DisplayImage) find_package(OpenCV REQUIRED) add_executable(DisplayImage DisplayImage.cpp) target_link_libraries(DisplayImage ${OpenCV_LIBS}) 编译： cmake . make 执行： ./DisplayImage lena.jpg 7.安装cudnn 5.1 从官网下载cudnn-8.0-linux-x64-v5.1.tgz for CUDA 8.0. 解压到当前目录： tar -zxvf cudnn-8.0-linux-x64-v5.1.tgz 解压后的文件如下： cuda/include/cudnn.h cuda/lib64/libcudnn.so cuda/lib64/libcudnn.so.5 cuda/lib64/libcudnn.so.5.1.5 cuda/lib64/libcudnn_static.a 然后执行： sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 8.安装MATLAB 2014a 需要注意的是Ubuntu16.04 LTS的gcc版本为5.4，而Matlab2014a支持的是gcc4.7。 降级安装gcc/g++版本为4.7.x 下载gcc/g++ 4.7.x sudo apt-get install -y gcc-4.7 sudo apt-get install -y g++-4.7 链接gcc/g++实现降级 cd /usr/bin sudo rm gcc sudo ln -s gcc-4.7 gcc sudo rm g++ sudo ln -s g++-4.7 g++ 升级 gcc 到 gcc-5版本 首先添加ppa到库： sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update 如果提示未安装，还需要先安装它的包： sudo apt-get install software-properties-common sudo apt-get upgrade sudo apt-get install gcc-5 g++-5 （非必须）现在可以考虑刷新一下，否则locate等命令是找不到的： sudo updatedb &amp;&amp; sudo ldconfig locate gcc 你会发现 gcc -v 显示出来的版本还是gcc-4.7的，因此需要更新一下链接： update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 53 \\ --slave /usr/bin/g++ g++ /usr/bin/g++-5 \\ --slave /usr/bin/gcc-ar gcc-ar /usr/bin/gcc-ar-5 \\ --slave /usr/bin/gcc-nm gcc-nm /usr/bin/gcc-nm-5 \\ --slave /usr/bin/gcc-ranlib gcc-ranlib /usr/bin/gcc-ranlib-5 ======================================================= 用Crack文件中的install替换matlab2014安装目录下/java/jar/下的install文件，然后执行install程序 cd \"MatlabFolder\" sudo ./install 注意：选择“不联网安装”；当出现密钥时，随意输入20个数字12345-67890-12345-67890即可；需要激活时选择不要联网激活，用Crack目录下的“license_405329_R2014a.lic”文件激活。 安装完成之后，将Crack/Linux目录下的libmwservices.so文件拷贝到/usr/local/MATLAB/R2014a/bin/glnxa64。 cd .. cd Crack/Linux/ sudo cp libmwservices.so /usr/local/MATLAB/R2014a/bin/glnxa64 打开Matlab并激活： cd /usr/local/MATLAB/R2014a/bin sudo ./matlab # sudo不可缺少，否则选择激活文件后报错 Test GPUdevice in Matlab: &gt;&gt; gpuDevice ans = CUDADevice with properties: Name: 'GeForce GTX TITAN X' Index: 1 ComputeCapability: '5.2' SupportsDouble: 1 DriverVersion: 8 ToolkitVersion: 5.5000 MaxThreadsPerBlock: 1024 MaxShmemPerBlock: 49152 MaxThreadBlockSize: [1024 1024 64] MaxGridSize: [2.1475e+09 65535 65535] SIMDWidth: 32 TotalMemory: 1.2796e+10 FreeMemory: 1.2475e+10 MultiprocessorCount: 24 ClockRateKHz: 1076000 ComputeMode: 'Default' GPUOverlapsTransfers: 1 KernelExecutionTimeout: 1 CanMapHostMemory: 1 DeviceSupported: 1 DeviceSelected: 1 9.Python 选用Ubuntu16.04默认的安装和配置，python版本2.7.12. 10.BLAS安装与配置 BLAS（基础线性代数集合）是一个应用程序接口的标准。caffe官网上推荐了三种实现：ATLAS, MKL, OpenBLAS。其中ATLAS可以直接通过命令行安装。MKL是微软开发的商业工具包，面向科研和学生免费开放。申请学生版的Parallel Studio XE Cluster Edition，下载parallel_studio_xe_2017.tgz。注意接收邮件中的序列号(2HWS-34Z7S69B)。 tar zxvf parallel_studio_xe_2017.tgz #解压下载文件 chmod 777 parallel_studio_xe_2017 -R #获取文件权限 cd parallel_studio_xe_2017/ sudo ./install_GUI.sh 安装完成之后，进行相关文件的链接： sudo gedit /etc/ld.so.conf.d/intel_mkl.conf 添加库文件: /opt/intel/lib/intel64 /opt/intel/mkl/lib/intel64 编译链接使lib文件生效： sudo ldconfig 如果选择安装ATLAS，在终端输入sudo apt-get install libatlas-base-dev即可。 11.Caffe的安装与配置 Caffe是由BVLC开发的一个深度学习框架，主要由贾扬清在UC Berkeley攻读PhD期间完成。参考官网上的教程以及Github上针对Ubuntu15.04和16.04的教程。从官方下载caffe源包caffe-master。 安装库文件： sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libboost-all-dev libhdf5-serial-dev libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler 安装依赖： sudo apt-get install -y build-essential cmake git pkg-config libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler libatlas-base-dev libgflags-dev libgoogle-glog-dev liblmdb-dev sudo apt-get install --no-install-recommends libboost-all-dev Python接口依赖： sudo apt-get install python-dev sudo apt-get install -y python-pip sudo apt-get install -y python-dev sudo apt-get install -y python-numpy python-scipy # (Python 2.7 development files) sudo apt-get install -y python3-dev sudo apt-get install -y python3-numpy python3-scipy # (Python 3.5 development files) 在python文件夹内，使用root执行依赖项的检查与安装： sudo su cd caffe-master/python for req in $(cat requirements.txt); do pip install $req; done Makefile.config： cd ~/caffe-master cp Makefile.config.example Makefile.config 配置如下： ## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers USE_OPENCV := 1 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA &lt; 6.0, comment the *_50 lines for compatibility. CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\ -gencode arch=compute_20,code=sm_21 \\ -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_50,code=compute_50 # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas BLAS := mkl # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. MATLAB_DIR := /usr/local/MATLAB/R2014a # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. PYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/local/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ # $(ANACONDA_HOME)/include/python2.7 \\ # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/share/OpenCV/3rdparty/lib/ # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 # N.B. both build and distribute dirs are cleared on `make clean` BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @ 在Makefile中配置： LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5 opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs hdf5的配置：官方说这对于Ubuntu 16.04是必须的。libhdf5的版本号需要根据实际来修改下。 sudo find . -type f -exec sed -i -e 's^\"hdf5.h\"^\"hdf5/serial/hdf5.h\"^g' -e 's^\"hdf5_hl.h\"^\"hdf5/serial/hdf5_hl.h\"^g' '{}' \\; cd /usr/lib/x86_64-linux-gnu sudo ln -s libhdf5_serial.so.10.1.0 libhdf5.so sudo ln -s libhdf5_serial_hl.so.10.0.2 libhdf5_hl.so 编译： cd ~/caffe-master make clean make all -j8 make test -j8 make runtest -j8 make pycaffe -j8 make matcaffe -j8 编译接口matcaffe时，有如下警告： Warning: You are using gcc version '5.4.0'. The version of gcc is not supported. The version currently supported with MEX is '4.7.x'. For a list of currently supported compilers see: http://www.mathworks.com/support/compilers/current_release. Warning: You are using gcc version '5.4.0-6ubuntu1~16.04.2)'. The version of gcc is not supported. The version currently supported with MEX is '4.7.x'. For a list of currently supported compilers see: http://www.mathworks.com/support/compilers/current_release. MEX completed successfully. 若OpenCV安装不正确则会在caffe编译过程中遇到如下错误： /usr/bin/ld: cannot find -lopencv_imgcodecs collect2: error: ld returned 1 exit status Makefile:566: recipe for target '.build_release/lib/libcaffe.so.1.0.0-rc3' failed make: *** [.build_release/lib/libcaffe.so.1.0.0-rc3] Error 1 MNIST测试： sh data/mnist/get_mnist.sh #数据预处理 sh examples/mnist/create_mnist.sh #重建lmdb文件。Caffe支持多种数据格式: Image(.jpg, .png等),leveldb,lmdb,HDF5. 生成mnist-train-lmdb 和 mnist-train-lmdb文件夹，这里包含了lmdb格式的数据集 sh examples/mnist/train_lenet.sh #训练mnist 输出： I1019 21:48:30.078994 20063 caffe.cpp:217] Using GPUs 0 I1019 21:48:30.092034 20063 caffe.cpp:222] GPU 0: GeForce GTX TITAN X ... .... ..... I1019 21:48:49.415398 20063 solver.cpp:317] Iteration 10000, loss = 0.00242468 I1019 21:48:49.415410 20063 solver.cpp:337] Iteration 10000, Testing net (#0) I1019 21:48:49.479605 20063 solver.cpp:404] Test net output #0: accuracy = 0.9914 I1019 21:48:49.479625 20063 solver.cpp:404] Test net output #1: loss = 0.0284448 (* 1 = 0.0284448 loss) I1019 21:48:49.479629 20063 solver.cpp:322] Optimization Done. I1019 21:48:49.479632 20063 caffe.cpp:254] Optimization Done. 12.Caffe下Matlab接口Demo测试 在使用Matlab运行caffe库时，即运行文件”caffe-master/matlab/demo/classification_demo.m”。遇到的错误信息如下： Invalid MEX-file 'caffe-master/matlab/+caffe/private/caffe_.mexa64': libcudart.so.8.0: cannot open shared object file: No such file or directory 错误原因是由于Matlab找不到caffe.mexa64所依赖的所有库文件的路径，此时可以使用ldd命令来查看caffe\\.mexa64内库文件的地址： //1. 在Ubuntu系统的命令终端 ldd *caffe_.mexa64 结果输出的是库文件对应的地址，与下文相对的缺失的库文件的地址可在此找到： libcudart.so.8.0 =&gt; /usr/local/cuda-8.0/lib64/libcudart.so.8.0 libcublas.so.8.0 =&gt; /usr/local/cuda-8.0/lib64/libcublas.so.8.0 libcurand.so.8.0 =&gt; /usr/local/cuda-8.0/lib64/libcurand.so.8.0 libcudnn.so.5 =&gt; /usr/local/cuda-8.0/lib64/libcudnn.so.5 //2. 在Matlab命令窗口输入 !ldd *caffe_.mexa64 结果在Matlab窗口的输出信息中发现： libcudart.so.8.0 =&gt; not found libcublas.so.8.0 =&gt; not found libcurand.so.8.0 =&gt; not found libcudnn.so.5 =&gt; not found 解决方法：通过如下命令将默认路径链接到真实路径下： sudo ln -s /usr/local/cuda-8.0/lib64/libcudart.so.8.0 /usr/local/MATLAB/R2014a/sys/os/glnxa64/libcudart.so.8.0 sudo ln -s /usr/local/cuda-8.0/lib64/libcublas.so.8.0 /usr/local/MATLAB/R2014a/sys/os/glnxa64/libcublas.so.8.0 sudo ln -s /usr/local/cuda-8.0/lib64/libcurand.so.8.0 /usr/local/MATLAB/R2014a/sys/os/glnxa64/libcurand.so.8.0 sudo ln -s /usr/local/cuda-8.0/lib64/libcudnn.so.5 /usr/local/MATLAB/R2014a/sys/os/glnxa64/libcudnn.so.5 重新启动Matlab使之生效。 另外，运行此例需要下载CaffeNet模型（Please download CaffeNet from Model Zoo before you run this demo.）https://github.com/BVLC/caffe/wiki/Model-Zoo || name: BVLC CaffeNet Model || caffemodel: bvlc_reference_caffenet.caffemodel || caffemodel_url: download || license: unrestricted 详细说明可参见”caffe-master/models/bvlc_reference_caffenet”… 参考： ubuntu14.04+cuda8.0（GTX1080）+caffe安装 深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0 深度学习框架torch/caffe/tensor/mxnet安装","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"","slug":"Ubuntu配置——Chrome XX-Net Sogou Nodejs Hexo","date":"2016-12-30T01:59:46.000Z","updated":"2016-12-30T01:59:46.000Z","comments":true,"path":"2016/12/30/Ubuntu配置——Chrome XX-Net Sogou Nodejs Hexo/","link":"","permalink":"http://gwang-cv.github.io/2016/12/30/Ubuntu配置——Chrome XX-Net Sogou Nodejs Hexo/","excerpt":"","text":"Ubuntu 配置——Chrome，XX-Net，Sogou，Nodejs，Hexo 1.安装Chrome sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/ 然后导入谷歌软件的公钥，用于下面步骤中对下载软件进行验证。命令将返回“OK”。 wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - 然后执行如下命令： sudo apt-get update sudo apt-get install google-chrome-stable 2.配置XX-Net 按照说明文档进行下载配置. 3.搜狗输入法 下载地址：http://pinyin.sogou.com/linux/ 安装： sudo dpkg -i sogoupinyin_2.1.0.0082_amd64.deb 若出现问题安装错误时，或是由于缺少依赖，因此可执行如下语句： sudo apt-get install -f 然后再次执行安装命令即可： sudo dpkg -i sogoupinyin_2.1.0.0082_amd64.deb 4.搭建hexo 4.1安装nodejs 一个是通过ubuntu自带的包管理进行安装。不过它自带的版本可能过低，所以需要添加源： sudo add-apt-repository ppa:chris-lea/node.js sudo apt-get update sudo apt-get install nodejs 创建一个nodejs到node的软链接: sudo ln -s /usr/bin/nodejs /usr/bin/node 4.2安装Git sudo apt-get install git 4.3安装hexo # 创建目录 mkdir hexo # 切换目录 cd hexo # 全局安装 Hexo，需要最高权限，记得输入root密码 sudo apt-get install npm sudo npm install -g hexo-cli # 初始化 Hexo hexo init 安装插件 npm install hexo-generator-index --save npm install hexo-generator-archive --save npm install hexo-generator-category --save npm install hexo-generator-tag --save npm install hexo-server --save npm install hexo-deployer-git --save npm install hexo-deployer-heroku --save npm install hexo-deployer-rsync --save npm install hexo-deployer-openshift --save npm install hexo-renderer-marked --save npm install hexo-renderer-stylus --save npm install hexo-generator-feed --save npm install hexo-generator-sitemap --save 测试安装成功 hexo server 4.4配置本机全局git环境首先请使用邮箱注册github账号，否则会影响下面操作，记住你注册的邮箱。 git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" 生成SSH秘钥 # -C后面跟住你在github的用户名邮箱，这样公钥才会被github认可 ssh-keygen -t rsa -C you@example.com # 回车后，输入一个文件夹名字，存储新的SSH 秘钥 /home/username/.ssh/id_rsa # 查看 公钥内容 稍后加入Github 账户的 sshkey中 less ~/.ssh/id_rsa.pub 将id_rsa.pub中的文本拷贝到github设置SSh。","categories":[],"tags":[]},{"title":"","slug":"Ubuntu backup","date":"2016-12-28T12:41:49.000Z","updated":"2016-12-28T12:41:49.000Z","comments":true,"path":"2016/12/28/Ubuntu backup/","link":"","permalink":"http://gwang-cv.github.io/2016/12/28/Ubuntu backup/","excerpt":"","text":"Ubuntu Backup backup 然后打开终端，输入以下命令： sudo su cd / ##转到根目录 然後，下面就是我用来备份我的系统的完整的命令： tar -cvpzf /media/gwang/Work/backup.tgz --exclude=/proc --exclude=/lost+found --exclude=/mnt --exclude=/sys --exclude=/media --exclude=/tmp / PS: tar 是用来备份的程序 c - 新建一个备份文档 v - 详细模式， tar程序将在屏幕上实时输出所有信息。 p - 保存权限，并应用到所有文件。 z - 采用‘gzip’压缩备份文件，以减小备份文件体积。 f - 说明备份文件存放的路径， /media/sda7/backup.tgz 是本例子中备份文件名。这个备份文件备份的位置是其它分区，也就是原来的WIN分区中。因为我的根目录的空间不足，所以只有备份在其它的地方了。 —excloude - 排除指定目录,使其不被备份 Linux 中美妙的事情之一就是在系统正在运行的情况下可以进行还原操作，而不需要启动光盘或者其他任何乱七八糟的东西。当然，如果您的系统已经崩溃，那您必须选择使用live CD，但是结果还是一样。 tar -xvpzf /media/gwang/Work/backup.tgz -C / 如果您使用的是bz2压缩的： tar -xvpjf /media/gwang/Work/backup.tar.bz2 -C / 如果系统已经崩溃可以使用Live usb登录，然后 mkdir /tmp/root mount /dev/sdaX /tmp/root tar -xvpjf /media/gwang/Work/backup.tar.bz2 -C /tmp/root 当然，恢复前可以先 rm -rf /tmp/root/* 这样就删除根目录下的所有文件. 这个只是在本机上还原，如果是还原到别的机子上记得修改fstab文件。（可能还需要安装grub） 恢复命令结束时，别忘了重新创建那些在备份时被排除在外的目录： # mkdir proc # mkdir lost+found # mkdir mnt # mkdir sys # mkdir media","categories":[],"tags":[]},{"title":"","slug":"数据挖掘界领军人物谢邦昌","date":"2016-11-29T10:08:05.000Z","updated":"2016-11-29T10:08:05.000Z","comments":true,"path":"2016/11/29/数据挖掘界领军人物谢邦昌/","link":"","permalink":"http://gwang-cv.github.io/2016/11/29/数据挖掘界领军人物谢邦昌/","excerpt":"","text":"好文丨数据挖掘界领军人物谢邦昌：深度剖析Data Mining 谢邦昌 深度剖析Data Mining 简介谢邦昌教授，是台北医学大学医务管理学系研究所暨大数据研究中心及管理学院主任，也是数据挖掘界领军人物及世界知名统计学家，他对数据挖掘的定义是：Data Mining是从巨大数据仓储中找出有用信息的一种过程与技术。 Data Mining主要功能 Data Mining实际应用功能可分为三大类六分项来说明：Classification和Clustering属于分类区隔类；Regression和Time-series属于推算预测类；Association和Sequence则属于序列规则类。 Classification是根据一些变量的数值做计算，再依照结果作分类。（计算的结果最后会被分类为几个少数的离散数值，例如将一组数据分为 “可能会响应” 或是 “可能不会响应” 两类）。 Classification常被用来处理如前所述之邮寄对象筛选的问题。我们会用一些根据历史经验已经分类好的数据来研究它们的特征，然后再根据这些特征对其他未经分类或是新的数据做预测。 这些我们用来寻找特征的已分类数据可能是来自我们的现有的客户数据，或是将一个完整数据库做部份取样，再经由实际的运作来测试；譬如利用一个大型邮寄对象数据库的部份取样来建立一个Classification Model，再利用这个Model来对数据库的其它数据或是新的数据作分类预测。 Clustering用在将数据分群，其目的在于将群间的差异找出来，同时也将群内成员的相似性找出来。Clustering与Classification不同的是，在分析前并不知道会以何种方式或根据来分类。所以必须要配合专业领域知识来解读这些分群的意义。 Regression是使用一系列的现有数值来预测一个连续数值的可能值。若将范围扩大亦可利用Logistic Regression来预测类别变量，特别在广泛运用现代分析技术如类神经网络或决策树理论等分析工具，推估预测的模式已不在止于传统线性的局限，在预测的功能上大大增加了选择工具的弹性与应用范围的广度。 Time-Series Forecasting与Regression功能类似，只是它是用现有的数值来预测未来的数值。两者最大差异在于Time-Series所分析的数值都与时间有关。Time-Series Forecasting的工具可以处理有关时间的一些特性，譬如时间的周期性、阶层性、季节性以及其它的一些特别因素（如过去与未来的关连性）。 Association是要找出在某一事件或是数据中会同时出现的东西。举例而言，如果A是某一事件的一种选择，则B也出现在该事件中的机率有多少。（例如：如果顾客买了火腿和柳橙汁，那么这个顾客同时也会买牛奶的机率是85%。） Sequence Discovery与Association关系很密切，所不同的是Sequence Discovery中事件的相关是以时间因素来作区隔（例如：如果A股票在某一天上涨12%，而且当天股市加权指数下降，则B股票在两天之内上涨的机率是 68%） 。 目前业界最常用的Data Mining分析工具 Data Mining工具市场大致可分为三类： 一般分析目的用的软件包：SAS Enterprise MinerMicrosoft SQL Server 2005 – 2008IBM Intelligent MinerUnica PRWSPSS ClementineSGI MineSetOracle DarwinAngoss KnowledgeSeekerStatistica 针对特定功能或产业而研发的软件：KD1（针对零售业）Options &amp; Choices（针对保险业）HNC（针对信用卡诈欺或呆帐侦测）Unica Model 1（针对营销业） 整合DSS（Decision Support Systems）/OLAP/Data Mining的大型分析系统：Cognos Scenario and Business Objects 对于刚刚接触Data Mining的人来说，怎样把它学好？ 先从问题着手，Domain Knowledge 是很重要的具体应重视三方面的问题： 强调需求，重视过程和结果。虽然统计学和数据挖掘一样，都是在寻求实际数据解决方案的过程中成长起来的，然而统计学家更关注模型，运用数据仅仅是为了发现新的模型，而数据挖掘则更强调知识的价值，模型是用来发现知识的工具。强调需求，重视过程和结果才能实现统计创新。 借鉴机器学习的特点，提炼方法，以算法的形式体现方法。统计学早已脱离正态的传统框架发展方法。但是，由于统计最新的可以被直接使用的成果太少，不仅阻碍了人们对统计方法的运用，甚至造成对先进统计方法的不甚了解。数据挖掘的兴起，为统计学与信息技术的结合带来了发展的契机。计算机技术将成为继数学之后，又一推动统计学发展的强大工具。 发挥统计软件的优势。许多“傻瓜”统计软件的设计，更适合统计学家研究使用，任何一个初通统计的数据分析员要想通过软件来进行数据分析，都极有可能由于对数据涵义的不求甚解，导致脱离实际的统计模型的滥用，数据挖掘软件也是如此；Clementine、SQL Server 2005及SAS和S-plus被设计为可以通过编程来调节软件的默认属性，用这样的软件工作可以增强统计研究者的算法意识；最后，统计软件为统计研究的目的，在图形和可视化方面的互动操作，应该在数据挖掘的软件中体现这一思想，因为它可以帮助数据分析员理解高维数据复杂的结构。 从数据挖掘在国际上的发展来看，数据挖掘的研究重点已从提出概念和发现方法，转向系统应用和方法创新上，研究注重多种发现策略和技术的集成，以及多种学科之间的相互渗透，数据挖掘技术迫切需要系统、科学的理论体系作为其发展的有力支撑。 最近，由经验统计方法和人工智能相结合而产生的衍生技术，如分类回归树（Classification And Regression Tree, 简称CART），卡方自动交互探测法（Chi-square Automatic Interaction Detector，简称CHAID）等前沿方法，以算法的形式展示了统计和信息技术结合发展的新方向。这些都预示着数据挖掘技术与统计学的集成已成为必然的趋势。 我们坚信，随着统计学与现代信息技术的融合，在方法上不断进行新的探索，一定会为统计学和数据挖掘未来的发展开辟一片新的天地。 Web Mining 和Data Mining的区别 如果将Web视为CRM的一个新的Channel，则Web Mining便可单纯看做Data Mining应用在网络数据的泛称。 该如何测量一个网站是否成功？哪些内容、优惠、广告是人气最旺的？主要访客是哪些人？什么原因吸引他们前来？如何从堆积如山之大量由网络所得数据中找出让网站运作更有效率的操作因素？以上种种皆属Web Mining 分析之范畴。 Web Mining 不仅只限于一般较为人所知的log file分析，除了计算网页浏览率以及访客人次外，举凡网络上的零售、财务服务、通讯服务、政府机关、医疗咨询、远距教学等等，只要由网络连结出的数据库够大够完整，所有Off-Line可进行的分析，Web Mining都可以做，甚或更可整合Off-Line及On-Line的数据库，实施更大规模的模型预测与推估，毕竟凭借因特网的便利性与渗透力再配合网络行为的可追踪性与高互动特质，一对一营销的理念是最有机会在网络世界里完全落实的。 整体而言，Web Mining具有以下特性 资料收集容易且不引人注意，所谓凡走过必留下痕迹，当访客进入网站后的一切浏览行为与历程都是可以立即被纪录的； 以交互式个人化服务为终极目标，除了因应不同访客呈现专属设计的网页之外，不同的访客也会有不同的服务； 可整合外部来源数据让分析功能发挥地更深更广，除了log file、cookies、会员填表数据、在线调查数据、在线交易数据等由网络直接取得的资源外，结合实体世界累积时间更久、范围更广的资源，将使分析的结果更准确也更深入。 利用Data Mining技术建立更深入的访客数据剖析，并赖以架构精准的预测模式，以期呈现真正智能型个人化的网络服务，是Web Mining努力的方向。 Data Warehousing（资料仓储） 和Data Mining 之间的关系若将Data Warehousing比喻作矿坑，Data Mining就是深入矿坑采矿的工作。毕竟Data Mining不是一种无中生有的魔术，也不是点石成金的炼金术，若没有够丰富完整的数据，是很难期待Data Mining能挖掘出什么有意义的信息的。 要将庞大的数据转换成为有用的信息，必须先有效率地收集信息。随着科技的进步，功能完善的数据库系统就成了最好的收集资料的工具。「数据仓储」，简单地说，就是搜集来自其它系统的有用数据，存放在一整合的储存区内。所以其实就是一个经过处理整合，且容量特别大的关系型数据库，用以储存决策支持系统（Design Support System）所需的数据，供决策支持或数据分析使用。从信息技术的角度来看，数据仓储的目标是在组织中，在正确的时间，将正确的数据交给正确的人。 许多人对于Data Warehousing和Data Mining时常混淆，不知如何分辨。其实，数据仓储是数据库技术的一个新主题，在数据科技日渐普及下，利用计算机系统帮助我们操作、计算和思考，让作业方式改变，决策方式也跟着改变。数据仓储本身是一个非常大的数据库，它储存着由组织作业数据库中整合而来的数据，特别是指从在线交易系统OLTP（On-Line Transactional Processing）所得来的数据。 将这些整合过的数据置放于数据仓储中，而公司的决策者则利用这些数据作决策；但是，这个转换及整合数据的过程，是建立一个数据仓储最大的挑战。因为将作业中的数据转换成有用的的策略性信息是整个数据仓储的重点。综上所述，数据仓储应该具有这些数据：整合性数据（integrated data）、详细和汇总性的数据(detailed and summarized data)、历史数据、解释数据的数据。 从数据仓储挖掘出对决策有用的信息与知识，是建立数据仓储与使用Data Mining的最大目的，两者的本质与过程是两码子事。 换句话说，数据仓储应先行建立完成，Data Mining才能有效率的进行，因为数据仓储本身所含数据是干净(不会有错误的数据参杂其中）、完备，且经过整合的。因此两者关系或许可解读为「 Data Mining是从巨大数据仓储中找出有用信息的一种过程与技术」。","categories":[],"tags":[]},{"title":"","slug":"初探计算机视觉的三个源头、兼谈人工智能｜正本清源","date":"2016-11-22T13:09:59.000Z","updated":"2016-11-22T13:09:59.000Z","comments":true,"path":"2016/11/22/初探计算机视觉的三个源头、兼谈人工智能｜正本清源/","link":"","permalink":"http://gwang-cv.github.io/2016/11/22/初探计算机视觉的三个源头、兼谈人工智能｜正本清源/","excerpt":"","text":"初探计算机视觉的三个源头、兼谈人工智能｜正本清源 2016-11-22 视觉求索 谈话人： 杨志宏 视觉求索公众号编辑 朱松纯 加州大学洛杉矶分校UCLA统计学和计算机科学教授 Song-Chun Zhu www.stat.ucla.edu/~sczhu 时间: 2016年10月 杨: 朱教授，你在计算机视觉领域耕耘20余年，获得很多奖项， 是很资深的研究人员。近年来你又涉足认知科学、机器人和人工智能。受 《视觉求索公众号》编辑部委托，我想与你探讨一下计算机视觉的起源，这个学科是什么时候创建的， 有哪些创始和代表人物。兼谈一下目前热门的人工智能。 朱: 好， 我们首先谈一下为什么需要讨论这个问题。 然后， 再来探讨一下计算机视觉的三个重要人物David Marr， King-Sun Fu， Ulf Grenander以及他们的学术思想。我认为他们是这个领域的主要创始人、或者叫有重要贡献的奠基人物。 第一节： 为什么要追溯计算机视觉的源头， 这有什么现实意义? 中国有句很有名的话：“一个民族如果忘记了历史,她也注定将失去未来。” 我认为这句话对一个学科来讲，同样发人深省。我们先来看看现实的状况吧。 首先，假设你当前是一个刚刚进入计算机视觉领域的研究生，很快你会有一种错觉，觉得这个领域好像就是5年前诞生的。 跟踪最新发表的视觉的论文，很少有文章能够引用到5年之前的文献，大部分文献只是2-3年前的，甚至是1年之内的。现在的信息交换比较快，大家都在比一些 Benchmarks,把结果挂到arXiv 网上发布。 很少有一些认真的讨论追溯到10年前，20年前， 或30年前的一些论文，提及当时的一些思想和框架性的东西。现在大家都用同样的方法，只是比拼，你昨天是18.3%的记录（错误率），我今天搞到17.9%了。大家都相当短视，那么研究生毕业以后变成了博士，可能也会带学生做研究，他只知道这几年的历史和流行的方法的话，怎么可能去传承这个学科，让其长期健康发展呢？特别是等当前这一波方法退潮之后，这批人就慢慢失去了根基和源创力。这是一个客观的现象。 其次，还有一个现象是，随着视觉与机器学习结合，再混合到人工智能的这么一个社会关注度很高的领域去以后，目前各种工业界，资本、投资界都往这里面来炒作。所以，你可以在互联网上看到各种推送的文字，什么这个大师，那个什么牛人、达人说得有声有色，一大堆封号。中国是有出“大师”的肥沃的土壤的，特别是在这个万众创新、浮躁的年代。 这些文字在混淆公众的视听。也有的是一些中国的研究人员、研究生， 半懂不懂，写出来一些， 某某梳理机器学习、神经网络和人工智能的历史大事。说得神乎其神。我的大学同学把这种帖子转发给我，让我担忧。 杨：这大多是以学术的名义写的软文，看起来像学术文章，实际上就是带广告性质的，一般都是说创投、创业公司里的人，带着资本的目的，带商业推广性质的。 朱: 我甚至不排除有些教授，比如与硅谷结合很紧密的、在IT公司或者风投公司兼职的，有意识地参与、引领这种炒作。 这对我们的年轻学生其实是很致命的，因为他们不了解这背后的动机， 缺乏免疫力。而且现在年轻人和公众都依赖短平快的社交媒体，很少去读专业文献。当公众的思想被这些文字占领了，得出错误的社会性的共识，变成了 false common sense， 对整个社会， 甚至对学术界，都会产生长久的负面冲击。 这就形成了新时代的皇帝的新装。我们需要对这种现象发声， 做一些严肃的探讨。所以，正本清源有着重要的现实意义。 第二节：计算机视觉和人工智能、机器学习的关系 杨：谈到这里，我想先问一下计算机视觉和人工智能是什么关系？还有机器学习这三个东西。 朱：人工智能是在60年代中后期起步的。一直到80年代，翻开它的教科书，就是一些启发式搜索，研究最多的是下棋， 从国际象棋一直到最近的围棋，都是比较抽象的表达。棋盘的位置是有限的、下棋的动作也是有限的， 没有感知和动作执行的不确定性。 所有的问题都变成一个图搜索的问题，教科书上甚至出现了一个通用图搜索算法号称可以解决任何人工智能问题。当时视觉问题还没引起大家重视。我这里有一份1966 年7月 的 MIT AI 实验室的第100号报告（备忘录memo 100），很短，题目叫做“The Summer Vision Project”。这个备忘录的基本意思就是暑假的时候找几个学生构造一个视觉系统。他们当时可能就觉得这个问题基本上是不需要做什么研究的。所以你就一个暑假，几个人一起写个程序，就把它干掉算了。现在说起来，当然是个笑话。 人的大脑皮层的活动， 大约70%是在处理视觉相关信息。视觉就相当于人脑的大门，其它如听觉、触觉、味觉那都是带宽较窄的通道。视觉相当于八车道的高速， 其它感觉是两旁的人行道。如果不能处理视觉信息的话，整个人工智能系统是个空架子，只能做符号推理，比如下棋、定理证明， 没法进入现实世界。所以你刚才问到的人工智能和计算机视觉的关系，视觉，它相当于说芝麻开门。大门就在这里面，这个门打不开, 就没法研究真实世界的人工智能。 到80年代，人工智能， 连带机器人研究就跌入了低谷， 所谓的冬天。那个时候，很多实验室都改名字了， 因为拿不到经费了。 客观来说，80年代， 一个微型计算机的它的内存只有640K字节，还不到一兆（1MB一百万字节），我们现在一张图像，随便就是几个兆的大小，它根本无法读入一张图像，还谈什么理解呢？等到我做博士论文的时候（1992-1996），我导师把当时哈佛机器人实验室最好的SUN工作站给我用，也就是32兆字节。我们实验室花了25万美元构建了一个图像采集系统，因为当时没有数字照相机。可以这么说，一直到90年代中期的时候，我们基本上不具备研究视觉这个问题的硬件条件和数据基础。只能用一些特征点的对应关系做射影几何，用一些线条做形状分析。因为图像做不了，所以80年代计算机视觉的研究，很大部分是做几何。 杨：90 年代后，就是数字照相机大量生产了。 朱：在90年代的末期的时候，发生了一个叫做感知器的革命。带动了大数据和机器学习的蓬勃发展。 杨：那机器学习与计算机视觉的关系呢？ 朱：计算机视觉是一个domain， 它有很多问题要研究， 就像物理学。 而机器学习基本是一个方法和工具，就像数学和统计学。 这个名词的兴起应该还是最近的事情， 在我看来，是来自于两股人马。 80年代人工智能走入低谷后，迎来了人工神经网络的一个高潮， 所谓的从符号主义到连接主义的过渡。在中国80年代与气功、人体科学一起走红，但这基本是昙花一现。到了90年代初， 退潮之后，就开始搞 NIPS这个会议， 引入统计的方法来做。还有一股就是做模式识别的一些工程人员EECS 背景的。 按道理来说， 这个领域应该叫做 统计学习 （Statistical Learning），因为它的方法都是由概率统计领域拿来的。这些人中的领军人物很有商业头脑， 把统计和物理的数理模型， 改名叫做机器， 比如模型（model）就叫机（machine），把一些层次模型（hierarchical model）说成是“网”（net）。这样，搞出了几个“机”和“网”之后， 这个领域就有了地盘。另一方面，我的那些做统计的同事们也都老实、图个清静，不与他们去争论， 也大多无力去争。当然，统计学领域也有不少人参与了机器学习的浪潮。简单说，机器学习中的 “机器”就是统计模型，“学习”就是用数据来拟合模型。 是由做计算机的人抢占了统计人的理论和方法，然后，应用到视觉、语音语言等 domains。 我在计算机和统计两个系当教授， 看得一清二楚。 这个问题我以后可以专门讨论。 这个机器学习的群体在2000年之后，加上大量数据的到来，很快就成长了， 商业上取得很大的成功。机器学习和计算机视觉大概有百分之六七十是重合的。顺便说一句，2019年我们两个领域会在一起在洛杉矶开CVPR 和 ICML年会， 我是CVPR19的大会主席。因为学习搞来搞去，最丰富的数据是在视觉（图像和视频）。现在这次机器学习的一些大的动作和工程上的推广工作，还是从计算机视觉这边开始的。 杨：谢谢你讲述人工智能,计算机视觉和机器学习的关系。下面我们回到本次访谈的主题。刚才说了这个感知器革命是90年代以后，出了很多的数据要处理了。那么为什么马尔（Marr）在70年代末思考的问题，在面对我们当今处理这个数据的时候, 还有意义？就是说马尔用了什么方法？什么思路框架？使它有生命力？ 朱：好，就回到1975-1980年这个时间段。我们今天的主题是想初步探讨一下计算机视觉的起源。我们这个领域也没有一个统一的教科书来谈这个事情。我认为视觉的起源，可以追溯到三个人，David Marr, King-Sun Fu 和Ulf Grenander。这三个人代表三个完全不同的方面，为计算机视觉这个领域奠定了基础。 杨：好， 我们逐个来介绍吧。 第三节：视觉的开创者之一：David Marr 的学术思想 朱： David Marr 【1945-1980】，中文音译为马尔， 他奠定了这个领域叫做Computational Vision计算视觉，这包含了两个领域： 一个就是计算机视觉（Computer Vision），一个是计算神经学（Computational Neuroscience）。他的工作对认知科学（CognitiveScience）也产生了很深远的影响。 我们计算机视觉CV，第一届国际会议ICCV 1987年就以David Marr的名字来命名最佳论文奖， 而且一直到2007年之前的20年间， 是CV唯一的奖项和最高的荣誉，两年一次。认知科学年会 （CogSci）也设有一个 Marr Prize给最佳的学生论文。这三个领域在80-90年代走得很近， 最近十多年交叉越来越少了。就是说，原来都是亲戚，表兄弟， 现在很少有人在之间走动了。 Marr 1972年从剑桥大学毕业，博士论文是从理论的角度研究大脑功能，具体来说，是研究的小脑， 主管运动的Cerebellum。1973年受MIT 人工智能实验室主任Minsky的邀请， 开始是做访问学者（博士后）。 1977年转为教职。 可是， 1978年冬诊断得了急性白血病。1980年转为正教授不久就去世了， 时年35岁。他在得知来日无多后，就赶紧整理了一本书，就叫 “Vision：A Computational Investigation into the HumanRepresentation and Processing of Visual Information”, 《视觉：从计算的视角研究人的视觉信息表达与处理》。他去世后由学生和同事修订，1982年出版。 杨：“Vision”2010年再版了，再版了以后在亚马逊仍然是卖得很好。 朱：它是个经典的东西。我是1989年冬天本科三年级从中科大认知科学实验室的老师那里，读到这本书的中文译本。因为缺乏背景知识，我当时基本读不懂。因为是中文，每句话都明白，但是一段话就不知道是什么意思了。在过去的20多年中， 我每隔1-2年都会再翻一翻这本书。后来我和同事花了大约8年时间，将他的一些思路转化成数理模型，比如primal sketch。 杨：这个人生故事是可以拍电影的。 朱：的确。 很多年前我与他的大弟子 Shimon Ullman饭桌上谈到这段历史， 他说当时大家到处找药，就是救不过来。当年这是一个30多岁正值科学顶峰的、交叉学科的领军人物。顺便说一句， 当年中日友好，1984播放日本电视剧《血疑》， 那是万人空巷， 感人至深。里面的大岛幸子（三口百惠饰）得的就是同样的病。 可惜， 目前计算机视觉这个领域，你如果去问学生的话，他们很多人都没听说过David Marr。“喔，想起来了，好像有个Marr奖吧”。可是你去问认知科学、神经科学的人，他们基本上对Marr非常的清楚。这也是我所担心的， 计算机视觉的发展太工程化、功利化了，逐步脱离了科学的范畴。这是短视和危险的。最近又受到机器学习来的冲击。 我这里顺便说一句， Marr 对我的另外一个间接的影响。他1973年来到MIT， 就租住在JayantShah的房子里， Shah 与 Minsky很熟， 他当时是研究代数几何（Algebraic geometry）的。 而我导师Mumford也是研究代数几何的， 并获得1974年的菲尔兹奖。他们两人很熟，后来在Shah的影响下，Mumford转入计算机视觉， 他们从提取物体边缘开始 （boundarydetection），也就是产生了著名的 Mumford-Shah 模型，搞图像处理的应用数学人员基本都是从这个模型开始做。这是后话。关于这段历史，我们以后可以展开谈。 杨：好， 那么 Marr的学术贡献是什么呢？ 朱：在我看来，David Marr对我们这个学科最主要的贡献有三条。从而基本上可以说，定义了这个学科的格局。 第一条，就是说在那个时代，60年代开始的时候大家已经很多人研究视觉神经生理学、心理学问题。也有人做一些边缘检测的工作。但是，视觉到底要解决哪些问题？是怎么实现的？大家莫衷一是，谈不清楚，那么David Marr的第一个贡献就是分出了三个层次。他说， 要解决这个问题，可以把它分成计算（其实应该说成是表达）、算法、和实现三层次。首先，在表达的层次，我们问一下这是个什么问题呢？如何把它写成一个数学问题。任务是什么？输出是什么？这是独立于解决问题的方法的。其次，对这个数学问题去求解时，可以选择不同的算法， 可以并行或者串行。再次，一个算法如何在硬件上实现，可以用CPU，DSP， 或者神经网络来实现。 很多观察到的心理学和神经科学的现象都是跟系统硬件有关的东西，比如说人的一些注意机制，记忆力。这些应该从表达层面剔除。这样， 视觉就可以从纯粹的理论、计算的角度来研究了。我们可以参考心理学和神经科学的结论， 但这不是主要的。 打个比方，要造飞机， 可以参考鸟类的结构， 但关键还是建立空气动力学，才能从根本上解释这个现象， 并创造各种飞行器， 走得更远。 杨：他这么一说，今天看来好像很自然的可以理解了，但是在当时，可能没有多少人，是把问题这样分解的。 朱：当时分不开。因为当时站在像神经科学和认知科学角度，是拿一些实验现象来说事，但是不知道这个现象是在哪一层出现的。 比如神经网络和目前的深度神经网络的学习，他们的模型（表达）、算法、和实现的结构三层 是混在一起的。就变成一个特用的计算设备， 算法就是由这个结构来实现的。当它性能不好的时候，到底是因为表达不对，还是算法不对，还是实现不对？ 这个不好分析了，目前的神经网络，或者是机器学习，深度学习，它的本源存在这个问题。 以前我们审稿的时候，会追问论文贡献是提出了一个新的模型？还是一个新的算法？在哪一个层级上你有贡献，必须说得清清楚楚。2012年，我作为国际计算机视觉和模式识别年会（CVPR）的大会主席， 就发生一个事件。收到神经网络和机器学习学派的一个领军人物 LeCun的抱怨信，他的论文报告了很好的实验结果， 但是审稿的三个人都认为论文说不清楚到底为什么有这个结果， 就拒稿。他一气之下就说再也不给CVPR投稿了，把审稿意见挂在网上以示抗议。2012 年是个转折点。 现在呢？随着深度学习的红火， 这三层就又混在一块去了。 一般论文直接就报告结果， 一堆表格、曲线图。我就是这么做，然后再这么做，我在某些个数据集上提高了两个百分点，那就行了。你审稿人也别问我这个东西里面有什么贡献，哪个节点代表是什么意思，你别问，我也不知道。那算法收敛了吗？是全局收敛还是一个局部收敛？我也不知道，但是我就提高了两个百分点。 杨：或者要用多少数据来训练材料才能够呢？ 朱：对，这个也不用管，而且说不清。反正我这个数据集就提高是吧？所以从这个角度来讲，它就很难是一个科学的方法。可以认为它就是一个工程或者是一个经验的，有点像中医。那么要往前再发展的时候，你必须要理清楚这三层的事情。 杨：对。 朱：那么他第二个贡献的话，是理清视觉到底要计算什么。Marr提出了一个系列的表达，从primal sketch（首要简约图）， 到2 ½ D sketch（深度简约图）， 到3D sketch。 这里面还包含了纹理、立体视觉、运动分析、表面形状、等等。比如说我要估计一个物体的深度和形状，我就估计它的光照，和物理材料特性；还有，三维几何形状怎么去表达？ 他试图去建立一个完整的体系。 现在的视觉就基本上被很多人错误地看成一个分类问题，你给我一张图像，我说这个图像里有一只狗或者没有狗，狗在哪儿都不知道。头在哪？脚在哪？不知道。Marr框架是有秩序的，现在的秩序在做深度学习的人眼中还不存在，或者没有忙过来。各人做各人的分类问题，比如说有人算这个动物分类，有的人算这个家具的分类。各种分类以后，他们之间怎么样的关系呢？要对这个图像或者场景要产生一个整体的语义解释。 第三个贡献，Marr提出了一个非常重要的概念，到现在一直还没有一个完整的解答。他说，计算视觉是一个计算的“过程”。这是什么意思？ 我们以前用贝叶斯方法（以及现在的深度网络）认为视觉就是表达成为一个后验概率，寻求一个最优解。这个解就是图像的解释。这个求解过程就会终止。可是Marr说的这个事情，它不是单纯去求一个解，而是一个连续不断的计算过程。我给你一张图像，你越看、越琢磨，你可能看到的东西会越多。 我给你一秒钟，你可能看到某些东西。我给你一分钟，你可能有另外一种理解，这两个理解可能是不一样的。还有一个重要的概念是你的任务决定了你怎么去看这个图像，比如说我在慌忙之中在做饭，那么我对这个场景，只看其中的很小一部分，足够来完成我的任务就行了。里面好多东西改变你根本没注意到。 杨：好像有些魔术就利用了这一点。 朱：就是， 很多心理学实验表明，你眼睛盯着这个图片看的时候，眼睛不眨，我告诉你这个图片在改变。你盯着看，结果它改了你都没看见。在让你看这个图片的时候，把你的注意力引到某个任务所需要计算的关键要素上，其它部分你就视而不见。视觉是受任务驱动的。而任务是时刻在改变之中。 比方说， 视觉求解不是打一个固定的靶子， 而是打一个运动目标。 杨：这听起来是一个耳目一新的概念。 朱：回到人工智能这个问题，视觉，它最后的用途，要给机器人用，机器人目前面临一个什么任务，来决定它要计算什么。这第三个贡献是在算法的层面。就是说我根据我们目前面临的任务，我才决定要计算什么。而且人的任务是在不断变化的，在此时此刻我任务都在变化，那么计算的过程中是没完没了地在改变。这个理念到目前，我们目前在研究这个事情，还没有完全实现。就是说，这将是人工智能和机器人视觉的一个关键。 杨：明白。 朱：我们现在很多人研究这个智能，比如说分类问题。他都是从谷歌的一些应用，比如搜索图片、广告投放，变成分类问题。 从而忽视了更大的本质问题。如果说人工智能往前发展机器人，要从机器人的角度来用视觉的话，那么它就有很多不同的任务。我现在做饭，我在打球，我在欣赏风景，这个时候我看到的东西是完全不一样的。我怎么样通过这千千万万的任务，而不是简单一个分类，来驱动我的计算的过程，来找到我的需求，来支持我目前的任务，这是一个巨大的研究的方向。David Marr的思想，到今天，反而意义非常重大，因为大家现在一窝蜂的去搞深度学习，把这些基本东西给忘掉了。但是这才是人工智能和机器人视觉的长远发展方向。 我前两年给过几个谈话，说研究视觉要从一个agent（执行者）的角度，带着任务进来的这么一个人或机器人，主动地去激发视觉。 目前的计算机视觉的研究还有一大部分是由视频监控的应用来驱动的，比如说我检测一些异常现象，看这个人是男还是女？那这也是一种被动的，就是说它只是在看，没有去做。要去做的话，就涉及到因果关系和更多的不确定性。所以现在的研究生觉得，他整天在做机器学习， 就在调参数，就在跟别人比拼百分之几的性能。 一些公司的研究所就报道， 他们在某某问题（数据集）上国际领先了，排名第一了。他们自己也觉得这个研究没多少意思。那是因为他们没有接触到这些基本的问题上来。 杨：他们可能还没有发现这个问题本身是多么有趣。 朱：因为作为一个科学来发展的话，那它就是要认认真真的来做，把这个理清楚。当前的火热来源于工业界， 工业界没有多少耐心资助他们的研究人员去做科学研究，大家很现实。 那么，David Marr先谈这么多好不好？以后我们可能还会继续深入谈的。 杨：好。那我们第二个人就谈一下傅京孫。 第四节：视觉的开创者之二：傅京孫（King-Sun Fu）的学术思想 朱： David Marr是从这个神经科学和脑科学这个方向来的。傅京孫【1930-1985】，他当时代表的是计算机科学，搞人工智能的人。他是一个有领导才能的人物。他和其他人于1973年组织了第一届国际模式识别会议（ICPR），并担任主席。会议后来演变成国际模式识别学会IAPR，在1976年成立，并被选为其主席。他重组了另外一个IEEE学会下面的模式识别委员会，并于1974年成为其第一任主席，创办了IEEE模式分析和机器智能（PAMI）会刊，并于1978年担任第一任总编。这是目前计算机视觉和相关领域最权威的一本期刊了。很多中国学生现在不知道，这个领域的老大本来是华人。目前， 国际模式识别学会IAPR设立了一个傅京孫奖， 作为终身成就奖， 是模式识别的最高荣誉。 杨：可惜他1985年去世了。听说去世前他每年都在中国举办讲座，并于1978年担任台湾的中央研究院院士。 朱：我正要说的这一点。他去世的时候55岁，在普渡大学，据说他的实验室是一个Chinatown。1978年中国打开国门，中国最早的一批中科院的计算机人员都到他那里进修，在普渡。所以他对中国计算机的发展，可以说是一个贡献非常巨大的人。我也是受到他的恩惠，我大学一二年级就开始跟着科大陈国良老师学习，他之前去普度进修。周末我有时就到陈老师家听他讲外面的一些研究人员和工作。你想想，计算机界那时候华人在美国站住脚的可能没几个人。 杨：对，他对中国计算机发展真的是有历史性的贡献的。我在科学院上研究生的时候，我们那些老师是说他过世太早了，要不然对中国的研究还会更好，他多活10来年就会好很多。 朱：他1985年拿到一个很大的国家项目，好像是开宴会的时候心脏病突发了。 他要是活着，华人在这个领域的话，不止是现在这个样子。不过在他之后， 稍晚一点我们有另外一个杰出华人，黄煦涛（Tom Huang）。他当时也在普渡任教，培养了大量华人研究人员。 我们以后会专门介绍。 杨：傅京孫的故事也可以拍电影。 朱：这是我们这个领域的不幸，两个奠基人很快就走了。他们刚刚把这个地基打起来，人就没了。 杨：那傅的主要贡献是什么呢？ 朱：傅京孫的贡献， 我也谈三点。第一个贡献应该就是对这个学科和学会的建设，以及工程师的培养上面，他起到了开创性的作用。一般公认他是模式识别的开山鼻祖，模式识别与计算机视觉分不开的。第二个作用，就是关于他的这个句法结构性的表达与计算，就是句法模式识别，Syntactic Pattern Recognition这个词，这个词其实非常深刻。他在走之前，他那个时候也没有多少数据，那么他只是画一些图，图表性的东西，来表达他的概念，他从计算机这边来的，你想很自然就会用到形式语言，因为计算机里面的几个基础之一是形式语言。逻辑、形式语言，对吧？ 杨：这好像是在编译原理里面学到过，因为编译的基础是形式语言。 朱：我们这个世界的模式， 一个最基本的组织原则是composition。一张图像就像语言、句子符合语法结构， 视频中的一个事件也有语法结构。寻找一个层次化、结构化的解释是计算视觉的核心问题。从傅京孫1985年丢下来这个摊子后，基本很少有人去碰。差不多18年以后，我和我第一个博士生继续做图像解译Image Parsing这个方向，于2003年得了Marr马尔奖。然后我和我导师专门于2006年写了一本小书，总结了图像的随机语法。我刚才谈到了，在做识别，做分类的时候，只是单独在分类某一个东西，怎么去把各个识别器和分类器给它整合在一起，变成一个统一的表达？就必须产生一个结构上的表达。现在机器学习界把它换了另外名字，叫做结构化的输出，其实是一个东西。他们提出一个新的名词，把原创的图像解译名称覆盖住，这事现在经常发生。所以我说机器学习领域经常到别人那里偷概念，改头换面。数学界不允许这样做的。我还是坚持把它叫做解译、语法。 因为语法，它就是一些规则，其实语法并不见得是一个确定性的，它可以跟统计连在一块，它也可以跟目前的一些神经网络结合，这个都没问题。它表达了一个骨架或者支柱，形成一个统一表达。 第三点，从算法的角度来讲，有一个层次化的表达以后，意义就不一样了，比如自底向上或自顶向下的计算的过程就可以在上面体现出来，就是马尔说的计算的过程，就可以在这里面体现出来。视觉的计算过程应该是由大量的自底向上（bottom-up）和 自顶向下（top-down）过程交互和同时进行的。顺便再说一句，当前的深度神经网络就是一个feedforward的自底向上的计算， 缺乏自顶向下的过程。而在人脑计算中，自顶向下的计算占据很大一部分。 杨：那就是说， 这个语法结构对计算过程有了规范和表达的途路。 朱：对，你的搜索的过程，这个计算的过程是什么？马尔他提出了第二个概念，说视觉是个计算的过程，那么这个计算过程你什么时候算哪个，这是个调度的问题，就像操作系统。那么David Marr计算的过程，没完没了的，随着你的任务不断改变，那么它就有一个调度的问题。所以说我现在要去做饭，或者我要欣赏风景，或者说我要去走路，开车，那么它的不同的任务产生了不同的进程。这个进程，要在层次化的表达里面的统一起来调度。从这个意义看，感知是计算一个解译图（parse graph）， 认知是对这个parse graph进一步推理扩大， 而机器人的任务规划（task planning）也是一个同样结构的parse graph， 那就更别说语言是用parse graph来表达的。所以，人工智能的一个核心表达就是随机的语法和解译图。 杨：对。 朱：这个是绕不掉的，不管谁来做，都要做这个事情。当然，现在有人千方百计想绕过去，重新发明一套名词， 让新来的学生忘记历史， 这样他们就可以变成社会公认的大师。有些教授、研究人员在学术上没什么原创贡献， 却在网上、社会上成了当红明星, 学科代言人。用社会上的知名度再给学术界施压。 总结一下，傅京孫三点主要贡献：一是学科的人才和组织基础，二是他提出这么一个的语法表达方法， 三是这个表达支撑了自底向上或自顶向下的计算的过程。他去世后， 这个方向一直处于一种休眠状态，我的研究有一条线是跟着这个方向做。2011年马里兰大学周少华他的导师有一个演讲，题目叫：语法模式识别—从傅到朱 （From Fu to Zhu）。我们在继承他的框架往前走。 杨：真好！那么咱们下面就谈第三个人Ulf Grenander。 朱：这个人的话，知道的人非常少。 杨：我翻看了网上资料，他是这个领域里头真正的是大神了，但绝对是个小众人物。 第五节：视觉的开创者之三：Ulf Grenander的学术思想 朱：Ulf Grenander 【1923-2016】是很少有人知道的。感觉有点像金庸小说《天龙八部》里的在藏经阁扫地的灰衣老僧。武功和思想都出神入化，但是，他基本是世外高人，不参与江湖争斗， 金庸也没有交代他的名字。所以江湖上的人大多没听说过他。 这样也好， 他自自在在活了93岁， 今年刚刚去世的。国际应用数学季刊邀请我和其他人写纪念文章，正准备出版专刊呢。 杨：对，我读他的生平，他这个人简直就是把欧洲美洲的，还有俄国的所有的精华的人物都接触过。 朱：那是，他出身在瑞典，他的导师叫Harald Cramér。概率论里面的一个重要的定理，还有数论里的一个猜想是用他命名的。然后，他也跟 Bohr（波尔），Kolmogorov（科尔莫戈罗夫）他们走得比较近。他的起点就是做概率统计， 时间序列， 随机过程，因为你现在想概率论和统计学的一些重要应用，就是那个时候发力了。 杨：从保险业开始了，北欧那边因为航海，保险业非常发达，所以这也有点道理。 朱：关于概率和统计学对于科学、视觉、以及人工智能的重要意义， Mumford 1999年写了一篇论文，是在一个大会的发言，叫做《随机性时代的曙光》（Dawning of the Age of Stochasticity）。 杨：对，那是你们老师写的， 网上能找到。 朱：他总结说，过去两千多年的西方科学的发展是建立在亚里士多德以来的数理逻辑基础之上的。但是，后面一千年包括人工智能、人的思维这些东西是随机性过程。人的思维应该是建立在概率推理基础之上。其实， 我们看到现在的机器学习， 人工智能完全就是从这个方向走了。 杨：你的导师说，整个世界的数学可以用概率的这套思想重新写一遍，就像罗素和怀特海的写这个数学原理似的，可以把数学重新建立起来，用概率的这种思想。 朱：这个工作已经有人做了。E. T. Jaynes就是发明最大熵原理的那个人，他写了一本很厚的书，《Probability Theory: The Logic of Science》， 他就是用这个原理去写。这也是一篇遗作。他没写完就过世了。这也是以后可以谈的话题。 朱： Ulf Grenander就诞生在这么一个概率发源的中心的地带，跟几个大师学习，博士毕业后出来游历，做概率论随机过程的这些东西。到六、七十年代的时候，他就开始提出来，想用数学来把这个模式识别与智能的现象的问题定义清楚。我们前面谈到的David Marr 是从神经科学、认知科学来的。傅京孫是一个计算机科学与工程的人。这两者基本没有多少严格的数学定义，提出的框架是漂浮的。Ulf是从数学的角度，奠定基础。他提出来一个应用数学的分支， 叫做 Pattern Theory。他的出发点完全不同， 就是要给世界上的各种模式、现象， 建立一个数学的框架来研究。 格局就很宏伟。而不是急于去解决某种实际问题， 后者叫做模式识别 （pattern recognition）。 他在90岁高龄出版了最后一本书， 想用数学来研究人的思想是从哪里来的。 你看我们脑袋里的念头、主意也往往是随机产生，像冒泡一样， 所谓思如泉涌。到底怎么来的？ 杨：那太了不起了。这个事说起来，我想到当时我的老师是让我读Geman and Geman 1984年的吉布斯采样算法，那就已经了不起了。 朱：Grenander最后落脚在布朗大学应用数学系，Geman是他当年（70年代末80年代初）招到组里的年轻教员之一。这个吉布斯采样（Gibbs Sampler）的算法是一个里程碑的东西，在80年代初引起轰动。但那只是这个学派的诸多贡献的一个片段。 Grenander的理论解释起来的确有点费劲，既然谈历史，我先从我个人的经历谈一下。 他1994年出了一部总结性的书，900多页，叫做《General Pattern Theory》，广义模式理论。有点爱因斯坦做广义相对论的意思。但这本书很抽象， 没多少人读。我1995年在哈佛研究纹理模型（texture models），因为我用的学习算法就是吉布斯采样，在训练的时候，跑一遍要等两个星期才收敛，机器被占了，我就有时间，也是耐着性子把这本书读完了。我估计世界上不超过20人，能有耐心完整地读他的书。然后，我1996年1月答辩论文，我导师和我每周开车去布朗大学参加讨论。波士顿的冬天很冷， 哈佛到布朗1个小时左右，漫天大雪， 我们有时在高速上车被陷住， 下来铲雪。到了6月， 我导师从哈佛提前退休，带着我一起加入布朗的应用数学系。那在当时是一个学术思想的中心。组会里有Grenander，Mumford， Geman 还有其他20来人， 一坐就是2个多小时。这些人都明察秋毫， 做报告的人无法含混过去的， 一步一步都必须理清楚，说不清楚你就下去想， 下次再来。 我一直认为计算机视觉和模式识别领域亏欠Grenander, 因为统计建模和随机计算逐渐成为我们领域的核心理论基础，而大家并不知道，很多思想、算法都源于这个人或者他的学派。所以，2012年， 我主持CVPR（国际计算机视觉和模式识别）大会， 特意放到布朗大学附近召开，我和另外两个主席一说，大家立即就同意了。并特制了一个银质的大奖章， 在大会上颁给他，表达我们的敬意。这里发生很多故事，我们以后再谈吧。 杨：那你能简短总结一下Grenander对计算机视觉、甚至人工智能的主要贡献吗。 朱：还是谈三点主要的吧。 首先，他提出了一个思想， 叫做 analysis-by-synthesis， 这是所谓 产生式建模的核心理念。当你要去识别、分析一个模式，比如一个动物，人脸， 一个事件， 你首先要建立一个数理模型， 这个模型通过数据来拟合， 也就是当前的机器学习。 那么， 判断这个模型好坏， 或者模型是否充分，的一个依据是什么呢？产生式建模的方法就是对这个模型随机抽样，也就是，合成（synthesis）。 我把这个过程直观叫做“计算机之梦”。计算机模型一开始初始化为空（完全随机）， 那它做的梦就是白噪声， 或者一张白纸。通俗来说， 这个模型就是一个“白痴”。人脑有这个功能，我们把眼睛一闭，没有外界输入了，就能做梦， 白日梦就是想象力的体现。一个好的模型采样产生的图片（模式）， 与真实观察的图片（模式）， 就应该是真假难辨。如果你能分辨，那说明这个模型不到位。 现在很多机器学习的方法是没法去随机合成图片的。 举个例子来说，我要检验你是不是真的听懂和理解中文，就看你能不能说流利的中文。如果你说话语法有错，词汇量不够，或者有口音，那就揭示你在哪方面还需要提高。 杨：这个要求好像比光是听懂 要更严格。 朱：的确。我们当年考英语， 多半是读，说和写都不行。我们考TOEFL， GRE Verbal的时候， 就算没搞懂， 也能蒙个60%-70%。 新东方的题海战术也很奏效。当你做了大量考题， 就算不懂， 也能考好。当前大数据、机器学习就用题海战术。 这个方法强调在实战中检验，考什么就拼命复习什么，不考的东西就不学，这也很有道理，很直接， 来得快。 但是， 因为你的模型没有真正理解， 没有“真懂”，考试大纲外面的东西更不懂， 那么后遗症就是， 遇到新考题， 缺乏泛化能力，遇到新问题，缺乏创造力。 想一想， 如果我的学生一步步考试都是靠题海战术这么学过来的， 那多可怕，要让他们去搞研究、创新，那就基本不可能。很遗憾的是，现在中国学生从幼儿园开始，就是在题海中泡大的。机器人、人工智能，靠题海战术是可以演示不少功能的， 但是， 那还离真正的智能比较遥远。 杨：好， 我明白这个analysis-by-synthesis 的意义了。他的第二贡献呢？ 朱：他提出了一整套建模的理论和方法。把代数、几何、概率整合起来。 代数指的是一些结构，比如群论， 记得在科大本科我学过 群、环、域这些概念吧？也就是说我有一些基本元素，叫 generator，连接成为图graph，然后是群group，在上面进行操作, 产生了各种各样的变化。还有很多几何， 变换， 在连续情况就产生形变。通过组合，语法、产生丰富的图模式。然后，再在这个图模式的空间上定义距离（测度）和概率。 朱：比如一个概率模型， 是定义在一个什么样的结构上，它是个什么样的解空间？这个数理上你必须交代清楚，否则你的论文写不下去了。现在它的一个很大的应用在医疗图像上面，比如说一个病人，他的肝变形了，那么他的肝的形状和正常人的肝的形状之间怎么定义一个合理的距离？两张人脸，怎么定义这个距离的呢？这个距离定义在一个流型上，数学的流型（manifold）。 杨：这些东西真用上了吗？ 朱：他有个Postdoc，名叫Michael Miller， 现在是Johns Hopkins 大学图像中心主任， 就用这一套方法来做医疗图像、脑科学（Brain Mapping）等方面的应用。 杨：他的第三方面的贡献呢？ 朱：第三个方面主要是算法上面。当我们去做求解的时候，在一个解空间，这个求解空间肯定是一个非凸的，他有千千万万的局部最优解local minimum 在里面。 杨：对。这是当时八十年代的时候提出来一个很尖锐的问题，好像有什么模拟煺火方法。 朱：很多蒙特卡洛算法都是他和这个学派的人提出来的。这个解空间是一个异构空间，空间里面非常复杂的，包含有很多子空间，子空间里面又包含又子空间，每个子空间维度又不一样，他们之间，从一个解跳到另外一个解的时候，这跳转必须是可逆的。在计算机里面就叫可以回溯。从这个学派走出来的人，他们设计算法每一个步骤都是有章法的，要做到合规合矩。包括上面提到的吉布斯采样算法、可逆蒙特卡洛跳转法，还有变分法（variational methods）和偏微分方程式， 还有一些随机下降法（stochastic gradient）， 这后者是目前训练深度学习模型的主要办法。他也开创了非参数模型的学习方法。这里面东西太多，先谈到这里吧。 正因为很多人没有接触过Grenander的理论， 缺乏这方面的理论素养， 造成我们学科发展的一个巨大的问题：很多教授、博士、研究生就是用别人的模型（机），拿来调试，基本缺乏自己发明新模型、新算法的能力。我们这个领域，很多美国名牌大学助理教授、副教授、教授， 他们的论文中的公式错误百出。现在干脆大家在论文中都不写公式了， 直接报告最后的实验结果，提高了几个百分点。这就“一俊掩百丑”了。 英文有个类似的说法叫做 “sweep the dirt under the carpet把污垢扫到地毯下”。 这些人在大量培养博士、他们出来的人评审论文。 这样一来，学科的发展堪忧！ 第六节：结束语 杨：听了你番谈话，我明白很多。记得我当时念研究生，包括念博士生的时候，实际上是很糊涂的。就是对这个领域到底做多少东西，没有信心。觉得很多研究像画鬼一样，原理不清楚。我觉得那样的话，与其那样做事情, 那不如干脆到工业界那更快乐。 朱：正因为我们这个领域很多历史、框架性的东西，没有搞清楚，培养出来的博士，缺乏分析能力。大家被一些工程的任务和数据驱动，被一些性能的指标牵制，对科学的发展比较迷茫。 杨：好， 谈了很多， 我们做个总结吧。 朱：那我就说两点。 首先， 我在开场白中提到 “一个民族如果忘记了历史, 她也注定将失去未来。”一个学科要健康发展，需要研究人员、研究生们理解自己领域的历史和大的发展方向，建立文化的认同。否则，自己家的东西，被别人偷取，浑然不知。就像日本打入中国，想把我们的地名改掉，大家开始说日语，把名字都改做山本太郎之类，感觉很酷吗？ 或者是韩国人把中国的文化拿去申报世界文化遗产，这都是要制止的。否则，过了一代人，还真说不清楚了。我记得刚来美国的时候，美国同事把汉字叫做“Kang-ji”，说是日本字。 我们领域很多人对保护这个领域的文化和传统缺乏清醒认识。皮之不存，毛将焉附？ 其次，一个学科内部，大家互相不够了解，各自为政。特别现在会议审稿人很多是研究生，以自己的狭窄的眼光和标准去评判别人的方法，造成很多混乱。搞工程的看不到理论的重要性，反之亦然。大家又都疏远心理学和认知科学的研究。我提倡我们的研究人员、学生要提高理论修养、培养长远眼光，向相关学科取经，取长补短。 我希望这个微信公众号，能够帮助大家正视问题，让计算机视觉这个领域健康、稳健、可持续地发展。","categories":[],"tags":[]},{"title":"","slug":"文本挖掘","date":"2016-11-22T13:04:33.000Z","updated":"2016-11-22T13:04:33.000Z","comments":true,"path":"2016/11/22/文本挖掘/","link":"","permalink":"http://gwang-cv.github.io/2016/11/22/文本挖掘/","excerpt":"","text":"文本挖掘 1.背景 随着互联网的大规模普及和企业信息化程度的提高,文本信息的快速积累使公司、政府和科研机构在信息处理和使用中面临前所未有的挑战。一方面,互联网和企业信息系统每天都不断产生大量文本数据,这些文本资源中蕴含着许多有价值的信息;而另一方面因为技术手段的落后,从大量数据资源中获取需要的信息十分困难。人们迫切需要研究出方便有效的工具去从大规模文本信息资源中提取符合需要的简洁、精炼、可理解的知识,文本挖掘就是为解决这个问题而产生的研究方向。 传统的自然语言理解是对文本进行较低层次的理解,主要进行基于词、语法和语义信息的分析,并通过词在句子中出现的次序发现有意义的信息。在这一层次遇到的问题多与句法和语义歧义性相关。对文本较高层次的理解主要集中在研究如何从各种形式的文本和文本集中抽取隐含的模式和知识。文本高层次理解的对象可以是仅包含简单句子的单个文本也可以是多个文本组成的文本集,但是现有的技术手段虽然基本上解决了单个句子的分析问题,但是还很难覆盖所有的语言现象,特别是对整个段落或篇章的理解还无从下手。 在19世纪早期发展起来的以统计技术为基础的数据挖掘技术已经发展的较为成熟,并在大规模结构化关系数据库上应用取得成功。将数据挖掘的成果用于分析以自然语言描述的文本,这种方法被称为文本挖掘(Text Mining, TM)或文本知识发现(Knowledge Discovery in Text, KDT)。与传统自然语言处理(Natural Lnaguage Proeessing, NLP)关注词语和句子的理解不同,文本挖掘的主要目标是在大规模文本集中发现隐藏的有意义的知识,即对文本集的理解和文本间关系的理解。因此,文本挖掘是自然语言处理和数据挖掘技术发展到一定阶段的产物。 在现实世界中,可获取的大部信息是以文本形式存储在文本数据库中的,由来自各种数据源的大量文档组成,如新闻文档、研究论文、书籍、数字图书馆、电子邮件和Web页面。由于电子形式的文本信息飞速增涨,文本挖掘已经成为信息领域的研究热点。 2.定义 文本数据库中存储的数据可能是高度非结构化的,如Web网页;也可能是半结构化的,如Email消息和一些XML网页;而其它的则可能是良结构化的。良结构化文本数据的典型代表是图书馆数据库中的文档,这些文档可能包含结构字段,如标题、作者、出版日期、长度、分类等等,也可能包含大量非结构化文本成分,如摘要和内容。通常,具有较好结构的文本数据库可以使用关系数据库系统实现,而对非结构化的文本成分需要采用特殊的处理方法对其进行转化。 文本挖掘是一个交叉的研究领域,它涉及到数据挖掘、信息检索、自然语言处理、机器学习等多个领域的内容,不同的研究者从各自的研究领域出发,对文本挖掘的含义有不同的理解,不同应用目的文本挖掘项目也各有其侧重点。因此,对文本挖掘的定义也有多种,其中被普遍认可的文本挖掘定义如下: 定义: 文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程,同时运用这些知识更好地组织信息以便将来参考。 直观的说,当数据挖掘的对象完全由文本这种数据类型组成时,这个过程就称为文本挖掘。 文本挖掘也称为文本数据挖掘[Hearst97]或文本知识发现[Fedlmna95],文本挖掘的主要目的是从非结构化文本文档中提取有趣的、重要的模式和知识。可以看成是基于数据库的数据挖掘或知识发现的扩展F[ayyda96,Simoudis96]。 文本挖掘是从数据挖掘发展而来,因此其定义与我们熟知的数据挖掘定义相类似。但与传统的数据挖掘相比,文本挖掘有其独特之处,主要表现在:文档本身是半结构化或非结构化的,无确定形式并且缺乏机器可理解的语义;而数据挖掘的对象以数据库中的结构化数据为主,并利用关系表等存储结构来发现知识。因此,有些数据挖掘技术并不适用于文本挖掘,即使可用,也需要建立在对文本集预处理的基础之上。 3.文本挖掘过程 文本知识发现主要由以下步骤组成： 文档集合---[文本预处理]---&gt;文档中间形式----[文本挖掘]-----&gt;模式----[评估与表示]-----&gt;知识 1)文本预处理: 选取任务相关的文本并将其转化成文本挖掘工具可以处理的中间形式。 文本集---&gt;特征抽取---&gt;特征选择---&gt;文本特征矩阵 通常包括两个主要步骤: (a)特征抽取:建立文档集的特征表示,将文本转化成一种类似关系数据且能表现文本内容的结构化形式,如信息检索领域经常采用的向量空间模型就是这样一种结构化模型。 (b)特征选择:一般说来结构化文本的特征空间维数较高,需要对其进行缩减,只保留对表达文本内容作用较大的一些特征。 2)文本挖掘: 在完成文本预处理后,可以利用机器学习、数据挖掘以及模式识别等方法提取面向特定应用目标的知识或模式。 3)模式评估与表示最后一个环节是利用已经定义好的评估指标对获取的知识或模式进行评价。如果评价结果符合要求,就存储该模式以备用户使用;否则返回到前面的某个环节重新调整和改进,然后再进行新一轮的发现。 4.研究现状 在文本挖掘过程中,文本的特征表示是整个挖掘过程的基础;而关联分析、文本分类、文本聚类是三种最主要也是最基本的功能。 4.1文本特征表示 传统数据挖掘所处理的数据是结构化的,其特征通常不超过几百个;而非结构化或半结构化的文本数据转换成特征向量后,特征数可能高达几万甚至几十万。所以,文本挖掘面临的首要问题是如何在计算机中合理的表示文本。这种表示法既要包含足够的信息以反映文本的特征,又不至于太过庞大使学习算法无法处理。这就涉及到文本特征的抽取和选择。 文本特征指的是关于文本的元数据,可以分为描述性特征,如文本的名称、日期、大小、类型以及语义性特征,如文本的作者、标题、机构、内容。描述性特征易于获得,而语义特征较难获得。在文本特征表示方面,内容特征是被研究得最多的问题。 当文本内容被简单地看成由它所包含的基本语言单位(字、词、词组或短语等)组成的集合时,这些基本的语言单位被称为项(Term)。如果用出现在文本中的项表示文本,那么这些项就是文本的特征。 对文本内容的特征表示主要有布尔模型、向量空间模型、概率模型和基于知识的表示模型。因为布尔模型和向量空间模型易于理解且计算复杂度较低,所以成为文本表示的主要工具。 (1)特征抽取 中文文档中的词与词之间不像英文文档那样具有分隔符,因此中、英文文档内容特征的提取步骤略有不同。 英文文档集合---&gt;消除停词---&gt;词干抽取---&gt;特征词集合 中文文档集合---&gt;消除停词---&gt;词语切分---&gt;特征词集合 消除停词:文本集有时包含一些没有意义但使用频率极高的词。这些词在所有文本中的频率分布相近,从而增加了文本之间的相似程度,给文本挖掘带来一定困难。解决这个问题的方法是用这些词构造一个停词表或禁用词表(stop word list)[Ricardo1991],在特征抽取过程中删去停词表中出现的特征词。 常用的停词包括虚词和实词两种,如 (i)虚词:英文中的”a,the,of,for,with,in,at,…”中文中的”的,地,得,把,被,就…” (ii)实词:数据库会议上的论文中的“数据库”一词,可视为停词。 词干抽取: 定义: 令V(s)是由彼此互为语法变形的词组成的非空词集,V(s)的规范形式称为词干(stem)。 例如,如果V(s)={connected,connecting,connection,connections},那么s=connect 是V(s)的词干。 词干抽取(stemming)有四种不同的策略:词缀排除(affix rermoval)、词干表查询(table lookup)、后继变化(successor variety)和n-gram。其中词缀排除最直观、简单且易于实现。多数词的变形是因添加后缀引起的,所以在基于词缀排除策略的抽取算法中后缀排除最为重要,Porter算法[Porter80]是后缀排除算法中最常用的一种。 词干抽取将具有不同词缀的词合并成一个词,降低文本挖掘系统中特征词的总数,从而提高了挖掘系统的性能。 当然,也有两点需要注意: (1)词干抽取对文本挖掘性能的提高仅在基于统计原理的各种分析和挖掘技术下有效。在进行涉及语义和语法的自然语言处理时,不适宜采用词干抽取技术。 (2)词干抽取对文本挖掘或信息检索准确性的影响至今没有令人信服的结论,因此许多搜索引擎和文本挖掘系统不使用任何词干抽取算法。 汉语切分: 汉语的分词问题己经基本解决,并出现了多种分词方法。这些分词方法可以分为两类:一类是理解式分词法,即利用汉语的语法知识、语义知识及心理学知识进行分词;另一类是机械式分词法,一般以分词词典为依据,通过文本中的汉字串和词表中的词逐一匹配完成词语切分。第一类分词方法算法复杂,实际应用中经常采用的是第二类分词方法。机械式分词法主要有正向最大匹配法,逆向最大匹配法,逐词遍历法。 由于词典的容量有限,在大规模真实文本处理中,会遇到许多词典中未出现的词,即未登录词。未登录现象是影响分词准确率的重要原因。为解决这个问题,人们提出利用N-gram语言模型进行词项划分[周01a,01b],从而摆脱基于词典的分词方法对词典的依赖。与基于词典的分词方法不同,基于N-gram技术得到的词项不一定具有实际意义。 例如:“文本挖掘”的所有N-gram项为: 1-gram:文,本,挖,掘 2-gram:文本,本挖,挖掘 3-gram:文本挖,本挖掘 4-gram:文本挖掘 其中除1-gram是单字外,2-gram中的“本挖”,3-gram中的“文本挖”,“本挖掘”都不具有实际意义。 (2)特征选择 特征选择也称特征子集选择或特征集缩减。经过特征抽取获得的特征词数量很多,有时达数万个特征。如此多的特征对许多文本挖掘方法,如文本分类、聚类、文本关联分析来说未必都是有意义的;而过大的特征空间还会严重影响文本挖掘的效率,因此选择适当的特征子集十分必要。 通常采用机器学习的方法进行文本特征选择。虽然机器学习中有许多选取特征子集的算法,但有些算法复杂且效率低下,不适于处理庞大的文本特征集。 国外对特征选择的研究较多[Mladenic99,Mladenic03,Lewis92,Liu96],特别是已有专门针对文本分类特征选择方法的比较研究[Yang97]。国内对这一问题的研究以跟踪研究为主,集中在将国外现有特征评估函数用于中文文本特征选择[周02]及对其进行改进[李99]。 4.2基于关键字的关联分析 文本数据一旦被转化成结构化中间形式后,这种中间形式就作为文本挖掘过程的基础。 与关系数据库中关联规则的挖掘方法类似,基于关键词的关联规则产生过程包括两个阶段: 关联挖掘阶段:这一阶段产生所有的支持度大等于最小支持度闭值的关键词集,即频繁项集。 规则生成阶段:利用前一阶段产生的频繁项集构造满足最小置信度约束的关联规则。 Feldman等人实现了基于上述思想的文本知识发现系统KDT[Feldman96]、FACT[Feldman97],KDT系统在Reuter22173语料集中发现的关联规则示例: [Iran,Nicaragua,Usa]-&gt;Reagan 6/1.00 [gold,copper]-&gt;Canada 5/0.556 [gold,silver]-&gt;USA 19/0.692 根据不同的挖掘需要,可以利用不同的挖掘方法,如关联挖掘、最大模式挖掘或层次关联挖掘,完成相应的文本分析任务。 4.3文本分类 文本分类是文本挖掘中一项非常重要的任务,也是国内外研究较多的一种挖掘技术。在机器学习中分类称作有监督学习或有教师归纳,其目的是提出一个分类函数或分类模型(也称作分类器),该模型能把数据库中的数据项映射到给定类别中的一个。 一般来讲,文本分类需要四个步骤: (1)获取训练文本集:训练文本集由一组经过预处理的文本特征向量组成,每个训练文本(或称训练样本)有一个类别标号; (2)选择分类方法并训练分类模型:文本分类方法有统计方法、机器学习方法、神经网络方法等等。在对待分类样本进行分类前,要根据所选择的分类方法,利用训练集进行训练并得出分类模型; (3)用导出的分类模型对其它待分类文本进行分类; (4)根据分类结果评估分类模型。 另外需要注意的是,文本分类的效果一般和数据集本身的特点有关。有的数据集包含噪声,有的存在缺失值,有的分布稀疏,有的字段或属性间相关性强。目前,普遍认为不存在某种方法能适合于各种特点的数据[Yang99a,Yang99b]。 随着nIetmet技术的发展和普及,在线文本信息迅速增加,文本分类成为处理和组织大量文本数据的关键技术。而近二十多年来计算机软、硬件技术的发展和自然语言处理、人工智能等领域的研究进展为文本自动分类提供了技术条件和理论基础。迄今为止,文本分类研究已经取得了很大的进展,提出了一系列有效的方法,其中分类质量较好的有k最近邻(k-Nearest Neighbor,KNN),[Iwayama95,Yang97,Yang99a]、支持向量机(Support Vector Machine,SVM)[Joachims98]、朴素贝叶斯(Naive Bayes,NB)[Lewis94,Chakra97,Lewis98]。1998年文献[Liu98]提出了基于关联规则的分类方法CBA,此后陆续有人进行这方面的研究,如CAEP[Dong99]、JEP[Li00a,Li00c]、DeEPs[Li0Ob]、CMAR[Li01]和用于文本分类的ARC[Zaiane02]。 国内对中文文本自动分类的研究起步较晚,尽管己有一些研究成果[李04,姚03,邹99,周01a],但由于尚没有通用的标准语料和评价方法,很难对这些成果进行比较。而对基于关联规则的文本分类的研究在国内还未见到。 4.4文本聚类 文本聚类是根据文本数据的不同特征,将其划分为不同数据类的过程。其目的是要使同一类别的文本间的距离尽可能小,而不同类别的文本间的距离尽可能的大。主要的聚类方法有统计方法、机器学习方法、神经网络方法和面向数据库的方法。在统计方法中,聚类也称聚类分析,主要研究基于几何距离的聚类。在机器学习中聚类称作无监督学习或无教师归纳。聚类学习和分类学习的不同主要在于:分类学习的训练文本或对象具有类标号,而用于聚类的文本没有类标号,由聚类学习算法自动确定。 传统的聚类方法在处理高维和海量文本数据时的效率不很理想,原因是:(1)传统的聚类方法对样本空间的搜索具有一定的盲目性;(2)在高维很难找到适宜的相似度度量标准。 虽然,文本聚类用于海量文本数据时存在不足。但与文本分类相比,文本聚类可以直接用于不带类标号的文本集,避免了为获得训练文本的类标号所花费的代价。根据聚类算法无需带有类标号样本这一优势,Nigam等人提出从带有和不带有类标号的混合文本中学习分类模型的方法[Ngiam98]。其思想是利用聚类技术减少分类方法对有标号训练样本的需求,减轻手工标记样本类别所需的工作量,这种方法也称为半监督学习。 文本聚类包括以下四个步骤: (1)获取结构化的文本集。 结构化的文本集由一组经过预处理的文本特征向量组成。从文本集中选取的特征好坏直接影响到聚类的质量。如果选取的特征与聚类目标无关,那么就难以得到良好的聚类结果。对于聚类任务,合理的特征选择策略应是使同类文本在特征空间中相距较近,异类文本相距较远。 (2)执行聚类算法,获得聚类谱系图。聚类算法的目的是获取能够反映特征空间样本点之间的“抱团”性质。 (3)选取合适的聚类阈值。在得到聚类谱系图后,领域专家凭借经验,并结合具体的应用场合确定阈值。阈值确定后,就可以直接从谱系图中得到聚类结果。 目前,常见的聚类算法可以分成以下几类[Han01]: (1)平面划分法:对包含n个样本的样本集构造样本集的k个划分,每个划分表示一个聚簇。常见的划分聚类算法有k-均值算法,k-中心点算法,CLARANS算法。 (2)层次聚类法:层次聚类法对给定的样本集进行层次分解。根据层次分解方向的不同可分为凝聚层次聚类和分裂层次聚类。凝聚法也称为自底向上的方法,如AGNES;分裂法也称自顶向下的方法,如DIANA、CURE、BIRCH、Chameleon。 (3)基于密度的方法:多数平面划分法使用距离度量样本间的相似程度,因此只能发现球状簇,难以发现任意形状簇。基于密度的聚类法根据样本点临近区域的密度进行聚类,使在给定区域内至少包含一定数据的样本点。DBSCAN就是一个具有代表性的基于密度的聚类算法。 (4)基于网格的方法:采用多分辨率的网格数据结构,将样本空间量化为数量有限的网格单元,所有聚类操作都在网格上进行,如STING算法。 (5)基于模型的方法:为每个簇假定一个模型,然后通过寻找样本对给定模型的最佳拟合进行聚类。 有些聚类算法集成多种算法的思想,因此难以将其划归到上述类别中的一类,如CLIQUE综合了密度和网格两种聚类方法。 文本聚类有着广泛的应用,比如可以用来: (1)改进信息检索系统的查全率和查准率[Ricardo99]; (2)用于文本集浏览[Cutting92]; (3)搜索引擎返回的相关文本的组织[Zamir97]; (4)自动产生文本集的类层次结构[Koller97]。在带有类标号的文本集上发现自然聚类[Aggarwal99],然后利用自然聚类改进文本分类器。 5.文本挖掘与相近领域的关系 5.1自然语言处理与文本挖掘的区别 文本挖掘与自然语言处理有着千丝万缕的联系,但也存在明显的不同: (1)文本挖掘通过归纳推理发现知识,而传统的自然语言处理多采用演绎推理的方法,很少使用归纳推理方法。 (2)文本挖掘在大规模文本集而不是少数文本中发现知识,其目的不在于改善对文本的理解而是发现文本中的关系。虽然自然语言处理的两个新兴领域:信息检索(Information Retrieval,IR)和信息提取(Information Extraction,IE)也是以大规模文本集为对象,但只要使用严格的演绎推理,那么就不能称作文本挖掘。主要原因是它们没有发现任何知识,只是发现符合某种约束条件的文本而不是知识本身。 [比较][方法不同][目标不同][对象范围不同] 自然语言处理：[演绎推理方法][更好的理解文本][以一篇或少数文本为研究对象，发现表示文本特点的关系] 文本挖掘：[归纳推理方法][更好的使用文本][以大量文本组成的文本集为研究对象，在文本集中发现文本间或文本集中词与词之间的关系] 1)信息检索与文本挖掘 信息检索是与数据库技术并行发展多年的领域,其中以文本为对象的文本信息检索以非结构或半结构化数据为处理对象,研究大量文本的信息组织和检索问题。 文本信息检索主要发现与用户检索要求(如关键词)相关的文本。例如,基于关键词的文本检索使用相关度量计算文本与用户查询间的相关性并按相关程度高低排序获得的文档。 近年来,基于自然语言处理技术发展起来的智能检索技术包含了对歧义信息的检索处理,如“苹果”,究竟是指水果还是电脑品牌;“华人”与“中华人民共和国”的区分,这类检索通过歧义知识描述库、全文索引、上下文分析以及用户相关反馈等技术实现文本信息检索的智能化。与文本挖掘不同,智能信息检索仍然只是关注从文本集中更有效地识别和提取相关文档,而不发现任何新的信息或知识。 2)信息提取与文本挖掘 信息提取(IE)是指提取文本集中预定义的事件或任务信息的过程,例如关于恐怖事件信息的提取,可以包括事件时间,地点,恐怖分子,受害者,恐怖分子采用的手段等等。其目的在于发现文本中的结构模式。主要过程是先根据需要确定结构模式,然后从文本中提取相应的知识填进该结构模式。文本挖掘任务则与之正好相反,它需要自动发现那些IE中给定的模式。 5.2文本挖掘与相关领域的交叉 虽然以上介绍的研究领域与文本挖掘存在明显的不同,但它们在某种程度上也存在交叉。最典型的交叉就是通过技术和方法的互相借鉴为各自领域提供新的有效的方法,如许多文本挖掘系统中采用的预处理方法就是最先在信息检索领域中提出并使用的。除此之外,还有其它的例子,如: (1)基于文本挖掘的汉语词性自动标注 利用文本挖掘研究词及词性的序列模式对词性的影响是非常有新意的研究,这与人在根据上下文对词性进行判断的方法是一致的,不但根据上下文的词、词性,而且可以根据二者的组合来判断某个词的词性。 国内从数据挖掘的角度对汉语文本词性标注规则的获取进行了研究[李01]。其方法是在统计语料规模较大的情况下,利用关联规则发现算法发现词性标注规则。只要规则的置信度足够高,获得的规则就可以用来处理兼类词的情况。该过程完全是自动的,而获取的规则在表达上是明确的,同时又是隐含在数据中、用户不易发现的。 (2)基于信息抽取的文本挖掘 为将非结构化的自然语言文档表示成结构化形式以便直接利用传统的数据挖掘技术来进行文本挖掘。已有多种结构化方法被提出,如前面提到的文本特征表示方法就是最典型的一种。此外,随着信息抽取技术的不断发展[Freitag98,Clifton99],它在文本挖掘领域扮演着日益重要的角色。信息抽取的主要任务是从自然语言文本集中查找特别的数据段,然后将非结构化文档转化为结构化的数据库,以便更容易地理解文本。基于信息抽取的文本挖掘系统框架[Nahm01]: Text-&gt;{Information Extraction-&gt;DB-&gt;KDD}-&gt;Rule Base 在这个系统中,IE模块负责在原始文本中捕获特别的数据段,并生成数据库提供给知识发现模块进一步挖掘。 5.3文本挖掘技术在Email处理方面的应用 由于Email文档和普通文档之间有许多相似之处,所以可以将挖掘普通文档涉及的技术和方法用于Email信息挖掘。目前,有许多关于利用文本挖掘技术有效地组织和分析Email信息的研究。例如,通过分析Email的语言和作者性别群进行计算机取证[Rajman97];将Email信息的结构特征和语言学特征与SVM结合进行作者身份鉴别。 文本挖掘技术除了用于组织Email信息外,还可以用于对Email消息进行分类。如:利用朴素贝叶斯算法[Rennie00]、Rocchio算法、SVM方法和Bayesian方法[sahami98,Andr00,Sakkis01]对Email信息进行分类和过滤。此外,文献[Cohen96]和[Lewis94]则提出两个基于规则的系统,这两个系统都是利用文本挖掘技术来分类Email信息。","categories":[],"tags":[]},{"title":"天下第一铭--汤晓鸥【转载】","slug":"天下第一铭[汤晓鸥]","date":"2016-11-08T07:36:00.000Z","updated":"2016-11-08T07:36:00.000Z","comments":true,"path":"2016/11/08/天下第一铭[汤晓鸥]/","link":"","permalink":"http://gwang-cv.github.io/2016/11/08/天下第一铭[汤晓鸥]/","excerpt":"","text":"天下第一铭 作者：汤晓鸥 2003年3月8日，陪秋梅过了最后一个属于我们二人世界的妇女节（一直觉得妇女节比情人节重要），第二天，我们安静的二人世界就变成了吵闹的三口之家。新来的第三者白白胖胖，叫铭铭，是在香港威尔士亲王医院10层楼的产科病房出生的。铭铭出生的那天，11层楼住进了一个特殊的病人，说是肺病。可是我们那几天太高兴了，根本没注意。过了两天医生叫我们去他办公室问我们要不要提前出院，因为楼上有个传染病人。我们觉得还有很多东西要和护士学，肺炎也没什么可怕的，不想早出院。等我们回到病房，发现一个层楼的新任妈妈都在收拾行李,已经走的差不多了。我们才明白问题很严重。回家后的两个多月，再没敢带铭铭出门一步。后来才知道11层的病人是香港第一例SARS. 铭铭的全名叫汤之铭，是佛教大师南怀瑾先生起的名字。一直觉得是老先生取的名字保佑了铭铭。名字是老先生根据2000多年前的一部畅销书《大学》里面的典故起的。经历了SARS的苦其心志，又有老先生的保佑, 我一直觉得天将降大任于铭铭，常和秋梅讲，铭铭将来很可能成就“天下第一铭”。真是想什么来什么，果然四个月大时，就显灵了。 那时我父母第一次从国内来看铭铭，第二天就要到了，可是我们一直在为一件很头疼的事伤脑筋，铭铭已经14天没大便了，看了几次医生，都说孩子活蹦乱跳的没问题，可能消化太好，听医生讲14 天可能是香港地区的纪录了，不知是不是华南地区纪录。不管怎样说，我儿子有了他自己的第一个地区级纪录了，可惜后来再没能破此纪录，最多一次才四天，可能上次太难受了，看来铭铭也不傻，不愿为虚名太苦了自己。好在我父母来的头一天，问题解决了。父母来了后抱着铭铭说，这孩子没照片上看着胖了，怎么这么轻，我当时后悔不已，不该逼铭铭做他不愿意做的事，否则铭铭至少比头一天重一倍。这孩子其实用心良苦。 铭铭六个月大的时候，妈妈的假期结束了，不得不回北京工作了。铭铭当然毫不犹豫地决定跟妈妈走（主要是从他的哭声中判断的），这样我又开始了对微软亚洲研究院的经久不息的访问。可能是访问实在太频了，结果我访问的媒体计算组的主任，时任研究院副院长的张宏江问我愿不愿意接管他的媒体计算组，还没等我们开始谈条件，没过多久，研究院重组，宏江成了新成立的工程院院长，另一位副院长Harry(沈向洋)成了研究院新院长。Harry好像觉得我来管媒体计算组不大合适。我也没问为啥。过了没多久，一个周三的下午，Harry突然来电邮说想和我谈谈。原来Harry想找我接管他自己的视觉计算组，又觉得对不起媒体计算组，所以干脆将两个组合并成一个，问我愿不愿带。我第二天就答应了，Harry也怕夜长梦多，隔天我们就把很多细节敲定了，没有经过任何面试，我就在几天之内成了研究院的人了。周六，我就买了房子。那一周，感觉上像两个恋人生怕对方反悔而匆匆领了结婚证。 我当然不会反悔，我对研究院其实爱慕已久，研究院在我心里很像铭铭，大有天下第一铭的气势。我一直觉得Bill一生中做了两个了不起的决定，第一是和IBM签了DOS协议，第二就是建立了微软亚洲研究院。当然，有些同学可能不同意这种说法，我有时也想，和世界上最大的计算机公司签约怎么能和同世界上最大的国家签约相比呢，所以也许建立了亚洲研究院应该更重要。 北京的学校差不多集中了中国十几亿人中最优秀的人才。研究院是中国唯一的一所由跨国公司成立的从事基础研究的地方。和国外一流研究机构相比，研究院近水楼台；和国内的一流研究院比，亚洲研究院具有国际一流的理念和管理模式；和IBM 及Google 在中国的研究院比，亚洲研究院从事基础研究而不是产品开发。这样独树一帜的地位，天下无双。 其后果自然是人才的高度集中。其程度让我想起了中国科大和麻省理工学院(MIT)。三个地方的人都挺好，却不太一样。说起来上世纪80年代的科大最难进，因为她只看高考成绩，没什么别的好说的。这样的后果是人才比较同质化，大家的长处都差不多，学生都很像运动员，会比赛，但缺少解决实际问题的能力。MIT就好申请多了，允许书面申请，这样即使某一方面较弱也可以申诉，强调自己的强项。课外活动有超常的地方也可以加分不少。微软亚洲研究院就更好进了，不但有书面申请，还可以当面申诉（面试），有机会全面表现自己。当然，全面表现的后果也很严重，就是进去以后要全面兑现。研究能力，编程能力，写作能力，吹牛能力，缺一不可。 视觉计算组的同事就具有这样的特性，感觉和他们在一起，没有什么题目做不出来的。所以我又想起了铭铭，总感觉和铭铭在一起的时间太少，想把每一分钟都记录下来，结果照了大量照片。于是很自私地号召大家做照片管理方面的研究，就有了我们在SIGCHI注1上的第一篇长论文。接着为了更方便地把照片中的人像一次从多张照片中分割出来，又做了多图分割的题目。为了快速方便地查找图像，我们做了实时图像检索技术。为了找到更多有趣的应用，又用人脸检测和照片管理技术做了一个将真人头像植入卡通图片的技术，于是很容易的用铭铭的照片将“小兵张嘎”动画图片系列变成了“小兵汤嘎”。我在研究院和一些高校做报告时经常把我们的研究课题总结为“下一代”图象处理技术，因为我们的技术多是应用在我们“下一代”儿童的照片上。 铭铭的照片经常用在视觉计算组的各种实验数据里，成了组里最受欢迎的形象模特 我们做的一些好玩的技术已经开始影响微软的图像管理和搜索产品开发。在计算机研究领域有个矛盾，要想在实际产品中应用，一项技术必需简单实用，要想发表文章，这项技术又必需显得复杂深奥。要想既像Google那样做出实用产品，又像MIT那样在顶级会议发表文章，就要付出更多辛苦。作为一个做基础研究的地方，我们对在顶级会议发表文章的重视程度和MIT没有什么区别。在过去三年中，我们在一流的计算机视觉会议（ICCV注2, CVPR注3, ECCV注4）发表了60多篇论文。至少在数量上已差不多“天下第一铭”。我常讲做研究就像比武论剑一样,要论剑就要到华山论剑,如果你一定要去太行山论剑, 去挺进大别山，那别人只能当你是游击队, 永远也别想成正规军。在计算机视觉领域，农村是永远也包围不了城市的。华山以外，很难论出好剑。 发这些论文的另外一个好处是吸引了很多好学生，这些年我见过很多非常优秀的学生，有些已不能用优秀来形容，只能说是天才。晓刚注5是我见到的第一个天才学生，在硕士阶段就发表了五篇CVPR/ICCV。他的才华和人品如此出众，以至于我毫不犹豫地将妹妹嫁给了他。后来我的另一个天才级学生达华注6发表了更多的文章，可是我已经没有妹妹可以再嫁了。好在最近的一个天才级学生靖宇注7，来的时候就有女朋友了。靖宇编程打字的速度是如此之快，以至于我看不清他在键盘上快速移动的手。这三个学生共同特点是都收到MIT 和斯坦福的全额奖学金。晓刚和达华去了MIT, 靖宇选择了斯坦福。我有种感觉，将来他们都会非常成功，成为各自领域的“天下第一铭”。我有种感觉，他们会越来越多。我更有种感觉，铭铭不属于他们。 铭铭让我自豪的地方也很多。比如铭铭长的很漂亮。这不是我一个人说了算，你可以去问晓晓，桃桃，月月，同同，扬扬，希希……我家院里每个四五岁大的小女孩儿都认为铭铭是她最好的朋友。铭铭四岁前所结交的女朋友（在幼儿园结识的不算）已超过他爸爸四十年艰苦努力的成果（在研究院结识的不算）。 可惜铭铭对学习的态度就像功夫熊猫阿波对面条的感觉，毫无兴趣。铭铭对面条的感觉倒像阿波对功夫的感觉，兴趣盎然。铭铭的人生理想和同龄孩子很不一样，不是做医生，警察，或宇航员，而是“吃饭，睡觉，做佳菲猫”。而且说到做到，铭铭唯一喜欢的课程是厨艺课。厨艺课老师Mariana也觉得铭铭是五岁孩子中厨艺最精湛的了。可惜和同龄孩子一起的时候，极少有比厨艺的时候，反倒是认字，背诗经常被拿出来做表演项目。为了培养铭铭对体育的兴趣，对艺术的热爱，及对中华民族的自豪感，秋梅和我一起带铭铭去看了奥运会开幕式。对于这场人类历史上最精彩最完美最盛大的演出，铭铭印象最深刻的是我在现场餐厅为他买的两根烤香肠。想起来那一定是这世界上代价最高的两根香肠了。 也许铭铭的血液里真的是流淌着面条汤？希望铭铭长大时，可以选择的已不只华山这一条路，总不能人人都上华山，太挤了，希望有更多的山可以上，有更多的路可以走。总得给铭铭这样不爱学习又厨艺精湛的孩子一条出路吧，但愿那条路不像面条一样弯延曲折。 秋梅近来常怪我乱讲天下第一铭，给讲坏了。我只好苦笑，怪自己当初求上帝的时候忘了说是正着数还是倒着数了。我就安慰秋梅说“在认字，背诗，音乐，数学，中文，英文，这几个小的方面，铭铭是比别人差一点，好吧，不只一点，差一节，一大节，我们可能也不用太担心，或许铭铭是想后发制人。” 秋梅温柔地看了我一眼，冷冷地说，“制谁呀！你看后面还有人么？” 作者介绍 汤晓鸥教授，是汤之铭的爸爸。1990年于中国科学技术大学获学士学位，1996年于麻省理工学院(MIT)获博士学位。现于香港中文大学信息工程系任终身教授。2005到2007年，于微软亚洲研究院担任视觉计算组主任。现任IEEE ICCV’09程序委员会主席 (Program Chair)及IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)编委 (Associate Editor)。他的研究领域包括计算机视觉、模式识别、及视频处理。 晓鸥在亚洲研究院期间，被一致推选为研究院文工团团长，兼团委书记，连续三年出任研究院年度文艺晚会主持人，他的演艺生涯开始于研究院，也是在研究院达到顶峰，为此，他为自己起了个艺名叫“小o”。小o的名言是：“看事物要一分为二，任何事物都有两个方面，有可笑的一面，同时也有更可笑的一面”。他就是这样看着铭铭一天天长大。 注1，SIGCHI: Special Interest Group for Computer Human Interaction，是世界上人机交互领域最大的专业组织，这是一个多学科交叉的学术组织，包括计算机科学家、软件工程师、心理学家、交互设计人员、图形设计人员、社会学家和人类学家等等。大家共同理念是”设计有用且可用的技术是一个多学科交叉的过程，这一过程的恰当实施可以改变人们的生活”。注2，ICCV: International Conference on Computer Vision，由IEEE主办的国际计算机视觉大会。作为世界顶级的学术会议，首届国际计算机视觉大会于1987年在伦敦揭幕，其后两年举办一届。2005年第10届ICCV在北京举行。注3，CVPR: Computer Vision and Pattern Recognition, 由IEEE主办的国际计算机视觉与模式识别大会，它是计算机视觉领域最顶级的三大学术会议之一。注4，ECCV: European Conference on Computer Vision，两年举办一次，是计算机视觉领域三大顶级学术会议之一。注5，王晓刚：中国科大本科毕业，少年班第一名，郭沫若奖学金获得者，于香港中文大学取得硕士学位，现于麻省理工学院攻读博士学位。注6，林达华：中国科大本科毕业，于香港中文大学取得硕士学位，获香港中文大学工程院优秀硕士论文奖（每年度全院只选一人），现于麻省理工学院攻读博士学位。注7，崔靖宇：清华大学本科及硕士毕业，随汤晓鸥在研究院做了一年半的实习生，获微软学者奖学金，现于斯坦福大学攻读博士学位。 转载自：http://blog.sina.com.cn/s/blog_4caedc7a0100bgu9.html","categories":[{"name":"Researcher","slug":"Researcher","permalink":"http://gwang-cv.github.io/categories/Researcher/"}],"tags":[{"name":"Researcher","slug":"Researcher","permalink":"http://gwang-cv.github.io/tags/Researcher/"}]},{"title":"安装lpsolve库 for MATLAB","slug":"安装lpsolve库 for MATLAB","date":"2016-11-03T02:29:57.000Z","updated":"2016-11-03T02:29:57.000Z","comments":true,"path":"2016/11/03/安装lpsolve库 for MATLAB/","link":"","permalink":"http://gwang-cv.github.io/2016/11/03/安装lpsolve库 for MATLAB/","excerpt":"","text":"lpsolve是sourceforge下的一个开源项目，它的介绍如下： Mixed Integer Linear Programming (MILP) solver lp_solve solves pure linear, (mixed) integer/binary, semi-cont and special ordered sets (SOS) models.lp_solve is written in ANSI C and can be compiled on many different platforms like Linux and WINDOWS . lpsolve是一个混合整数线性规划求解器，可以求解纯线性、（混合）整数/二值、半连续和特殊有序集模型。并且经过实际验证，有极高的求解效率。 sourceforge主页 1.在Windows x64和Matlab环境下使用lpsolve 需要在网址lpsolve_5.5.2.5 提供的文件列表中下载文件lp_solve_5.5.2.5_MATLAB_exe_win64.zip。或者下载文件lp_solve_5.5.2.5_source.tar.gz自行编译dll。 注：在Matlab下运行示例报错： Error using mxlpsolve Failed to initialise lpsolve library. 参考Using lpsolve from MATLAB: http://web.mit.edu/lpsolve/doc/MATLAB.htm给出的说明，需要将编译得到的mxlpsolve.dll拷贝到 WINDOWS\\system32文件夹下。 2.在MacOS下使用lpsolve 参考Using lp_solve in Java with Mac OS X的配置说明，下载源文件lp_solve_5.5.2.5_source.tar.gz，然后跳转到lp_solve_5.5/lpsolve55文件夹内，并执行ccc.osx： cd lp_solve_5.5/lpsolve55 sh ccc.osx 生成的文件所在的文件夹：lpsolve55/bin/osx64/，将此文件夹内生成的两个文件liblpsolve55.dylib，liblpsolve55.a拷贝到/usr/local/lib文件夹内即可： sudo cp liblpsolve55.a liblpsolve55.dylib /usr/local/lib 测试demo： cd lp_solve_5.5/demo sh ccc ./demo 3.在MacOS的Matlab中需要文件mxlpsolve.mexmaci64 需要从lpsolve主页下载源文件lp_solve_5.5.2.5_MATLAB_source.tar.gz，解压缩后，在Matlab中执行文件Makefile.m，期间需要添加各种头文件： lp_Hash.h lp_lib.h lp_matrix.h lp_mipbb.h lp_SOS.h lp_types.h lp_utils.h 可下载lp_solve_5.5.2.5_dev_ux64.tar.gz并从中获取即可。 4.举例 mxlpsove.m是建模的核心函数，一个线性规划模型的所有配置和求解都是通过这个函数完成的。lp_maker.m和lp_solve.m是对mxlpsolve.m的高层包装，简化了模型建立和求解的过程。例如用lpsolve求解数学规划问题： $$ \\max 4x_1+2x_2+x_3\\\\s.t.~ 2x_1+x_2\\le 1\\\\x_1+2x_3\\le 2\\\\x_1+x_2+x_3=1\\\\0\\le x_1\\le1\\\\0\\le x_2\\le1\\\\0\\le x_3\\le2$$ 相应的Matlab语句为： f = [4 2 1]; A = [2 1 0; 1 0 2; 1 1 1]; b = [1; 2; 1]; l = [ 0 0 0]; u = [ 1 1 2]; lp=mxlpsolve('make_lp', 1, 3); mxlpsolve('set_verbose', lp, 3); mxlpsolve('set_obj_fn', lp, f); mxlpsolve('add_constraint', lp, A(1, :), 1, b(1)); mxlpsolve('add_constraint', lp, A(2, :), 1, b(2)); mxlpsolve('add_constraint', lp, A(3, :), 0, b(3); mxlpsolve('set_lowbo', lp, l); mxlpsolve('set_upbo', lp, u); mxlpsolve('write_lp', lp, 'a.lp'); mxlpsolve('get_mat', lp, 1, 2) mxlpsolve('solve', lp) mxlpsolve('get_objective', lp) mxlpsolve('get_variables', lp) mxlpsolve('get_constraints', lp) mxlpsolve('delete_lp', lp) 重要函数说明： lp_solve LP_SOLVE Solves mixed integer linear programming problems. SYNOPSIS: [obj,x,duals] = lp_solve(f,a,b,e,vlb,vub,xint,scalemode,keep) solves the MILP problem max v = f'*x a*x &lt;&gt; b vlb &lt;= x &lt;= vub x(int) are integer ARGUMENTS: The first four arguments are required: f: n vector of coefficients for a linear objective function. a: m by n matrix representing linear constraints. b: m vector of right sides for the inequality constraints. e: m vector that determines the sense of the inequalities: e(i) = -1 ==&gt; Less Than e(i) = 0 ==&gt; Equals e(i) = 1 ==&gt; Greater Than vlb: n vector of lower bounds. If empty or omitted, then the lower bounds are set to zero. vub: n vector of upper bounds. May be omitted or empty. xint: vector of integer variables. May be omitted or empty. scalemode: scale flag. Off when 0 or omitted. keep: Flag for keeping the lp problem after it's been solved. If omitted, the lp will be deleted when solved. OUTPUT: A nonempty output is returned if a solution is found: obj: Optimal value of the objective function. x: Optimal value of the decision variables. duals: solution of the dual problem. Example of usage. To create and solve following lp-model: max: -x1 + 2 x2; C1: 2x1 + x2 &lt; 5; -4 x1 + 4 x2 &lt;5; int x2,x1; The following command can be used: &gt;&gt; [obj, x]=lp_solve([-1, 2], [2, 1; -4, 4], [5, 5], [-1, -1], [], [], [1, 2]) obj = 3 x = 1 2 lp_maker LP_MAKER Makes mixed integer linear programming problems. SYNOPSIS: lp_handle = lp_maker(f,a,b,e,vlb,vub,xint,scalemode,setminim) make the MILP problem max v = f'*x a*x &lt;&gt; b vlb &lt;= x &lt;= vub x(int) are integer ARGUMENTS: The first four arguments are required: f: n vector of coefficients for a linear objective function. a: m by n matrix representing linear constraints. b: m vector of right sides for the inequality constraints. e: m vector that determines the sense of the inequalities: e(i) &lt; 0 ==&gt; Less Than e(i) = 0 ==&gt; Equals e(i) &gt; 0 ==&gt; Greater Than vlb: n vector of non-negative lower bounds. If empty or omitted, then the lower bounds are set to zero. vub: n vector of upper bounds. May be omitted or empty. xint: vector of integer variables. May be omitted or empty. scalemode: Autoscale flag. Off when 0 or omitted. setminim: Set maximum lp when this flag equals 0 or omitted. OUTPUT: lp_handle is an integer handle to the lp created. Example of usage. To create following lp-model: max: -x1 + 2 x2; C1: 2x1 + x2 &lt; 5; -4 x1 + 4 x2 &lt;5; int x2,x1; The following command can be used: &gt;&gt; lp=lp_maker([-1, 2], [2, 1; -4, 4], [5, 5], [-1, -1], [], [], [1, 2]) lp = 0 To solve the model and get the solution: &gt;&gt; mxlpsolve('solve', lp) ans = 0 &gt;&gt; mxlpsolve('get_objective', lp) ans = 3 &gt;&gt; mxlpsolve('get_variables', lp) ans = 1 2 注意：Don’t forget to free the handle and its associated memory when you are done: &gt;&gt; mxlpsolve('delete_lp', lp); 5.参考 https://diegoresearch.wordpress.com/2008/07/10/using-lp_solve-in-java-with-mac-os-x/ http://web.mit.edu/lpsolve/doc/MATLAB.htm http://www.cnblogs.com/kane1990/p/3428129.html","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"http://gwang-cv.github.io/tags/Matlab/"}]},{"title":"Clarifai API体验","slug":"Clarifai API体验","date":"2016-11-02T07:37:27.000Z","updated":"2016-11-02T07:37:27.000Z","comments":true,"path":"2016/11/02/Clarifai API体验/","link":"","permalink":"http://gwang-cv.github.io/2016/11/02/Clarifai API体验/","excerpt":"","text":"参考官方给出的文档：https://developer.clarifai.com/guide/tag#guide-tag-responses，在本机进行简单的体验。 首先注册账户，然后创建Application，获取ID及Secret，并在Application页面下生成“Access Token”码：XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX 〇、客户端API参考：https://github.com/Clarifai/clarifai-python pip install clarifai==2.0.10 clarifai config CLARIFAI_APP_ID: CLARIFAI_APP_SECRET: 在python中实现测试： from clarifai.rest import ClarifaiApp app = ClarifaiApp() model = app.models.get('general-v1.3') print model.predict_by_url('https://samples.clarifai.com/metro-north.jpg') # or local image print model.predict_by_filename('/Users/USER/my_image.jpeg') 一、打开MAC终端，输入Request命令： 测试在线图片 curl \"https://api.clarifai.com/v1/tag/\" \\ -X POST --data-urlencode \"url=https://samples.clarifai.com/metro-north.jpg\" \\ -H \"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX\" 测试本地图片 curl \"https://api.clarifai.com/v1/tag/\" \\ -X POST -F \"encoded_data=@/Users/USER/my_image.jpeg\" \\ -H \"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX\" 测试多幅图像 curl \"https://api.clarifai.com/v1/tag/\" \\ -X POST -F \"encoded_data=@/Users/USER/my_image1.jpeg\" \\ -F \"encoded_data=@/Users/USER/my_image2.jpeg\" \\ -F \"encoded_data=@/Users/USER/my_image3.jpeg\" \\ -F \"encoded_data=@/Users/USER/my_image4.jpeg\" \\ -H \"Authorization: Bearer XXx2QXEzmXXfj1eXkXXXUFLYXXX7DX\" 二、读取Response结果 在python中使用json来解析文本结果： # -*- coding: utf-8 -*- import json for line in open(\"response.txt\"): print line str=json.loads(line) results=str['results'] strNum=len(str['results']) for ind in range(0,strNum): print ind print results[ind]['result']['tag']['classes'] 输出解析结果（图像的tag类别）： 0 [u'sketch', u'illustration', u'cute', u'man', u'no person', u'funny', u'fun', u'vector', u'character', u'graphic design', u'business', u'child', u'art', u'retro', u'love', u'moon', u'Halloween', u'graphic', u'design', u'isolated'] 1 [u'illustration', u'vector', u'sketch', u'retro', u'design', u'sketch', u'business', u'symbol', u'man', u'family', u'no person', u'graphic', u'humor', u'people', u'outdoors', u'image', u'art', u'nature', u'house', u'animal'] 2 [u'vector', u'vector', u'illustration', u'no person', u'internet', u'technology', u'design', u'symbol', u'graphic design', u'data', u'flat', u'data', u'business', u'stripe', u'set', u'design', u'square', u'science', u'education', u'creativity'] 3 [u'sleeve', u'illustration', u'isolated', u'polo', u'shirt', u'vector', u'design', u'image', u'wear', u'garment', u'sale', u'man', u'fashion', u'shopping', u'casual', u'front', u'shop', u'apparel', u'graphic', u'flat'] 4 [u'illustration', u'vector', u'sketch', u'Halloween', u'cute', u'animal', u'skittish', u'funny', u'design', u'art', u'graphic', u'fun', u'ghost', u'scary', u'no person', u'vicious', u'desktop', u'retro', u'image', u'business']","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"DL学习笔记","slug":"DL学习笔记","date":"2016-10-28T09:08:10.000Z","updated":"2016-10-28T09:08:10.000Z","comments":true,"path":"2016/10/28/DL学习笔记/","link":"","permalink":"http://gwang-cv.github.io/2016/10/28/DL学习笔记/","excerpt":"","text":"（一）DeepLearning (DL)概述1. 什么是DL？机器学习ML的框架： (1)数据：\\({(x_i,y_i)}, 1\\le i \\le m\\) (2)模型：\\(\\mathcal{F}={f(x;\\theta)}, \\theta\\in\\Theta\\) i. 线性： \\(y=f(x)=w^Tx+b\\) 【x——&gt;y】 ii. 广义线性：\\(y=f(x)=w^T\\phi(x)+b\\) 【x——&gt;[\\(\\bar{x}=\\phi(x)\\)]——&gt;y，其中的\\(\\phi\\)为模式识别中的特征Feature，这一步也称为特征学习。】 iii. 非线性：人工神经网络（ANN） (3)准则：损失函数\\(L(y,f(x))\\) 经验风险：\\(R(\\theta)=\\frac{1}{m}\\sum_{i=1}^m{L(y,f(x_i,\\theta))}\\) 正则化项：\\(|w|_2^2\\) Minimizing： \\(R(\\theta)+\\lambda|w|_2^2\\)，或稀疏正则项L1. 因此转化为一个最优化问题。 神经网络ANN中 \\(y=\\sigma(\\sum_i{w_i x_i+b})\\) 相当于从P维到Q维的一个映射函数。则DL就是解决这个深度前馈神经网络的算法。 2. 存在的困难及挑战可训练的参数太多；【参数过多带来的问题具体包括：计算资源要大，数据要多(否则出现过拟合)，算法效率要高】 多层网络以后的优化问题变为非凸优化问题； 梯度弥散问题，即从网络层由上往下的参数调节变得非常困难； 解释困难(可通过一些可视化的方法一定程度上来进行解释)； 3.算法历史1958年，感知机Perception：一个神经细胞的处理能力较差，与或运算无法实现。 1986年，神经网络的概念出现：BP算法，对浅层网络做了很多的工作。一方面受限于当时的硬件和软件问题。 1998年，CNN卷积神经网络，在手写体识别中取得了成功。—LeCun 2006年，DBN深度置信网络，—Hinton 4.为什么学习DL有效！【语音识别，目标识别，NLP，CV….】 5.领域概述学术机构： TorontoU，Hinton，1975年EdinburghU’s PHD;NewYorkU，LeCun，1987年PHD;MentrealU，Bengio，1991年McGillU’s PHD;StanfordU, Ng，2003年UCBerkeley’s PHD; 学术会议：NIPS，ICML，ICRL，… 参考: 深度学习课程-概述 （二）FNN &amp; BP一.前馈神经网络FNN1.神经元、神经层、神经网 神经元 x_1 -w_1-\\ .. -w_i--〇z---&gt;a x_p -w_p-/ 一个神经元的输出是一个线性函数与一个非线性函数的复合：\\([z=\\sum w_ix_i+b,~a=f(z)]=&gt;a=f(\\sum w_ix_i+b)\\). 其中激活函数包括：sigmod: \\(\\sigma(x)=\\frac{1}{1+e^x};~\\tan(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}};~|x|;~\\)等 神经层 x_1 -w_1--〇--\\-〇--&gt;a_1 X .. -w_i--〇--X-〇--&gt;... X x_p -w_p--〇--/-〇--&gt;a_p 神经层上的每个神经元的输入时相同的，但权值是不同的。我们用 \\( a^{(l)}_i \\)表示第 \\( l \\)层第 \\( i \\)单元的激活值（输出值）。 神经网 -〇-\\ -〇-\\ -〇--〇-/-〇--〇-... -〇--〇-\\-〇--〇-... -〇-/ -〇-/ n_1 n_2 n_3 ... n_L 2.记号超参数(并不是学习出来的，而是认为根据需要设定的参数，也称元参数)：层数L，第l层的神经元个数\\(n^{(l)}\\)，神经元非线性函数\\(f_l()\\). 要学习的参数：连接权weight参数\\(w_i^{(1)},w_i^{(2)},..w_i^{(L)}\\)，两层之间的连接权。 偏bias参数\\(b^{(1)},b^{(2)},..b^{(L)}\\). 第l层神经元的状态\\(z^{(l)},1\\le l\\le L\\)；第l层神经元的激活activation：\\(a^{(l)}\\) 3.前馈计算(1)基本公式 \\(z^{(l+1)}=w^l a^l+b^l \\) \\(a^l=f_l(z^l) \\) \\(h_{w,b}(x)=a^{(l+1)}=f(z^{(l+1)})\\) (2)前馈计算(\\(W=(w^1,…,w^l),b=(b^1,..,b^l)\\)) \\(l=1, a^l=x\\) 计算步骤：\\(a^1-&gt;z^2-&gt;a^2-&gt;…-&gt;z^L-&gt;a^L\\) “目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是\\(nl\\)层的神经网络，第1层是输入层，第 \\(n_l\\)层是输出层，中间的每个层 \\(l\\)与层 \\(l+1\\)紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 \\(L_2\\)层的所有激活值，然后是第 \\(L_3\\)层的激活值，以此类推，直到第 \\(L{n_l}\\)层。这是一个前馈神经网络的例子，因为这种联接图没有闭环或回路。”——UFLDL 4.应用于ML数据D：\\((x_i,y_i),1\\le i\\le N\\) 模型M：\\(y=h(x|w,b)\\) 准则C: \\(\\sum_i|y_i-a^l(x|w,b)|^2+\\lambda|w|_F^2) \\)=min 其中，第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过度拟合: \\(||w||F^2=\\sum_i\\sum_jw{ij}^2\\) 权重衰减参数 \\( \\lambda \\)用于控制公式中两项的相对重要性。 使用梯度下降法进行求解这个优化问题。 将上式写为：\\(\\sum_i J(x_i,y_i;w,b)+\\lambda|w|_F^2\\)=min 目标函数关于待求参数的导数： 其中：$$\\frac{\\partial|w|_F^2}{\\partial w}=2w$$ 然后重点求:$$\\frac{\\partial \\sum J(.)}{\\partial w};~~\\frac{\\partial \\sum J(.)}{\\partial b}$$ 迭代公式：$$w^{t+1}=w^t-\\alpha \\frac{\\partial \\sum J(.)}{\\partial w}$$$$b^{t+1}=b^t-\\alpha \\frac{\\partial \\sum J(.)}{\\partial b}$$其中\\(\\alpha\\)是学习速率。 二.BP算法“我们的目标是针对参数 \\( W \\)和 \\( b \\)来求其函数 \\( J(W,b) \\)的最小值。为了求解神经网络，我们需要将每一个参数 \\( W^{(l)}{ij} \\)和 \\( b^{(l)}_i \\)初始化为一个很小的、接近零的随机值（比如说，使用正态分布 \\( {Normal}(0,\\epsilon^2) \\)生成的随机值，其中 \\( \\epsilon \\)设置为 \\( 0.01 \\) ），之后对目标函数使用诸如批量梯度下降法的最优化算法。因为 \\( J(W, b) \\)是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。最后，需要再次强调的是，要将参数进行随机初始化，而不是全部置为 0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，对于所有 \\( i\\)，\\( W^{(1)}{ij}\\)都会取相同的值，那么对于任何输入 \\( x \\)都会有：\\( a^{(2)}_1 = a^{(2)}_2 = a^{(2)}_3 = \\ldots \\)）。随机初始化的目的是使对称失效。”——UFLDL 1.多元函数的偏导数(1)$$X=[x_1,…,x_p]^T\\in R^p,~ y=f(X)=f(x_1,…,x_p)$$$$\\nabla_x f(x)=\\nabla_x y=\\frac{\\partial y}{\\partial x}=[\\frac{\\partial f(x_1)}{\\partial x},…,\\frac{\\partial f(x_p)}{\\partial x}]^T\\in R^p$$(2)$$x\\in R^p, y=[..]^T\\in R^q$$$$y=[f_1(x),..,f_q(x)]^T$$$$\\nabla_xy=\\nabla_x f(x)=\\frac{\\partial y}{\\partial x}=[\\frac{\\partial f(x)}{\\partial x_i}]^T\\in R^{p\\times q}$$(3) 导数法则：链式法则 2.BP算法BP思路：给定一个样例 \\( (x,y)\\)，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \\( h_{W,b}(x) \\)的输出值。之后，针对第 \\( l \\)层的每一个节点 \\( i\\)，我们计算出其“残差” \\( \\delta^{(l)}_i\\)，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 \\( \\delta^{(n_l)}_i \\)（第 \\( n_l \\)层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点（第 \\( l+1 \\)层节点）残差的加权平均值计算 \\( \\delta^{(l)}_i\\)，这些节点以 \\( a^{(l)}_i \\)作为输入。 BP算法步骤： (i)进行前馈传导计算，利用前向传导公式，得到 \\( L2, L_3, \\ldots \\) 直到输出层 \\( L{n_l} \\)的激活值。 (ii)对于第 \\( n_l \\)层（输出层）的每个输出单元 \\( i\\)，我们根据以下公式计算残差： $$\\delta_i^{(n_l)}= \\frac{\\partial J}{\\partial z_i^{n_l}}=-(y_i-a_i^{(n_l)})\\cdot f’(z_i^{(n_l)})$$ (iii)令\\(\\delta^l=\\frac{\\partial J}{\\partial z^l}\\)，则对 \\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\)的各个层，第 \\( l \\)层的第 \\( i \\)个节点的残差计算方法如下：（矩阵向量形式） $$\\begin{equation}\\begin{split}\\delta^{l}&amp;=\\frac{\\partial J}{\\partial z^{l}}=\\frac{\\partial a^l}{\\partial z^l}\\frac{\\partial z^{l+1}}{\\partial a^l}\\frac{\\partial J}{\\partial z^{l+1}}\\\\&amp;=diag(f’(z^l))\\cdot((w^l)^T\\cdot\\delta^{l+1})\\\\&amp;=(f’_l(z^l))\\odot((w^l)^T\\cdot\\delta^{l+1})\\\\&amp;=((w^l)^T\\cdot\\delta^{l+1})\\odot(f’_l(z^l))\\end{split}\\end{equation}$$以上逐次从后向前求导的过程即为“反向传导（BP）”的本意所在. (iv)计算所需的偏导数：$$\\frac{\\partial J}{\\partial w^l}=\\delta^{l+1}\\cdot(a^l)^T$$$$\\frac{\\partial J}{\\partial b^l}=\\delta^{l+1}$$ 其中，假设 \\( f(z) \\)是sigmoid函数，并且我们已经在前向传导运算中得到了 \\( a^{(l)}_i\\)。那么，使用我们早先推导出的 \\( f’(z)\\)表达式，就可以计算得到 \\( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)\\)。 3.梯度下降法求解过程(1) 对所有 \\( l\\)，令 \\( \\Delta W^{(l)} := 0 \\), \\( \\Delta b^{(l)} := 0 \\)（设置为全零矩阵或全零向量） (2) For \\( i = 1 \\) to \\( m\\)，使用反向传播算法计算: \\(\\nabla_{W^{(l)}} J(W,b;x,y) \\) \\( \\nabla_{b^{(l)}} J(W,b;x,y) \\) \\( \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y) \\) \\( \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y) \\) (3) 更新参数：$$ \\begin{align}W^{(l)} &amp;= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\b^{(l)} &amp;= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\\end{align}$$ 重复梯度下降法的迭代步骤来减小代价函数 \\( J(W,b)\\) ，以训练我们的神经网络。","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"Clarifai图像自动化标签","slug":"Clarifai","date":"2016-10-21T13:40:07.000Z","updated":"2016-10-21T13:40:07.000Z","comments":true,"path":"2016/10/21/Clarifai/","link":"","permalink":"http://gwang-cv.github.io/2016/10/21/Clarifai/","excerpt":"","text":"Clarifai 公司：www.clarifai.com Clarifai创始人Matt Zeiler是New York University (NYU) Rob Fergus教授门下的学生。从上个世纪开始，NYU就一直是neural computation的重镇。现在Deep net的前身ConvNet，就是出自 NYU 的 Yann LeCun教授组. ImageNet Large Scale Visual Recognition Competition 2013 (ILSVRC2013) 其中Matt Zeiler (http://Clarifai.com) 的算法排名第一，在不用额外训练数据的情况下，跑到了error rate 0.1174这样的成绩。 这个成绩是这样解读的：任选一张图片，扔给算法，算法返回5个结果。如果5个结果中，有一个猜对了物体类别，就算正确。换言之，如果允许猜5次，Clarifai已经有接近90%的准确率了。这里的物体类别包括了英语中两万多个名词，几乎涵盖了各大类别。 Clarifai API 参考 https://zhuanlan.zhihu.com/p/19821292","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"深度学习硬件配置","slug":"深度学习硬件配置","date":"2016-10-21T04:03:58.000Z","updated":"2016-10-21T04:03:58.000Z","comments":true,"path":"2016/10/21/深度学习硬件配置/","link":"","permalink":"http://gwang-cv.github.io/2016/10/21/深度学习硬件配置/","excerpt":"","text":"1.收到NVIDIA资助的显卡GeForce Titan X (12GB) 2.其他硬件设备： CPU： Intel i7-6700 主板：华硕B150M-PLUS (LGA1151) 内存：4GB x 2 硬盘：希捷1TB 电源：鑫谷Segotep GP700G（金牌认证、宽幅） 额定600W 机箱：Tt小板机箱 显卡接口转换器：DVI-&gt;VGI 3.备注 深度学习对CPU的要求并不是特别高，根据实际情况选择。 主板建议还是选择一个好的品牌，预算充足，可以考虑X99平台。 显卡GTX1080比老Titan X的性价比要高。 内存建议32G(16x2)，近期内存价格暴涨。 硬盘，最好配SSD，用来存放软件和数据集，提升IO效率。 电源根据显卡和整机功率需求选择(TitanX+CPU等差不多350W)。 机箱可扩展，散热好即可。 4.组装 主板安装CPU及散热器，将主板安装到机箱内(面板接口要仔细)，然后按照电源、硬盘，最后将电源线依次插入相应供电接口位置。 5.参考 个人深度学习环境搭建：主机配置与组装","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"MacOS安装torch7","slug":"MacOS安装torch7","date":"2016-04-07T08:07:25.000Z","updated":"2016-04-07T08:07:25.000Z","comments":true,"path":"2016/04/07/MacOS安装torch7/","link":"","permalink":"http://gwang-cv.github.io/2016/04/07/MacOS安装torch7/","excerpt":"","text":"手动安装： $ # in a terminal, run the commands $ git clone https://github.com/torch/distro.git ~/torch --recursive $ cd ~/torch; bash install-deps; $ ./install.sh －－－－－－－－－－－－－－－－－－－－－－－－－ $ source ~/torch/install/bin/torch-activate －－－－－－－－－－－－－－－－－－－－－－－－－ $ th ______ __ | Torch7 /_ __/__ ________/ / | Scientific computing for Lua. / / / _ \\/ __/ __/ _ \\ | Type ? for help /_/ \\___/_/ \\__/_//_/ | https://github.com/torch | http://torch.ch th&gt; torch.Tensor{5,6,7} 5 6 7 [torch.DoubleTensor of size 3] 退出 th&gt; os.exit()","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"MacOS安装caffe","slug":"MacOS安装caffe","date":"2016-04-06T17:03:37.000Z","updated":"2016-04-06T17:03:37.000Z","comments":true,"path":"2016/04/07/MacOS安装caffe/","link":"","permalink":"http://gwang-cv.github.io/2016/04/07/MacOS安装caffe/","excerpt":"","text":"1.需要安装CUDA，不管有没有N卡2.安装OpenBLAS，并在caffe设置文件中设置为： # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! BLAS_INCLUDE := /opt/OpenBLAS/include BLAS_LIB := /opt/OpenBLAS/lib 3.安装依赖库，可用brew list检查安装是否完全4.添加路径： export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/lib export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/cuda/lib:$HOME/anaconda/lib:/usr/local/lib:/usr/lib:/opt/OpenBLAS/lib:/opt/OpenBLAS 5.下载caffe源文件，编译：make all时报错： PROTOC src/caffe/proto/caffe.proto make: protoc: No such file or directory 解决： sudo chown yourname /usr/local brew link yourlibpackage 6.Build: 继续编译：make test， make runtest 添加路径： export DYLD_LIBRARY_PATH=/usr/local/cuda/lib #python for req in $(cat python/requirements.txt); do pip install $req; done make pycaffe export PYTHONPATH=~/technologies/caffe/python/:$PYTHONPATH cd .. make runtest的结果： [----------] 10 tests from PowerLayerTest/0, where TypeParam = N5caffe9CPUDeviceIfEE [ RUN ] PowerLayerTest/0.TestPower [ OK ] PowerLayerTest/0.TestPower (2 ms) [ RUN ] PowerLayerTest/0.TestPowerZeroGradient [ OK ] PowerLayerTest/0.TestPowerZeroGradient (1 ms) [ RUN ] PowerLayerTest/0.TestPowerTwoGradient ... [----------] 10 tests from PowerLayerTest/0 (16 ms total) Bugs: library not found for -lboost_python 解决： brew install boost-python 测试MNIST 下载mnist数据，如下代码，报错，需安装brew install wget ./data/mnist/get_mnist.sh 转数据： ./examples/mnist/create_mnist.sh Creating lmdb... Done. ./examples/mnist/train_lenet.sh 问题： Cannot use GPU in CPU-only Caffe: check mode. 解决： 修改lenet_solver.prototxt 最后一行的GPU改为CPU，继续执行./examples/mnist/train_lenet.sh I0407 00:51:59.023721 2035871744 caffe.cpp:178] Use CPU. I0407 00:51:59.024705 2035871744 solver.cpp:48] Initializing solver from parameters: test_iter: 100 test_interval: 500 base_lr: 0.01 display: 100 max_iter: 10000 lr_policy: \"inv\" gamma: 0.0001 power: 0.75 momentum: 0.9 weight_decay: 0.0005 snapshot: 5000 snapshot_prefix: \"examples/mnist/lenet\" solver_mode: CPU net: \"examples/mnist/lenet_train_test.prototxt\" I0407 00:51:59.024927 2035871744 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt I0407 00:51:59.026877 2035871744 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist I0407 00:51:59.026897 2035871744 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy ...结果... I0407 00:57:52.664399 2035871744 solver.cpp:317] Iteration 10000, loss = 0.0032766 I0407 00:57:52.664429 2035871744 solver.cpp:337] Iteration 10000, Testing net (#0) I0407 00:57:54.761032 2035871744 solver.cpp:404] Test net output #0: accuracy = 0.9901 I0407 00:57:54.761072 2035871744 solver.cpp:404] Test net output #1: loss = 0.0290336 (* 1 = 0.0290336 loss) I0407 00:57:54.761081 2035871744 solver.cpp:322] Optimization Done. I0407 00:57:54.761087 2035871744 caffe.cpp:222] Optimization Done.","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"Python爬取优酷视频","slug":"Python爬取优酷视频","date":"2016-04-04T14:12:31.000Z","updated":"2016-04-04T14:12:31.000Z","comments":true,"path":"2016/04/04/Python爬取优酷视频/","link":"","permalink":"http://gwang-cv.github.io/2016/04/04/Python爬取优酷视频/","excerpt":"","text":"get youku videos using “flvcd.com”借用flvcd.com实现对优酷视频的爬取。 参考： www.flvcd.com 格式： http://www.flvcd.com/parse.php?kw=...&amp;format=... format='normal'; 'high'; 'super' # -*- coding:utf-8 -*- from lxml import etree import urllib2 import urllib import os import requests def getHtml(url): html = requests.get(url).content selector = etree.HTML(html) return selector def getContent(htm, xpathStr): selector = htm content = selector.xpath(xpathStr) return content def getFlv(cons, title, folder): fn = '%s' % title pa = os.path.dirname(__file__) + '/' + 'youku/' + folder # check and create folder if not os.path.exists(pa): os.mkdir(pa) fl = pa + '/%s.flv' % fn r = requests.get(cons) with open(fl, \"wb\") as code: code.write(r.content) # = = = = = = # videourl = 'http://v.youku.com/v_show/id_XMTUyMjE2MTcwOA==.html' format = 'normal' # 'high' 'normal' 'super' url = 'http://www.flvcd.com/parse.php?kw=' + urllib.quote(videourl) + '&amp;format=' + format print url req = urllib2.Request(url) req.add_header('Referer', 'http://www.flvcd.com/') req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 6.2; rv:16.0) Gecko/20100101 Firefox/16.0') res = urllib2.urlopen(req) html = res.read() # print html selector = etree.HTML(html) # get flv title xp_title='//*[@id=\"subtitle\"]' htm0=getHtml(videourl) cons=getContent(htm0,xp_title) title=cons[0].text print title # get flv href xp = '//*[@class=\"mn STYLE4\"]//@href' content = selector.xpath(xp) print '%s' % len(content) x=0 for con in content: if 'http://k.youku.com' in con: print con getFlv(con, '%s' % x, title) # urllib.urlretrieve(con, getPath('%s' % x, title))# , callbackfunc) x+=1","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://gwang-cv.github.io/tags/Python/"}]},{"title":"Python爬取arxiv的paper","slug":"Python爬取arxiv的paper","date":"2016-04-01T07:21:47.000Z","updated":"2016-04-01T07:21:47.000Z","comments":true,"path":"2016/04/01/Python爬取arxiv的paper/","link":"","permalink":"http://gwang-cv.github.io/2016/04/01/Python爬取arxiv的paper/","excerpt":"","text":"每天都要去arxiv上关注最新一天的论文（computer vision）更新，而且每次下载的论文的名称都是arxiv的代号，需要花时间去整理，于是用python写一个非常简单的爬虫，自己使用，足够节省时间了。 import requests from lxml import etree import os import time import re from multiprocessing.dummy import Pool def getHtml(url): html = requests.get(url).content selector = etree.HTML(html) return selector def getContent(htm, xpathStr): selector = htm content = selector.xpath(xpathStr) return content def getDownPdf(cons, title, folder): fn = '%s' % title pa = os.path.dirname(__file__) + '/' + 'arxiv' + '/%s' % folder # check and create folder if not os.path.exists(pa): os.mkdir(pa) fl = pa + '/%s.pdf' % fn r = requests.get(cons) with open(fl, \"wb\") as code: code.write(r.content) #### main ### url0 = 'http://arxiv.org/list/cs.CV/recent' print url0 # xpath of each page xp1 = '//dl[1]//*[@class=\"list-identifier\"]//a[2]//@href' # pdf href list xp2 = '//dl[1]//*[@class=\"list-title\"]/text()' # Title xp_date = '//*[@id=\"dlpage\"]/h3[1]/text()' # date-&gt;folder htm0 = getHtml(url0) cons1 = getContent(htm0, xp1) # get pdfs' href cons2 = getContent(htm0, xp2) # get papers' title cons_date = getContent(htm0, xp_date) # get date folder = cons_date[0].split(', ') # get date string print folder[1] + ': having %s' % len(cons1) + ' files' print 'pdfs are downloading...' for indx in range(0, len(cons1)): href = 'http://arxiv.org' + cons1[indx] title = cons2[2 * indx + 1] print '%s.' % (1 + indx) + ' ' + href + ' ' + title getDownPdf(href, title, folder[1])","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://gwang-cv.github.io/tags/Python/"}]},{"title":"Deep Learning Libraries by Language","slug":"深度学习开发包","date":"2016-03-31T13:27:56.000Z","updated":"2016-03-31T13:27:56.000Z","comments":true,"path":"2016/03/31/深度学习开发包/","link":"","permalink":"http://gwang-cv.github.io/2016/03/31/深度学习开发包/","excerpt":"","text":"Python Theano is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built. a. Keras is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU. b. Pylearn2 is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano. c. Lasagne is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism , focus and restraint principles. d. Blocks a framework that helps you build neural network models on top of Theano. Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google’s DeepDream is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface. nolearn contains a number of wrappers and abstractions around existing neural network libraries, most notably Lasagne, along with a few machine learning utility modules. Gensim is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms. Chainer bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the flexible framework for Deep Learning. deepnet is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets. Hebel is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping. CXXNET is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction. DeepPy is a Pythonic deep learning framework built on top of NumPy. DeepLearning is deep learning library, developed with C++ and python. Neon is Nervana’s Python based Deep Learning framework. Matlab ConvNet Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs. DeepLearnToolBox is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets. cuda-convnet is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm. MatConvNet is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs More… http://www.teglor.com/b/deep-learning-libraries-language-cm569/ Author: teglor","categories":[{"name":"Research","slug":"Research","permalink":"http://gwang-cv.github.io/categories/Research/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://gwang-cv.github.io/tags/DeepLearning/"}]},{"title":"Python爬取微博图片","slug":"Python爬取微博图片","date":"2016-03-31T08:13:29.000Z","updated":"2016-03-31T08:13:29.000Z","comments":true,"path":"2016/03/31/Python爬取微博图片/","link":"","permalink":"http://gwang-cv.github.io/2016/03/31/Python爬取微博图片/","excerpt":"","text":"爬取指定用户的页面内容，并将图片下载保存到本地 import requests import urllib2 from lxml import etree import re from multiprocessing.dummy import Pool def tosave(texta): f = open('weibo.txt', 'a') f.write(texta + '\\n') f.close() cook = {\"Cookie\": \"XXXXXX\"} url0 = 'http://weibo.cn/u/XXXXX' def getContent(url, cook): html = requests.get(url, cookies=cook).content selector = etree.HTML(html) # read text from weibo content = selector.xpath('//span[@class=\"ctt\"]') for each in content: text = each.xpath('string(.)') print text content = selector.xpath('//*[@class=\"ib\"]/@src') # copy from chrome # print content return content def getDownImg(cons, page): x = 0 for each in cons: print each fn = '%s' % page + '%s' % x fl='%s.jpg' % fn url=each.replace('wap180','large') print url print fn r = requests.get(url) with open(fl, \"wb\") as code: code.write(r.content) x += 1 for page in range(1, 4): url = url0 + '?page=' + '%s' % page print url cons = getContent(url, cook) getDownImg(cons, page)","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://gwang-cv.github.io/tags/Python/"}]},{"title":"Python爬取微博关注","slug":"Python爬取微博关注","date":"2016-03-31T08:13:17.000Z","updated":"2016-03-31T08:13:17.000Z","comments":true,"path":"2016/03/31/Python爬取微博关注/","link":"","permalink":"http://gwang-cv.github.io/2016/03/31/Python爬取微博关注/","excerpt":"","text":"爬取微博（weibo.cn）指定用户的关注者，最多200条记录（20页，每页10条记录）。 # -*- coding: utf-8 -*- import requests import urllib2 from lxml import etree import os import time import codecs import re from multiprocessing.dummy import Pool # import sys # reload(sys) # sys.setdefaultencoding('utf-8') # write text to .txt def tosave(texta): fn = 'following' pa = os.path.dirname(__file__) + '/' + username # check and create folder if not os.path.exists(pa): os.mkdir(pa) fl = pa + '/%s.txt' % fn f = codecs.open(fl, 'a', 'utf-8') f.write(texta + '\\n') f.close() def getHtml(url, cook): html = requests.get(url, cookies=cook).content selector = etree.HTML(html) return selector def getContent(htm, xpathStr): selector = htm content = selector.xpath(xpathStr) # copy from chrome # print content return content def getPageNum(htm): xps='//*[@id=\"pagelist\"]/form/div/input[1]//@value' pnum = getContent(htm, xps) print pnum[0] return int(pnum[0]) cook = {\"Cookie\": \"XXXXXX\"} username = 'XXXXX' url0='http://weibo.cn/%s/follow' % username print url0 htm = getHtml(url0, cook) pagenum=getPageNum(htm) for pn in range(1, pagenum): urls = url0 + '?page=%s' % pn print urls htms = getHtml(urls, cook) xptxt = '//td[2]//a[1]' # homepage contains many tables xphref='//td[2]//a[1]//@href' # href consTxt = getContent(htms, xptxt) consHref=getContent(htms,xphref) for indx in range(1,len(consTxt)): followTxt=consTxt[indx].text + ' ' + consHref[indx] print followTxt tosave(followTxt)","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://gwang-cv.github.io/tags/Python/"}]},{"title":"Markdown插入公式","slug":"Markdown插入公式","date":"2016-03-31T05:16:50.000Z","updated":"2016-03-31T05:16:50.000Z","comments":true,"path":"2016/03/31/Markdown插入公式/","link":"","permalink":"http://gwang-cv.github.io/2016/03/31/Markdown插入公式/","excerpt":"","text":"在Markdown中插入数学公式可以采用如下几种方式： 解决方法1:使用Google Chart的服务器 &lt;img src=\"http://chart.googleapis.com/chart?cht=tx&amp;chl= \\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}\" style=\"border:none;\"&gt; 解决方法2:使用CodeCogs网站的latex服务接口 &lt;img src=\"http://latex.codecogs.com/svg.latex? \\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}\" style=\"border:none;\"&gt; 解决方法3:使用forkosh服务器 &lt;img src=\"http://www.forkosh.com/mathtex.cgi? \\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}\"&gt; 解决方法4:使用MathJax引擎 &lt;script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"&gt;&lt;/script&gt; $$\\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}$$ 行间 \\\\(\\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}\\\\) 行内 $$\\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}$$ \\(\\frac{1}{\\pi}=\\frac{2\\sqrt{2}}{9801}\\sum_{k=0}^\\infty\\frac{(4k)!(1103+26390k)}{(k!)^4396^{4k}}\\)","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Python中文编码错误(UnicodeEncodeError)","slug":"python中文编码错误","date":"2016-03-30T14:40:33.000Z","updated":"2016-03-30T14:40:33.000Z","comments":true,"path":"2016/03/30/python中文编码错误/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/python中文编码错误/","excerpt":"","text":"UnicodeEncodeError: ‘ascii’ codec can’t encode 解决方案1： import sys reload(sys) sys.setdefaultencoding('utf-8') 解决方案2: 不使用open打开文件，而使用codecs.open(), 在输出中文文本时，采用UTF-8编码： f = codecs.open(‘XX.txt’, ‘a’, ‘utf-8’) f.write(txts) f.close()","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"机器学习之数学基础","slug":"机器学习数学基础","date":"2016-03-30T14:38:23.000Z","updated":"2016-03-30T14:38:23.000Z","comments":true,"path":"2016/03/30/机器学习数学基础/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/机器学习数学基础/","excerpt":"","text":"Math for Machine Learning author：Hal Daum\u0013e III The goal of this document is to provide a “refresher” on continuous mathematics for computer science students. It is by no means a rigorous course on these topics. The presentation, motivation, etc., are all from a machine learning perspective. The hope, however, is that it’s useful in other contexts. The two majortopics covered are linear algebra and calculus (probability is currently left off). PDF","categories":[{"name":"Math","slug":"Math","permalink":"http://gwang-cv.github.io/categories/Math/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://gwang-cv.github.io/tags/ML/"}]},{"title":"Homebrew设置PATH路径","slug":"Homebrew设置PATH","date":"2016-03-30T14:37:51.000Z","updated":"2016-03-30T14:37:51.000Z","comments":true,"path":"2016/03/30/Homebrew设置PATH/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/Homebrew设置PATH/","excerpt":"","text":"Homebrew安装时”-bash:brew:command not found”的问题 按照[Homebrew]官网上的安装教程安装，安装成功后，运行brew时，终端提示： -bash:brew:command not found 无法找到brew命令。 卸载Homebrew重装后，终端提示： /usr/local/bin不在PATH中 解决： 用vim打开，在profile文件最后添加如下语句： PATH=“.;$PATH:/usr/local/bin” 注意，若以前修改过该文件，或者PATH中已有多个路径，只需在后面添加即可，使用：隔开。","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Mac安装Nose pip","slug":"install nose numpy scipy","date":"2016-03-30T14:37:44.000Z","updated":"2016-03-30T14:37:44.000Z","comments":true,"path":"2016/03/30/install nose numpy scipy/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/install nose numpy scipy/","excerpt":"","text":"ImportError: Need nose import numpy;numpy.test() Running unit tests for numpy Traceback (most recent call last): File \"\", line 1, in File \"/usr/lib/python2.7/site-packages/numpy/testing/nosetester.py\", line 326, in test self._show_system_info() File \"/usr/lib/python2.7/site-packages/numpy/testing/nosetester.py\", line 187, in _show_system_info nose = import_nose() File \"/usr/lib/python2.7/site-packages/numpy/testing/nosetester.py\", line 69, in import_nose raise ImportError(msg) ImportError: Need nose &gt;= 0.10.0 for tests - see http://somethingaboutorange.com/mrl/projects/nose 安装 pip mac默认是不带pip的： sudo easy_install pip 安装nose 最好用easy_install： sudo easy_install nose 别用pip，我用pip装出各种问题 terminal退出phthon命令： quit() or Control + D 安装matplotlib sudo pip install matplotlib","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Mac终端安装lxml失败","slug":"Mac终端安装lxml失败","date":"2016-03-30T14:37:23.000Z","updated":"2016-03-30T14:37:23.000Z","comments":true,"path":"2016/03/30/Mac终端安装lxml失败/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/Mac终端安装lxml失败/","excerpt":"","text":"安装lxml，出现错误 sudo pip install lxml Collecting lxml Downloading lxml-3.6.0.tar.gz (3.7MB) 100% |████████████████████████████████| 3.7MB 157kB/s Installing collected packages: lxml Running setup.py install for lxml ... error 解决方法 sudo C_INCLUDE_PATH=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/libxml2:/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/libxml2/libxml:/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include pip install lxml Installing collected packages: lxml Running setup.py install for lxml ... done Successfully installed lxml-3.6.0` 同样的,如果需要设置libpath,也在pip之前加入C_LIB_PATH再使用pip.","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Xcode安装command line tools","slug":"xcode安装command_line_tools","date":"2016-03-30T14:37:17.000Z","updated":"2016-03-30T14:37:17.000Z","comments":true,"path":"2016/03/30/xcode安装command_line_tools/","link":"","permalink":"http://gwang-cv.github.io/2016/03/30/xcode安装command_line_tools/","excerpt":"","text":"command line tools 打开命令行，输入： xcode-select --install","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Mac安装OpenCV","slug":"Mac安装OpenCV","date":"2016-01-26T12:35:51.000Z","updated":"2016-03-30T14:37:34.000Z","comments":true,"path":"2016/01/26/Mac安装OpenCV/","link":"","permalink":"http://gwang-cv.github.io/2016/01/26/Mac安装OpenCV/","excerpt":"","text":"1、安装Homebrew ruby -e \"$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)\" 2、安装CMake brew install cmake 3、安装OpenCV 在下载的OpenCV源码文件夹内，执行如下命令： mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_PYTHON_SUPPORT=ON -D BUILD_EXAMPLES=ON .. make sudo make install 4、配置Python路径 在~目录下 vim .bash_profile 在.bash_profile中添加： export PYTHONPATH=/usr/local/lib/python2.7/site-packages/ 保存(Esc后要输入:进入命令模式,键入w!强制写入；键入q!强制退出vim)，在~目录下执行下面命令: source .bash_profile","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Mac设置环境变量","slug":"Mac设置环境变量","date":"2016-01-25T12:35:51.000Z","updated":"2016-03-30T14:37:29.000Z","comments":true,"path":"2016/01/25/Mac设置环境变量/","link":"","permalink":"http://gwang-cv.github.io/2016/01/25/Mac设置环境变量/","excerpt":"","text":"（1）首先要知道你使用的Mac OS X是什么样的Shell，使用命令 echo $SHELL 如果输出的是：csh或者是tcsh，那么你用的就是C Shell。 如果输出的是：bash，sh，zsh，那么你的用的可能就是Bourne Shell的一个变种。 Mac OS X 10.2之前默认的是C Shell。 Mac OS X 10.3之后默认的是Bourne Shell。 （2）如果是Bourne Shell。 那么你可以把你要添加的环境变量添加到你主目录下面的.profile或者.bash_profile，如果存在没有关系添加进去即可，如果没有生成一个。 1. open/vim /etc/profile （建议不修改这个文件 ） 全局（公有）配置，不管是哪个用户，登录时都会读取该文件。 2. /etc/bashrc （一般在这个文件中添加系统级环境变量） 全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件。 我在这里加入mysqlstart、mysql和mysqladmin命令的别名，保证每一个用户都可以使用这3个命令。 3. ~/.bash_profile （一般在这个文件中添加用户级环境变量） （注：Linux 里面是 .bashrc 而 Mac 是 .bash_profile） 若bash shell是以login方式执行时，才会读取此文件。该文件仅仅执行一次!默认情况下,他设置一些环境变量 我在这里：设置终端配色、 我在这里：设置命令别名 alias ll='ls -la' 我在这里：设置环境变量： export PATH=/opt/local/bin:/opt/local/sbin:$PATH MAC 修改host文件 sudo vi /etc/hosts","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"Mac连接Github","slug":"Mac Link Github","date":"2016-01-23T12:35:51.000Z","updated":"2016-03-30T14:37:40.000Z","comments":true,"path":"2016/01/23/Mac Link Github/","link":"","permalink":"http://gwang-cv.github.io/2016/01/23/Mac Link Github/","excerpt":"","text":"出现问题：permission denied 解决方法： Step 1: Check for SSH keys $ ls -al ~/.ssh Step 2: Generate a new SSH key $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Step 3: Add your key to the ssh-agent $ eval \"$(ssh-agent -s)\" $ ssh-add ~/.ssh/id_rsa Step 4: Add your SSH key to your account $ pbcopy &lt; ~/.ssh/id_rsa.pub Add the copied key to GitHub: 登录github网站完成。 Step 5: Test the connection $ ssh -T git@github.com","categories":[{"name":"Debug","slug":"Debug","permalink":"http://gwang-cv.github.io/categories/Debug/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://gwang-cv.github.io/tags/Mac/"}]},{"title":"数学体系解读","slug":"数学体系解读（by MIT Lin Dahua）","date":"2015-11-15T13:35:51.000Z","updated":"2016-03-30T14:38:07.000Z","comments":true,"path":"2015/11/15/数学体系解读（by MIT Lin Dahua）/","link":"","permalink":"http://gwang-cv.github.io/2015/11/15/数学体系解读（by MIT Lin Dahua）/","excerpt":"","text":"by MIT 林达华 目录 (Contents) 1 为什么要深入数学的世界 2 集合论：现代数学的共同基础 3 分析：在极限基础上建立的宏伟大厦 3.1 微积分：分析的古典时代——从牛顿到柯西 3.2 实分析：在实数理论和测度理论上建立起现代分析 3.3 拓扑学：分析从实数轴推广到一般空间——现代分析的抽象基础 3.4 微分几何：流形上的分析——在拓扑空间上引入微分结构 4 代数：一个抽象的世界 4.1 关于抽象代数 4.2 线性代数：“线性”的基础地位 4.3 泛函分析：从有限维向无限维迈进 4.4 继续往前：巴拿赫代数，调和分析，和李代数 5 现代概率论：在现代分析基础上再生 为什么要深入数学的世界 作为计算机的学生，我没有任何企图要成为一个数学家。我学习数学的目的，是要想爬上巨人的肩膀，希望站在更高的高度，能把我自己研究的东西看得更深广一些。说起来，我在刚来这个学校的时候，并没有预料到我将会有一个深入数学的旅程。我的导师最初希望我去做的题目，是对appearance和motion建立一个unified的model。这个题目在当今Computer Vision中百花齐放的世界中并没有任何特别的地方。事实上，使用各种Graphical Model把各种东西联合在一起framework，在近年的论文中并不少见。 我不否认现在广泛流行的Graphical Model是对复杂现象建模的有力工具，但是，我认为它不是panacea，并不能取代对于所研究的问题的深入的钻研。如果统计学习包治百病，那么很多 “下游”的学科也就没有存在的必要了。事实上，开始的时候，我也是和Vision中很多人一样，想着去做一个Graphical Model——我的导师指出，这样的做法只是重复一些标准的流程，并没有很大的价值。经过很长时间的反复，另外一个路径慢慢被确立下来——我们相信，一个 图像是通过大量“原子”的某种空间分布构成的，原子群的运动形成了动态的可视过程。微观意义下的单个原子运动，和宏观意义下的整体分布的变换存在着深刻的 联系——这需要我们去发掘。 在深入探索这个题目的过程中，遇到了很多很多的问题，如何描述一个一般的运动过程，如何建立一个稳定并且广泛适用的原子表达，如何刻画微观运动和宏观分布变换的联系，还有很多。在这个过程中，我发现了两个事情： 我原有的数学基础已经远远不能适应我对这些问题的深入研究。在数学中，有很多思想和工具，是非常适合解决这些问题的，只是没有被很多的应用科学的研究者重视。于是，我决心开始深入数学这个浩瀚大海，希望在我再次走出来的时候，我已经有了更强大的武器去面对这些问题的挑战。 我的游历并没有结束，我的视野相比于这个博大精深的世界的依旧显得非常狭窄。在这里，我只是说说，在我的眼中，数学如何一步步从初级向高级发展，更高级别的数学对于具体应用究竟有何好处。 集合论：现代数学的共同基础 现代数学有数不清的分支，但是，它们都有一个共同的基础——集合论——因为 它，数学这个庞大的家族有个共同的语言。集合论中有一些最基本的概念：集合(set)，关系(relation)，函数(function)，等价 (equivalence)，是在其它数学分支的语言中几乎必然存在的。对于这些简单概念的理解，是进一步学些别的数学的基础。我相信，理工科大学生对于 这些都不会陌生。 不过，有一个很重要的东西就不见得那么家喻户晓了——那就是“选择公理” (Axiom of Choice)。这个公理的意思是“任意的一群非空集合，一定可以从每个集合中各拿出一个元素。”——似乎是显然得不能再显然的命题。不过，这个貌似平常 的公理却能演绎出一些比较奇怪的结论，比如巴拿赫-塔斯基分球定理——“一个球，能分成五个部分，对它们进行一系列刚性变换（平移旋转）后，能组合成两个一样大小的球”。正因为这些完全有悖常识的结论，导致数学界曾经在相当长时间里对于是否接受它有着激烈争论。现在，主流数学家对于它应该是基本接受的，因为很多数学分支的重要定理都依赖于它。在我们后面要回说到的学科里面，下面的定理依赖于选择公理： 拓扑学：Baire Category Theorem实分析（测度理论）：Lebesgue 不可测集的存在性泛函分析四个主要定理：Hahn-Banach Extension Theorem, Banach-Steinhaus Theorem (Uniform boundedness principle), Open Mapping Theorem, Closed Graph Theorem在集合论的基础上，现代数学有两大家族：分析(Analysis)和代数(Algebra)。至于其它的，比如几何和概率论，在古典数学时代，它们是和代数并列的，但是它们的现代版本则基本是建立在分析或者代数的基础上，因此从现代意义说，它们和分析与代数并不是平行的关系。 分析：在极限基础上建立的宏伟大厦 微积分：分析的古典时代——从牛顿到柯西 先说说分析(Analysis)吧，它是从微积分(Caculus)发展起来 的——这也是有些微积分教材名字叫“数学分析”的原因。不过，分析的范畴远不只是这些，我们在大学一年级学习的微积分只能算是对古典分析的入门。分析研究 的对象很多，包括导数(derivatives)，积分(integral)，微分方程(differential equation)，还有级数(infinite series)——这些基本的概念，在初等的微积分里面都有介绍。如果说有一个思想贯穿其中，那就是极限——这是整个分析（不仅仅是微积分）的灵魂。 一个很多人都听说过的故事，就是牛顿(Newton)和莱布尼茨 (Leibniz)关于微积分发明权的争论。事实上，在他们的时代，很多微积分的工具开始运用在科学和工程之中，但是，微积分的基础并没有真正建立。那个 长时间一直解释不清楚的“无穷小量”的幽灵，困扰了数学界一百多年的时间——这就是“第二次数学危机”。直到柯西用数列极限的观点重新建立了微积分的基本 概念，这门学科才开始有了一个比较坚实的基础。直到今天，整个分析的大厦还是建立在极限的基石之上。 柯西(Cauchy)为分析的发展提供了一种严密的语言，但是他并没有解决微 积分的全部问题。在19世纪的时候，分析的世界仍然有着一些挥之不去的乌云。而其中最重要的一个没有解决的是“函数是否可积的问题”。我们在现在的微积分 课本中学到的那种通过“无限分割区间，取矩阵面积和的极限”的积分，是大约在1850年由黎曼(Riemann)提出的，叫做黎曼积分。但是，什么函数存 在黎曼积分呢（黎曼可积）？数学家们很早就证明了，定义在闭区间内的连续函数是黎曼可积的。可是，这样的结果并不令人满意，工程师们需要对分段连续函数的 函数积分。 实分析：在实数理论和测度理论上建立起现代分析 在19世纪中后期，不连续函数的可积性问题一直是分析的重要课题。对于定义在 闭区间上的黎曼积分的研究发现，可积性的关键在于“不连续的点足够少”。只有有限处不连续的函数是可积的，可是很多有数学家们构造出很多在无限处不连续的 可积函数。显然，在衡量点集大小的时候，有限和无限并不是一种合适的标准。在探讨“点集大小”这个问题的过程中，数学家发现实数轴——这个他们曾经以为已 经充分理解的东西——有着许多他们没有想到的特性。在极限思想的支持下，实数理论在这个时候被建立起来，它的标志是对实数完备性进行刻画的几条等价的定理 （确界定理，区间套定理，柯西收敛定理，Bolzano-Weierstrass Theorem和Heine-Borel Theorem等等）——这些定理明确表达出实数和有理数的根本区别：完备性（很不严格的说，就是对极限运算封闭）。随着对实数认识的深入，如何测量“点 集大小”的问题也取得了突破，勒贝格创造性地把关于集合的代数，和Outer content（就是“外测度”的一个雏形）的概念结合起来，建立了测度理论(Measure Theory)，并且进一步建立了以测度为基础的积分——勒贝格(Lebesgue Integral)。在这个新的积分概念的支持下，可积性问题变得一目了然。 上面说到的实数理论，测度理论和勒贝格积分，构成了我们现在称为实分析 (Real Analysis)的数学分支，有些书也叫实变函数论。对于应用科学来说，实分析似乎没有古典微积分那么“实用”——很难直接基于它得到什么算法。而且， 它要解决的某些“难题”——比如处处不连续的函数，或者处处连续而处处不可微的函数——在工程师的眼中，并不现实。但是，我认为，它并不是一种纯数学概念 游戏，它的现实意义在于为许多现代的应用数学分支提供坚实的基础。下面，我仅仅列举几条它的用处： 黎曼可积的函数空间不是完备的，但是勒贝格可积的函数空间是完备的。简单的 说，一个黎曼可积的函数列收敛到的那个函数不一定是黎曼可积的，但是勒贝格可积的函数列必定收敛到一个勒贝格可积的函数。在泛函分析，还有逼近理论中，经 常需要讨论“函数的极限”，或者“函数的级数”，如果用黎曼积分的概念，这种讨论几乎不可想像。我们有时看一些paper中提到Lp函数空间，就是基于勒 贝格积分。勒贝格积分是傅立叶变换（这东西在工程中到处都是）的基础。很多关于信号处理的初等教材，可能绕过了勒贝格积分，直接讲点面对实用的东西而不谈它的数学基础，但是，对于深层次的研究问题——特别是希望在理论中能做一些工作——这并不是总能绕过去。在下面，我们还会看到，测度理论是现代概率论的基础。 拓扑学：分析从实数轴推广到一般空间——现代分析的抽象基础 随着实数理论的建立，大家开始把极限和连续推广到更一般的地方的分析。事实 上，很多基于实数的概念和定理并不是实数特有的。很多特性可以抽象出来，推广到更一般的空间里面。对于实数轴的推广，促成了点集拓扑学(Point- set Topology)的建立。很多原来只存在于实数中的概念，被提取出来，进行一般性的讨论。在拓扑学里面，有4个C构成了它的核心： Closed set（闭集合）。在现代的拓扑学的公理化体系中，开集和闭集是最基本的概念。一切从此引申。这两个概念是开区间和闭区间的推广，它们的根本地位，并不是 一开始就被认识到的。经过相当长的时间，人们才认识到：开集的概念是连续性的基础，而闭集对极限运算封闭——而极限正是分析的根基。Continuous function （连续函数）。连续函数在微积分里面有个用epsilon-delta语言给出的定义，在拓扑学中它的定义是“开集的原像是开集的函数”。第二个定义和第 一个是等价的，只是用更抽象的语言进行了改写。我个人认为，它的第三个（等价）定义才从根本上揭示连续函数的本质——“连续函数是保持极限运算的函数” ——比如y是数列x1, x2, x3, … 的极限， 那么如果 f 是连续函数，那么 f(y) 就是 f(x1), f(x2), f(x3), …的极限。连续函数的重要性，可以从别的分支学科中进行类比。比如群论中，基础的运算是“乘法”，对于群，最重要的映射叫“同态映射”——保持“乘法”的 映射。在分析中，基础运算是“极限”，因此连续函数在分析中的地位，和同态映射在代数中的地位是相当的。Connected set （连通集合）。比它略为窄一点的概念叫(Path connected)，就是集合中任意两点都存在连续路径相连——可能是一般人理解的概念。一般意义下的连通概念稍微抽象一些。在我看来，连通性有两个重 要的用场：一个是用于证明一般的中值定理(Intermediate Value Theorem)，还有就是代数拓扑，拓扑群论和李群论中讨论根本群(Fundamental Group)的阶。Compact set（紧集）。Compactness似乎在初等微积分里面没有专门出现，不过有几条实数上的定理和它其实是有关系的。比如，“有界数列必然存在收敛子 列”——用compactness的语言来说就是——“实数空间中有界闭集是紧的”。它在拓扑学中的一般定义是一个听上去比较抽象的东西——“紧集的任意 开覆盖存在有限子覆盖”。这个定义在讨论拓扑学的定理时很方便，它在很多时候能帮助实现从无限到有限的转换。对于分析来说，用得更多的是它的另一种形式 ——“紧集中的数列必存在收敛子列”——它体现了分析中最重要的“极限”。Compactness在现代分析中运用极广，无法尽述。微积分中的两个重要定 理：极值定理(Extreme Value Theory)，和一致收敛定理(Uniform Convergence Theorem)就可以借助它推广到一般的形式。从某种意义上说，点集拓扑学可以看成是关于“极限”的一般理论，它抽象于实数理论，它的概念成为几乎所有现代分析学科的通用语言，也是整个现代分析的根基所在。 微分几何：流形上的分析——在拓扑空间上引入微分结构 拓扑学把极限的概念推广到一般的拓扑空间，但这不是故事的结束，而仅仅是开 始。在微积分里面，极限之后我们有微分，求导，积分。这些东西也可以推广到拓扑空间，在拓扑学的基础上建立起来——这就是微分几何。从教学上说，微分几何 的教材，有两种不同的类型，一种是建立在古典微机分的基础上的“古典微分几何”，主要是关于二维和三维空间中的一些几何量的计算，比如曲率。还有一种是建 立在现代拓扑学的基础上，这里姑且称为“现代微分几何”——它的核心概念就是“流形”(manifold)——就是在拓扑空间的基础上加了一套可以进行微 分运算的结构。现代微分几何是一门非常丰富的学科。比如一般流形上的微分的定义就比传统的微分丰富，我自己就见过三种从不同角度给出的等价定义——这一方 面让事情变得复杂一些，但是另外一个方面它给了同一个概念的不同理解，往往在解决问题时会引出不同的思路。除了推广微积分的概念以外，还引入了很多新概 念：tangent space, cotangent space, push forward, pull back, fibre bundle, flow, immersion, submersion 等等。 近些年，流形在machine learning似乎相当时髦。但是，坦率地说，要弄懂一些基本的流形算法， 甚至“创造”一些流形算法，并不需要多少微分几何的基础。对我的研究来说，微分几何最重要的应用就是建立在它之上的另外一个分支：李群和李代数——这是数 学中两大家族分析和代数的一个漂亮的联姻。分析和代数的另外一处重要的结合则是泛函分析，以及在其基础上的调和分析。 代数：一个抽象的世界 关于抽象代数 回过头来，再说说另一个大家族——代数。 如果说古典微积分是分析的入门，那么现代代数的入门点则是两个部分：线性代数(linear algebra)和基础的抽象代数(abstract algebra)——据说国内一些教材称之为近世代数。 代数——名称上研究的似乎是数，在我看来，主要研究的是运算规则。一门代数， 其实都是从某种具体的运算体系中抽象出一些基本规则，建立一个公理体系，然后在这基础上进行研究。一个集合再加上一套运算规则，就构成一个代数结构。在主 要的代数结构中，最简单的是群(Group)——它只有一种符合结合率的可逆运算，通常叫“乘法”。如果，这种运算也符合交换率，那么就叫阿贝尔群 (Abelian Group)。如果有两种运算，一种叫加法，满足交换率和结合率，一种叫乘法，满足结合率，它们之间满足分配率，这种丰富一点的结构叫做环(Ring)， 如果环上的乘法满足交换率，就叫可交换环(Commutative Ring)。如果，一个环的加法和乘法具有了所有的良好性质，那么就成为一个域(Field)。基于域，我们可以建立一种新的结构，能进行加法和数乘，就 构成了线性代数(Linear algebra)。 代数的好处在于，它只关心运算规则的演绎，而不管参与运算的对象。只要定义恰 当，完全可以让一只猫乘一只狗得到一头猪:-)。基于抽象运算规则得到的所有定理完全可以运用于上面说的猫狗乘法。当然，在实际运用中，我们还是希望用它 干点有意义的事情。学过抽象代数的都知道，基于几条最简单的规则，比如结合律，就能导出非常多的重要结论——这些结论可以应用到一切满足这些简单规则的地 方——这是代数的威力所在，我们不再需要为每一个具体领域重新建立这么多的定理。 抽象代数有在一些基础定理的基础上，进一步的研究往往分为两个流派：研究有限 的离散代数结构（比如有限群和有限域），这部分内容通常用于数论，编码，和整数方程这些地方；另外一个流派是研究连续的代数结构，通常和拓扑与分析联系在 一起（比如拓扑群，李群）。我在学习中的focus主要是后者。 线性代数：“线性”的基础地位 对于做Learning, vision, optimization或者statistics的人来说，接触最多的莫过于线性代数——这也是我们在大学低年级就开始学习的。线性代数，包括建立在它 基础上的各种学科，最核心的两个概念是向量空间和线性变换。线性变换在线性代数中的地位，和连续函数在分析中的地位，或者同态映射在群论中的地位是一样的 ——它是保持基础运算（加法和数乘）的映射。 在learning中有这样的一种倾向——鄙视线性算法，标榜非线性。也许在 很多场合下面，我们需要非线性来描述复杂的现实世界，但是无论什么时候，线性都是具有根本地位的。没有线性的基础，就不可能存在所谓的非线性推广。我们常 用的非线性化的方法包括流形和kernelization，这两者都需要在某个阶段回归线性。流形需要在每个局部建立和线性空间的映射，通过把许多局部线 性空间连接起来形成非线性；而kernerlization则是通过置换内积结构把原线性空间“非线性”地映射到另外一个线性空间，再进行线性空间中所能 进行的操作。而在分析领域，线性的运算更是无处不在，微分，积分，傅立叶变换，拉普拉斯变换，还有统计中的均值，通通都是线性的。 泛函分析：从有限维向无限维迈进 在大学中学习的线性代数，它的简单主要因为它是在有限维空间进行的，因为有 限，我们无须借助于太多的分析手段。但是，有限维空间并不能有效地表达我们的世界——最重要的，函数构成了线性空间，可是它是无限维的。对函数进行的最重 要的运算都在无限维空间进行，比如傅立叶变换和小波分析。这表明了，为了研究函数（或者说连续信号），我们需要打破有限维空间的束缚，走入无限维的函数空 间——这里面的第一步，就是泛函分析。 泛函分析(Functional Analysis)是研究的是一般的线性空间，包括有限维和无限维，但是很多东西在有限维下显得很trivial，真正的困难往往在无限维的时候出现。在 泛函分析中，空间中的元素还是叫向量，但是线性变换通常会叫作“算子”(operator)。除了加法和数乘，这里进一步加入了一些运算，比如加入范数去 表达“向量的长度”或者“元素的距离”，这样的空间叫做“赋范线性空间”(normed space)，再进一步的，可以加入内积运算，这样的空间叫“内积空间”(Inner product space)。 大家发现，当进入无限维的时间时，很多老的观念不再适用了，一切都需要重新审视。 所有的有限维空间都是完备的（柯西序列收敛），很多无限维空间却是不完备的（比如闭区间上的连续函数）。在这里，完备的空间有特殊的名称：完备的赋范空间叫巴拿赫空间(Banach space)，完备的内积空间叫希尔伯特空间(Hilbert space)。在有限维空间中空间和它的对偶空间的是完全同构的，而在无限维空间中，它们存在微妙的差别。在有限维空间中，所有线性变换（矩阵）都是有界变换，而在无限维，很多算子是无界的(unbounded)，最重要的一个例子是给函数求导。在有限维空间中，一切有界闭集都是紧的，比如单位球。而在所有的无限维空间中，单位球都不是紧的——也就是说，可以在单位球内撒入无限个点，而不出现一个极限点。在有限维空间中，线性变换（矩阵）的谱相当于全部的特征值，在无限维空间 中，算子的谱的结构比这个复杂得多，除了特征值组成的点谱(point spectrum)，还有approximate point spectrum和residual spectrum。虽然复杂，但是，也更为有趣。由此形成了一个相当丰富的分支——算子谱论(Spectrum theory)。在有限维空间中，任何一点对任何一个子空间总存在投影，而在无限维空间中， 这就不一定了，具有这种良好特性的子空间有个专门的名称切比雪夫空间(Chebyshev space)。这个概念是现代逼近理论的基础(approximation theory)。函数空间的逼近理论在Learning中应该有着非常重要的作用，但是现在看到的运用现代逼近理论的文章并不多。继续往前：巴拿赫代数，调和分析，和李代数 基本的泛函分析继续往前走，有两个重要的方向。第一个是巴拿赫代数 (Banach Algebra)，它就是在巴拿赫空间（完备的内积空间）的基础上引入乘法（这不同于数乘）。比如矩阵——它除了加法和数乘，还能做乘法——这就构成了一 个巴拿赫代数。除此以外，值域完备的有界算子，平方可积函数，都能构成巴拿赫代数。巴拿赫代数是泛函分析的抽象，很多对于有界算子导出的结论，还有算子谱 论中的许多定理，它们不仅仅对算子适用，它们其实可以从一般的巴拿赫代数中得到，并且应用在算子以外的地方。巴拿赫代数让你站在更高的高度看待泛函分析中 的结论，但是，我对它在实际问题中能比泛函分析能多带来什么东西还有待思考。 最能把泛函分析和实际问题在一起的另一个重要方向是调和分析 (Harmonic Analysis)。我在这里列举它的两个个子领域，傅立叶分析和小波分析，我想这已经能说明它的实际价值。它研究的最核心的问题就是怎么用基函数去逼近 和构造一个函数。它研究的是函数空间的问题，不可避免的必须以泛函分析为基础。除了傅立叶和小波，调和分析还研究一些很有用的函数空间，比如Hardy space，Sobolev space，这些空间有很多很好的性质，在工程中和物理学中都有很重要的应用。对于vision来说，调和分析在信号的表达，图像的构造，都是非常有用的 工具。 当分析和线性代数走在一起，产生了泛函分析和调和分析；当分析和群论走在一 起，我们就有了李群(Lie Group)和李代数(Lie Algebra)。它们给连续群上的元素赋予了代数结构。我一直认为这是一门非常漂亮的数学：在一个体系中，拓扑，微分和代数走到了一起。在一定条件下， 通过李群和李代数的联系，它让几何变换的结合变成了线性运算，让子群化为线性子空间，这样就为Learning中许多重要的模型和算法的引入到对几何运动 的建模创造了必要的条件。因此，我们相信李群和李代数对于vision有着重要意义，只不过学习它的道路可能会很艰辛，在它之前需要学习很多别的数学。 现代概率论：在现代分析基础上再生 最后，再简单说说很多Learning的研究者特别关心的数学分支：概率论。 自从Kolmogorov在上世纪30年代把测度引入概率论以来，测度理论就成为现代概率论的基础。在这里，概率定义为测度，随机变量定义为可测函数，条 件随机变量定义为可测函数在某个函数空间的投影，均值则是可测函数对于概率测度的积分。值得注意的是，很多的现代观点，开始以泛函分析的思路看待概率论的 基础概念，随机变量构成了一个向量空间，而带符号概率测度则构成了它的对偶空间，其中一方施加于对方就形成均值。角度虽然不一样，不过这两种方式殊途同 归，形成的基础是等价的。 在现代概率论的基础上，许多传统的分支得到了极大丰富，最有代表性的包括鞅论 (Martingale)——由研究赌博引发的理论，现在主要用于金融（这里可以看出赌博和金融的理论联系，:-P），布朗运动(Brownian Motion)——连续随机过程的基础，以及在此基础上建立的随机分析(Stochastic Calculus)，包括随机积分（对随机过程的路径进行积分，其中比较有代表性的叫伊藤积分(Ito Integral)），和随机微分方程。对于连续几何运用建立概率模型以及对分布的变换的研究离不开这些方面的知识。 [转载自：http://blog.jobbole.com/94591/]","categories":[{"name":"Math","slug":"Math","permalink":"http://gwang-cv.github.io/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"http://gwang-cv.github.io/tags/Math/"}]}]}